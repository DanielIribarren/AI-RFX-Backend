"""
üéØ RFX Processor Service - Core business logic for RFX document processing
Extracts logic from rfx_webhook.py and improves it with better practices

REFACTORIZADO: Sistema modular con validaciones Pydantic, templates Jinja2,
modo debug con confidence scores y arquitectura extensible.
"""
import io
import re
import json
import PyPDF2
import time
from datetime import datetime
from typing import List, Dict, Any, Optional, Union, Tuple
from enum import Enum
from openai import OpenAI

# Pydantic imports for robust validation
from pydantic import BaseModel, Field, validator, ValidationError

# Jinja2 imports for dynamic templates
from jinja2 import Template, Environment, BaseLoader

from backend.models.rfx_models import (
    RFXInput, RFXProcessed, RFXProductRequest, RFXType, RFXStatus,
    CompanyModel, RequesterModel, RFXHistoryEvent,
    # Legacy aliases for backwards compatibility
    ProductoRFX, TipoRFX, EstadoRFX
)
from backend.models.proposal_models import ProposalRequest, ProposalNotes
from backend.core.config import get_openai_config
from backend.core.database import get_database_client
from backend.utils.validators import EmailValidator, DateValidator, TimeValidator
from backend.utils.text_utils import chunk_text, clean_json_string
from backend.core.feature_flags import FeatureFlags

import logging

logger = logging.getLogger(__name__)


# ============================================================================
# üéØ MODELOS PYDANTIC PARA VALIDACI√ìN ESTRUCTURADA
# ============================================================================

class ExtractionConfidence(BaseModel):
    """Modelo para tracking de confidence scores en extracciones"""
    field_name: str = Field(..., description="Nombre del campo extra√≠do")
    confidence: float = Field(..., ge=0.0, le=1.0, description="Confidence score 0-1")
    source: str = Field(..., description="Fuente de la extracci√≥n (AI, manual, fallback)")
    metadata: Dict[str, Any] = Field(default_factory=dict, description="Metadata adicional")

class ProductExtraction(BaseModel):
    """Modelo Pydantic para validaci√≥n robusta de productos extra√≠dos"""
    nombre: str = Field(..., min_length=1, max_length=200, description="Nombre del producto")
    cantidad: int = Field(..., ge=1, le=10000, description="Cantidad del producto")
    unidad: str = Field(..., min_length=1, max_length=50, description="Unidad de medida")
    confidence: float = Field(default=0.8, ge=0.0, le=1.0, description="Confidence score")
    
    @validator('nombre')
    def validate_nombre(cls, v):
        """Validador para nombre de producto"""
        if v.lower() in ['null', 'none', 'undefined', '']:
            raise ValueError('Nombre de producto no puede ser nulo o vac√≠o')
        return v.strip().title()
    
    @validator('unidad')
    def validate_unidad(cls, v):
        """Validador para unidad de medida"""
        valid_units = ['unidades', 'personas', 'kg', 'litros', 'porciones', 'servicios', 'pax']
        v_clean = v.lower().strip()
        if v_clean not in valid_units:
            # Auto-map common variants
            unit_mapping = {
                'unidad': 'unidades', 'pcs': 'unidades', 'units': 'unidades',
                'persona': 'personas', 'ppl': 'personas', 'people': 'personas',
                'litro': 'litros', 'l': 'litros', 'lts': 'litros',
                'porcion': 'porciones', 'servicio': 'servicios'
            }
            v_clean = unit_mapping.get(v_clean, 'unidades')
        return v_clean

class ChunkExtractionResult(BaseModel):
    """Modelo para resultado de extracci√≥n por chunk con confidence tracking"""
    email: Optional[str] = Field(None, description="Email extra√≠do")
    email_empresa: Optional[str] = Field(None, description="Email de la empresa")
    nombre_solicitante: Optional[str] = Field(None, description="Nombre del solicitante")
    nombre_empresa: Optional[str] = Field(None, description="Nombre de la empresa") 
    telefono_solicitante: Optional[str] = Field(None, description="Tel√©fono del solicitante")
    telefono_empresa: Optional[str] = Field(None, description="Tel√©fono de la empresa")
    cargo_solicitante: Optional[str] = Field(None, description="Cargo del solicitante")
    tipo_solicitud: Optional[str] = Field(None, description="Tipo de solicitud")
    productos: List[ProductExtraction] = Field(default_factory=list, description="Lista de productos")
    fecha: Optional[str] = Field(None, description="Fecha de entrega")
    hora_entrega: Optional[str] = Field(None, description="Hora de entrega")
    lugar: Optional[str] = Field(None, description="Lugar del evento")
    texto_original_relevante: Optional[str] = Field(None, description="Texto original relevante")
    
    # Campos de confidence y debugging
    confidence_scores: List[ExtractionConfidence] = Field(default_factory=list)
    extraction_metadata: Dict[str, Any] = Field(default_factory=dict)
    chunk_index: int = Field(default=0, description="√çndice del chunk procesado")

class ExtractionDebugInfo(BaseModel):
    """Informaci√≥n de debug para el modo debug"""
    chunk_count: int
    total_characters: int
    processing_time_seconds: float
    ai_calls_made: int
    retries_attempted: int
    confidence_summary: Dict[str, float]
    extraction_quality: str  # 'high', 'medium', 'low', 'fallback'

# ============================================================================
# üè≠ SISTEMA MODULAR DE EXTRACTORES
# ============================================================================

class ExtractionStrategy(Enum):
    """Estrategias de extracci√≥n disponibles"""
    CONSERVATIVE = "conservative"  # Alta precisi√≥n, baja sensibilidad
    BALANCED = "balanced"         # Balance entre precisi√≥n y sensibilidad 
    AGGRESSIVE = "aggressive"     # Alta sensibilidad, menor precisi√≥n

class BaseExtractor:
    """Extractor base con funcionalidad com√∫n"""
    
    def __init__(self, debug_mode: bool = False):
        self.debug_mode = debug_mode
        self.confidence_threshold = 0.7
        
    def calculate_confidence(self, extracted_value: Any, raw_text: str) -> float:
        """Calcula confidence score basado en heur√≠sticas"""
        if not extracted_value:
            return 0.0
        
        # Heur√≠sticas b√°sicas para confidence
        confidence = 0.5  # Base confidence
        
        # Boost si el valor extra√≠do aparece directamente en el texto
        if isinstance(extracted_value, str) and extracted_value.lower() in raw_text.lower():
            confidence += 0.3
            
        # Boost por longitud/format validity 
        if isinstance(extracted_value, str) and len(extracted_value) > 3:
            confidence += 0.1
            
        return min(confidence, 1.0)

class ProductExtractor(BaseExtractor):
    """Extractor especializado para productos con validaci√≥n robusta"""
    
    def extract_products(self, chunk_data: Dict[str, Any], raw_text: str) -> List[ProductExtraction]:
        """Extrae y valida productos de los datos del chunk"""
        productos_raw = chunk_data.get("productos", [])
        productos_validated = []
        
        if self.debug_mode:
            logger.debug(f"üîç ProductExtractor: procesando {len(productos_raw)} productos raw")
        
        for i, producto_raw in enumerate(productos_raw):
            try:
                # Intentar crear ProductExtraction con validaci√≥n Pydantic
                if isinstance(producto_raw, dict):
                    # Extract and clean product data
                    nombre = self._extract_product_name(producto_raw)
                    cantidad = self._extract_product_quantity(producto_raw)
                    unidad = self._extract_product_unit(producto_raw)
                    
                    if nombre:  # Solo proceder si tenemos nombre v√°lido
                        confidence = self.calculate_confidence(nombre, raw_text)
                        
                        producto = ProductExtraction(
                            nombre=nombre,
                            cantidad=cantidad,
                            unidad=unidad,
                            confidence=confidence
                        )
                        productos_validated.append(producto)
                        
                        if self.debug_mode:
                            logger.debug(f"‚úÖ Producto {i+1} validado: {producto.nombre} (confidence: {confidence:.2f})")
                    else:
                        if self.debug_mode:
                            logger.warning(f"‚ö†Ô∏è Producto {i+1} rechazado: nombre inv√°lido")
                            
                elif isinstance(producto_raw, str) and producto_raw.strip():
                    # Handle simple string products
                    confidence = self.calculate_confidence(producto_raw, raw_text)
                    producto = ProductExtraction(
                        nombre=producto_raw.strip(),
                        cantidad=1,
                        unidad="unidades",
                        confidence=confidence
                    )
                    productos_validated.append(producto)
                    
            except ValidationError as e:
                if self.debug_mode:
                    logger.warning(f"‚ö†Ô∏è Validaci√≥n fallida para producto {i+1}: {e}")
                continue
        
        return productos_validated
    
    def _extract_product_name(self, producto_dict: Dict[str, Any]) -> Optional[str]:
        """Extrae nombre del producto desde m√∫ltiples claves posibles"""
        name_keys = ["nombre", "name", "product", "producto", "item"]
        for key in name_keys:
            if key in producto_dict and producto_dict[key]:
                name = str(producto_dict[key]).strip()
                if name and name.lower() not in ["null", "none", "undefined", ""]:
                    return name
        return None
    
    def _extract_product_quantity(self, producto_dict: Dict[str, Any]) -> int:
        """Extrae cantidad del producto con fallback a 1"""
        qty_keys = ["cantidad", "quantity", "qty", "count", "numero"]
        for key in qty_keys:
            if key in producto_dict and producto_dict[key]:
                try:
                    return max(1, int(float(producto_dict[key])))
                except (ValueError, TypeError):
                    continue
        return 1
    
    def _extract_product_unit(self, producto_dict: Dict[str, Any]) -> str:
        """Extrae unidad del producto con fallback a 'unidades'"""
        unit_keys = ["unidad", "unit", "units", "medida"]
        for key in unit_keys:
            if key in producto_dict and producto_dict[key]:
                return str(producto_dict[key]).strip().lower()
        return "unidades"

class SolicitanteExtractor(BaseExtractor):
    """Extractor especializado para informaci√≥n del solicitante"""
    
    def extract_solicitante_info(self, chunk_data: Dict[str, Any], raw_text: str) -> Dict[str, Any]:
        """Extrae informaci√≥n del solicitante con confidence tracking"""
        client_info = {}
        confidence_scores = []
        
        # Email extraction with confidence
        email = chunk_data.get("email_solicitante") or chunk_data.get("email")
        if email:
            confidence = self.calculate_confidence(email, raw_text)
            # Additional email validation boost
            if "@" in email and "." in email:
                confidence += 0.2
            client_info["email"] = email
            confidence_scores.append(ExtractionConfidence(
                field_name="email", confidence=min(confidence, 1.0), 
                source="AI", metadata={"format_valid": "@" in email}
            ))
        
        # Company email
        email_empresa = chunk_data.get("email_empresa")
        if email_empresa:
            confidence = self.calculate_confidence(email_empresa, raw_text)
            if "@" in email_empresa and "." in email_empresa:
                confidence += 0.2
            client_info["email_empresa"] = email_empresa
            confidence_scores.append(ExtractionConfidence(
                field_name="email_empresa", confidence=min(confidence, 1.0),
                source="AI", metadata={"format_valid": "@" in email_empresa}
            ))
        
        # Solicitante name extraction
        nombre_solicitante = chunk_data.get("nombre_solicitante")
        if nombre_solicitante:
            confidence = self.calculate_confidence(nombre_solicitante, raw_text)
            client_info["nombre_solicitante"] = nombre_solicitante
            confidence_scores.append(ExtractionConfidence(
                field_name="nombre_solicitante", confidence=confidence, source="AI"
            ))
        
        # Company name extraction
        nombre_empresa = chunk_data.get("nombre_empresa")
        if nombre_empresa:
            confidence = self.calculate_confidence(nombre_empresa, raw_text)
            client_info["nombre_empresa"] = nombre_empresa
            confidence_scores.append(ExtractionConfidence(
                field_name="nombre_empresa", confidence=confidence, source="AI"
            ))
        
        # Phone numbers
        for phone_field in ["telefono_solicitante", "telefono_empresa"]:
            phone = chunk_data.get(phone_field)
            if phone:
                confidence = self.calculate_confidence(phone, raw_text)
                client_info[phone_field] = phone
                confidence_scores.append(ExtractionConfidence(
                    field_name=phone_field, confidence=confidence, source="AI"
                ))
        
        # Position/cargo
        cargo = chunk_data.get("cargo_solicitante")
        if cargo:
            confidence = self.calculate_confidence(cargo, raw_text)
            client_info["cargo_solicitante"] = cargo
            confidence_scores.append(ExtractionConfidence(
                field_name="cargo_solicitante", confidence=confidence, source="AI"
            ))
        
        client_info["confidence_scores"] = confidence_scores
        return client_info

class EventExtractor(BaseExtractor):
    """Extractor especializado para informaci√≥n del evento"""
    
    def extract_event_info(self, chunk_data: Dict[str, Any], raw_text: str) -> Dict[str, Any]:
        """Extrae informaci√≥n del evento con validaci√≥n"""
        event_info = {}
        confidence_scores = []
        
        # Date extraction
        fecha = chunk_data.get("fecha")
        if fecha:
            confidence = self.calculate_confidence(fecha, raw_text)
            # Boost confidence for date-like patterns
            if re.match(r'\d{4}-\d{2}-\d{2}', str(fecha)):
                confidence += 0.2
            event_info["fecha"] = fecha
            confidence_scores.append(ExtractionConfidence(
                field_name="fecha", confidence=min(confidence, 1.0), source="AI"
            ))
        
        # Time extraction  
        hora_entrega = chunk_data.get("hora_entrega")
        if hora_entrega:
            confidence = self.calculate_confidence(hora_entrega, raw_text)
            # Boost confidence for time-like patterns
            if re.match(r'\d{1,2}:\d{2}', str(hora_entrega)):
                confidence += 0.2
            event_info["hora_entrega"] = hora_entrega
            confidence_scores.append(ExtractionConfidence(
                field_name="hora_entrega", confidence=min(confidence, 1.0), source="AI"
            ))
        
        # Location extraction
        lugar = chunk_data.get("lugar")
        if lugar:
            confidence = self.calculate_confidence(lugar, raw_text)
            event_info["lugar"] = lugar
            confidence_scores.append(ExtractionConfidence(
                field_name="lugar", confidence=confidence, source="AI"
            ))
        
        # Request type
        tipo_solicitud = chunk_data.get("tipo_solicitud")
        if tipo_solicitud:
            confidence = self.calculate_confidence(tipo_solicitud, raw_text)
            event_info["tipo_solicitud"] = tipo_solicitud
            confidence_scores.append(ExtractionConfidence(
                field_name="tipo_solicitud", confidence=confidence, source="AI"
            ))
        
        event_info["confidence_scores"] = confidence_scores
        return event_info

# ============================================================================
# üé® SISTEMA DE TEMPLATES JINJA2 PARA PROMPTS DIN√ÅMICOS
# ============================================================================

class PromptTemplateManager:
    """Manager para templates de prompts usando Jinja2"""
    
    def __init__(self):
        self.jinja_env = Environment(loader=BaseLoader())
        self._init_templates()
    
    def _init_templates(self):
        """Inicializa templates predefinidos"""
        self.templates = {
            'system_prompt': self._get_system_prompt_template(),
            'extraction_prompt': self._get_extraction_prompt_template(),
            'debug_prompt': self._get_debug_prompt_template()
        }
    
    def _get_system_prompt_template(self) -> str:
        """Template del system prompt con configuraci√≥n din√°mica"""
        return """Eres un especialista experto en an√°lisis de documentos RFX, catering y eventos corporativos. 
Tu trabajo es extraer informaci√≥n EXACTA del texto proporcionado.

CONFIGURACI√ìN:
- Estrategia: {{ strategy }}
- Modo Debug: {{ debug_mode }}
- Confidence Threshold: {{ confidence_threshold }}

REGLAS IMPORTANTES:
- Si NO encuentras un dato espec√≠fico, usa null (no inventes informaci√≥n)
- Extrae solo lo que est√° expl√≠citamente mencionado en el texto
- Para productos, incluye TODAS las cantidades y unidades mencionadas
- Preserva el texto original relevante para referencia
- Responde √öNICAMENTE con JSON v√°lido, sin texto adicional

{% if debug_mode %}
MODO DEBUG ACTIVADO:
- Incluye confidence scores para cada campo extra√≠do
- A√±ade metadata de debugging en el JSON
- Registra heur√≠sticas usadas para extraction decisions
{% endif %}"""

    def _get_extraction_prompt_template(self) -> str:
        """Template del prompt de extracci√≥n con campos din√°micos"""
        return """Analiza cuidadosamente este texto de un documento de catering/evento y extrae la siguiente informaci√≥n en formato JSON:

{
{% for field in required_fields %}
  "{{ field.name }}": "{{ field.description }} ({{ field.format if field.format else 'null si no se encuentra' }})",
{% endfor %}
{% if include_products %}
  "productos": [
    {
      "nombre": "nombre exacto del producto/servicio",
      "cantidad": n√∫mero_entero,
      "unidad": "unidades/personas/kg/litros/etc"
      {% if debug_mode %},
      "confidence": "confidence score 0.0-1.0",
      "extraction_notes": "notas sobre la extracci√≥n"
      {% endif %}
    }
  ],
{% endif %}
{% if debug_mode %}
  "extraction_metadata": {
    "chunk_quality": "calidad del chunk: high/medium/low",
    "extraction_strategy": "{{ strategy }}",
    "processing_notes": "notas del procesamiento"
  },
{% endif %}
  "texto_original_relevante": "fragmento del texto donde encontraste la informaci√≥n principal"
}

REGLAS CR√çTICAS PARA EMPRESA vs SOLICITANTE:
- EMPRESA = compa√±√≠a/organizaci√≥n que solicita el servicio
- SOLICITANTE = persona individual dentro de la empresa
- Si ves "juan.perez@chevron.com" ‚Üí nombre_empresa="Chevron", email_solicitante="juan.perez@chevron.com"
- Si ves "Sofia Elena Camejo" ‚Üí nombre_solicitante="Sofia Elena Camejo"
- Si solo hay un email corporativo, extrae la empresa del dominio (@chevron.com ‚Üí Chevron)

EJEMPLOS DE DIFERENCIACI√ìN:
- Email: "sofia.elena@chevron.com" + Nombre: "Sofia Elena"
  ‚Üí nombre_empresa: "Chevron"
  ‚Üí nombre_solicitante: "Sofia Elena"
  ‚Üí email_solicitante: "sofia.elena@chevron.com"
- "Contacto: Maria Rodriguez, Coordinadora de PDVSA"
  ‚Üí nombre_empresa: "PDVSA"
  ‚Üí nombre_solicitante: "Maria Rodriguez"
  ‚Üí cargo_solicitante: "Coordinadora"

INSTRUCCIONES ESPEC√çFICAS PARA PRODUCTOS:
- Busca CUALQUIER tipo de comida, bebida o servicio de catering
- Incluye: {{ product_categories | join(', ') }}
- Busca cantidades: n√∫meros seguidos de "personas", "pax", "unidades", "kg", "litros", "porciones"
- Si solo encuentras nombres sin cantidades, usa cantidad = 1
- Si encuentras "para X personas" pero no productos espec√≠ficos, crea producto "Catering para X personas"
- SIEMPRE incluye al menos un producto si el texto menciona cualquier tipo de comida o catering

{% if strategy == 'aggressive' %}
MODO AGRESIVO: Busca productos incluso en menciones indirectas o impl√≠citas
{% elif strategy == 'conservative' %}
MODO CONSERVADOR: Solo extrae productos expl√≠citamente mencionados con claridad
{% endif %}

INSTRUCCIONES CR√çTICAS PARA REQUIREMENTS:
- REQUIREMENTS = instrucciones espec√≠ficas o restricciones del cliente
- BUSCA: preferencias, restricciones, experiencia requerida, presupuesto, alergias, est√°ndares
- NO EXTRAER: descripciones generales como "necesitamos catering" o "queremos buen servicio"
- SI NO HAY REQUIREMENTS ESPEC√çFICOS: usar null y confidence 0.0

EJEMPLOS DE REQUIREMENTS V√ÅLIDOS:
- "Personal con m√°s de 5 a√±os de experiencia" ‚Üí requirements: "Personal con m√°s de 5 a√±os de experiencia", confidence: 0.9
- "Sin frutos secos por alergias" ‚Üí requirements: "Sin frutos secos por alergias", confidence: 0.95
- "Presupuesto m√°ximo $500" ‚Üí requirements: "Presupuesto m√°ximo $500", confidence: 1.0
- "Opciones vegetarianas √∫nicamente" ‚Üí requirements: "Opciones vegetarianas √∫nicamente", confidence: 0.9
- "Servicio debe estar listo 2 horas antes" ‚Üí requirements: "Servicio debe estar listo 2 horas antes", confidence: 0.85

EJEMPLOS DE NO-REQUIREMENTS (usar null):
- "Necesitamos catering para evento" ‚Üí requirements: null, confidence: 0.0
- "Queremos buen servicio" ‚Üí requirements: null, confidence: 0.0
- "Solicito cotizaci√≥n" ‚Üí requirements: null, confidence: 0.0

EJEMPLOS DE PRODUCTOS V√ÅLIDOS:
{% for example in product_examples %}
- "{{ example.input }}" ‚Üí {"nombre": "{{ example.nombre }}", "cantidad": {{ example.cantidad }}, "unidad": "{{ example.unidad }}"}
{% endfor %}

TEXTO A ANALIZAR:
{{ chunk_text }}

Responde SOLO con el JSON solicitado:"""

    def _get_debug_prompt_template(self) -> str:
        """Template espec√≠fico para modo debug con informaci√≥n detallada"""
        return """{{ base_prompt }}

INFORMACI√ìN DE DEBUG ADICIONAL:
- Chunk Index: {{ chunk_index }}
- Chunk Size: {{ chunk_size }} caracteres
- Previous Extractions: {{ previous_extractions_count }}
- Context: {{ context_info }}

INSTRUCCIONES DE DEBUG:
1. Incluye confidence scores detallados para cada campo
2. Explica la l√≥gica de extracci√≥n en extraction_notes
3. Identifica posibles ambig√ºedades en ambiguity_flags
4. Registra palabras clave encontradas en keywords_found

FORMATO JSON EXTENDIDO:
{
  ... campos normales ...,
  "debug_info": {
    "confidence_details": {
      "field_name": {"score": 0.85, "reasoning": "explicaci√≥n"}
    },
    "extraction_notes": "notas detalladas del proceso",
    "ambiguity_flags": ["lista de ambig√ºedades detectadas"],
    "keywords_found": ["palabras clave que influyeron en la extracci√≥n"],
    "processing_time_estimate": "estimaci√≥n del tiempo de procesamiento"
  }
}"""

    def render_prompt(self, template_name: str, **kwargs) -> str:
        """Renderiza un template con los par√°metros dados"""
        if template_name not in self.templates:
            raise ValueError(f"Template '{template_name}' no encontrado")
        
        template = self.jinja_env.from_string(self.templates[template_name])
        return template.render(**kwargs)
    
    def get_system_prompt(self, strategy: ExtractionStrategy = ExtractionStrategy.BALANCED, 
                         debug_mode: bool = False, confidence_threshold: float = 0.7) -> str:
        """Genera system prompt din√°mico"""
        return self.render_prompt('system_prompt', 
                                strategy=strategy.value,
                                debug_mode=debug_mode,
                                confidence_threshold=confidence_threshold)
    
    def get_extraction_prompt(self, chunk_text: str, strategy: ExtractionStrategy = ExtractionStrategy.BALANCED,
                            debug_mode: bool = False, chunk_index: int = 0, **kwargs) -> str:
        """Genera prompt de extracci√≥n din√°mico"""
        
        # Definir campos requeridos con descripciones mejoradas para mejor diferenciaci√≥n
        required_fields = [
            {"name": "nombre_empresa", "description": "EMPRESA: nombre de la compa√±√≠a/organizaci√≥n (ej: Chevron, Microsoft, PDVSA). Si solo tienes un email como juan@chevron.com, extrae 'Chevron' del dominio", "format": None},
            {"name": "email_empresa", "description": "EMPRESA: email corporativo general de la empresa (ej: info@chevron.com, contacto@empresa.com). NO el email personal del solicitante", "format": None},
            {"name": "nombre_solicitante", "description": "PERSONA: nombre y apellido de la persona individual que hace la solicitud (ej: 'Sofia Elena Camejo Copello', 'Juan P√©rez')", "format": None},
            {"name": "email_solicitante", "description": "PERSONA: email personal/de trabajo de la persona espec√≠fica que solicita (ej: 'sofia.camejo@chevron.com', 'juan.perez@empresa.com')", "format": None},
            {"name": "telefono_solicitante", "description": "PERSONA: n√∫mero telef√≥nico personal de la persona que solicita", "format": None},
            {"name": "telefono_empresa", "description": "EMPRESA: n√∫mero telef√≥nico principal/general de la empresa", "format": None},
            {"name": "cargo_solicitante", "description": "PERSONA: puesto/cargo que ocupa la persona en la empresa (ej: 'Gerente', 'Asistente', 'Coordinador')", "format": None},
            {"name": "tipo_solicitud", "description": "tipo de solicitud de catering/evento", "format": None},
            {"name": "fecha", "description": "fecha de entrega del evento", "format": "YYYY-MM-DD"},
            {"name": "hora_entrega", "description": "hora de entrega del evento", "format": "HH:MM"},
            {"name": "lugar", "description": "direcci√≥n completa o ubicaci√≥n donde se realizar√° el evento", "format": None},
            # üÜï MVP: Campo requirements para instrucciones espec√≠ficas del cliente
            {"name": "requirements", "description": "REQUIREMENTS: Instrucciones, preferencias o requisitos espec√≠ficos mencionados por el cliente (ej: 'empleados con +5 a√±os experiencia', 'solo opciones vegetarianas', 'presupuesto m√°ximo $1000', 'sin frutos secos por alergias'). Solo extraer si hay instrucciones claras y espec√≠ficas, NO descripciones generales", "format": None},
            {"name": "requirements_confidence", "description": "CONFIDENCE: Nivel de confianza 0.0-1.0 sobre la extracci√≥n de requirements. 1.0 = muy espec√≠ficos y claros, 0.5 = algo ambiguos, 0.0 = no hay requirements espec√≠ficos", "format": "decimal 0.0-1.0"}
        ]
        
        # Categor√≠as de productos
        product_categories = [
            "sandwiches", "bocadillos", "ensaladas", "bebidas", "caf√©", "agua", 
            "postres", "men√∫s", "comidas", "teque√±os", "empanadas", "canap√©s"
        ]
        
        # Ejemplos de productos
        product_examples = [
            {"input": "Catering para 60 personas", "nombre": "Catering", "cantidad": 60, "unidad": "personas"},
            {"input": "30 sandwiches", "nombre": "Sandwiches", "cantidad": 30, "unidad": "unidades"},
            {"input": "Caf√© para todos", "nombre": "Caf√©", "cantidad": 1, "unidad": "servicio"}
        ]
        
        if debug_mode:
            # Usar template de debug
            base_prompt = self.render_prompt('extraction_prompt',
                                           required_fields=required_fields,
                                           include_products=True,
                                           product_categories=product_categories,
                                           product_examples=product_examples,
                                           strategy=strategy.value,
                                           debug_mode=debug_mode,
                                           chunk_text=chunk_text)
            
            return self.render_prompt('debug_prompt',
                                    base_prompt=base_prompt,
                                    chunk_index=chunk_index,
                                    chunk_size=len(chunk_text),
                                    previous_extractions_count=kwargs.get('previous_extractions_count', 0),
                                    context_info=kwargs.get('context_info', 'N/A'))
        else:
            # Usar template normal
            return self.render_prompt('extraction_prompt',
                                    required_fields=required_fields,
                                    include_products=True,
                                    product_categories=product_categories,
                                    product_examples=product_examples,
                                    strategy=strategy.value,
                                    debug_mode=debug_mode,
                                    chunk_text=chunk_text)

# ============================================================================
# üîß EXTRACTOR MODULAR MEJORADO
# ============================================================================

class ModularRFXExtractor:
    """Extractor modular que coordina todos los extractores especializados"""
    
    def __init__(self, strategy: ExtractionStrategy = ExtractionStrategy.BALANCED, debug_mode: bool = False):
        self.strategy = strategy
        self.debug_mode = debug_mode or FeatureFlags.eval_debug_enabled()
        
        # Inicializar extractores especializados
        self.product_extractor = ProductExtractor(debug_mode=self.debug_mode)
        self.solicitante_extractor = SolicitanteExtractor(debug_mode=self.debug_mode)
        self.event_extractor = EventExtractor(debug_mode=self.debug_mode)
        
        # Inicializar template manager
        self.template_manager = PromptTemplateManager()
        
        # Estad√≠sticas de debugging
        self.extraction_stats = {
            'chunks_processed': 0,
            'ai_calls_made': 0,
            'retries_attempted': 0,
            'total_processing_time': 0.0
        }
        
        if self.debug_mode:
            logger.info(f"üîß ModularRFXExtractor inicializado - Estrategia: {strategy.value}, Debug: {debug_mode}")
    
    def extract_from_chunk(self, chunk_text: str, chunk_index: int = 0, 
                          openai_client: OpenAI = None, openai_config: Any = None) -> ChunkExtractionResult:
        """
        Extrae informaci√≥n de un chunk usando el sistema modular mejorado
        
        Args:
            chunk_text: Texto del chunk a procesar
            chunk_index: √çndice del chunk para tracking
            openai_client: Cliente OpenAI configurado
            openai_config: Configuraci√≥n de OpenAI
            
        Returns:
            ChunkExtractionResult: Resultado validado con confidence scores
        """
        start_time = time.time()
        
        try:
            if self.debug_mode:
                logger.debug(f"üîç Procesando chunk {chunk_index} ({len(chunk_text)} chars) con estrategia {self.strategy.value}")
            
            # Generar prompts din√°micos usando templates
            system_prompt = self.template_manager.get_system_prompt(
                strategy=self.strategy,
                debug_mode=self.debug_mode,
                confidence_threshold=0.7
            )
            
            extraction_prompt = self.template_manager.get_extraction_prompt(
                chunk_text=chunk_text,
                strategy=self.strategy,
                debug_mode=self.debug_mode,
                chunk_index=chunk_index,
                previous_extractions_count=self.extraction_stats['chunks_processed']
            )
            
            # Llamada a OpenAI con retry logic
            raw_result = self._call_openai_with_retry(
                openai_client=openai_client,
                openai_config=openai_config,
                system_prompt=system_prompt,
                user_prompt=extraction_prompt
            )
            
            # Procesar resultado con extractores especializados
            chunk_result = self._process_extraction_result(raw_result, chunk_text, chunk_index)
            
            # Actualizar estad√≠sticas
            processing_time = time.time() - start_time
            self.extraction_stats['chunks_processed'] += 1
            self.extraction_stats['total_processing_time'] += processing_time
            
            if self.debug_mode:
                logger.debug(f"‚úÖ Chunk {chunk_index} procesado en {processing_time:.3f}s - Productos: {len(chunk_result.productos)}")
            
            return chunk_result
            
        except Exception as e:
            logger.error(f"‚ùå Error procesando chunk {chunk_index}: {e}")
            # Retornar resultado vac√≠o en caso de error
            return ChunkExtractionResult(
                chunk_index=chunk_index,
                extraction_metadata={
                    'error': str(e),
                    'processing_time': time.time() - start_time,
                    'fallback_used': True
                }
            )
    
    def _call_openai_with_retry(self, openai_client: OpenAI, openai_config: Any,
                               system_prompt: str, user_prompt: str, max_retries: int = 2) -> Dict[str, Any]:
        """Llama a OpenAI con retry logic y parsing robusto"""
        
        for attempt in range(max_retries):
            try:
                self.extraction_stats['ai_calls_made'] += 1
                
                if self.debug_mode:
                    logger.debug(f"ü§ñ Llamada OpenAI intento {attempt + 1}/{max_retries}")
                
                response = openai_client.chat.completions.create(
                    model=openai_config.model,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    temperature=openai_config.temperature,
                    max_tokens=openai_config.max_tokens,
                    timeout=30
                )
                
                output = response.choices[0].message.content.strip()
                
                # Parsear JSON con manejo robusto
                return self._parse_json_response(output)
                
            except Exception as e:
                self.extraction_stats['retries_attempted'] += 1
                
                if attempt < max_retries - 1:
                    wait_time = (2 ** attempt) + 1
                    if self.debug_mode:
                        logger.warning(f"‚ö†Ô∏è Intento {attempt + 1} fall√≥: {e}, reintentando en {wait_time}s")
                    time.sleep(wait_time)
                else:
                    logger.error(f"‚ùå Todos los intentos OpenAI fallaron: {e}")
                    raise
    
    def _parse_json_response(self, output: str) -> Dict[str, Any]:
        """Parsea respuesta JSON con manejo robusto de errores"""
        # Extraer JSON del output
        json_start = output.find('{')
        json_end = output.rfind('}') + 1
        
        if json_start >= 0 and json_end > json_start:
            json_str = output[json_start:json_end]
            
            try:
                return json.loads(json_str)
            except json.JSONDecodeError:
                # Intentar limpiar JSON
                json_str = clean_json_string(json_str)
                return json.loads(json_str)
        else:
            raise ValueError("No se encontr√≥ estructura JSON v√°lida en la respuesta")
    
    def _process_extraction_result(self, raw_result: Dict[str, Any], chunk_text: str, 
                                 chunk_index: int) -> ChunkExtractionResult:
        """Procesa resultado raw usando extractores especializados"""
        
        # Extraer productos con validaci√≥n Pydantic
        productos = self.product_extractor.extract_products(raw_result, chunk_text)
        
        # Extraer informaci√≥n del cliente
        client_info = self.solicitante_extractor.extract_solicitante_info(raw_result, chunk_text)
        
        # Extraer informaci√≥n del evento
        event_info = self.event_extractor.extract_event_info(raw_result, chunk_text)
        
        # Combinar confidence scores
        all_confidence_scores = []
        all_confidence_scores.extend(client_info.get('confidence_scores', []))
        all_confidence_scores.extend(event_info.get('confidence_scores', []))
        for producto in productos:
            all_confidence_scores.append(ExtractionConfidence(
                field_name=f"producto_{producto.nombre}",
                confidence=producto.confidence,
                source="ProductExtractor"
            ))
        
        # Construir resultado final
        result = ChunkExtractionResult(
            # Informaci√≥n del solicitante y empresa
            email=client_info.get('email'),
            email_empresa=client_info.get('email_empresa'),
            nombre_solicitante=client_info.get('nombre_solicitante'),
            nombre_empresa=client_info.get('nombre_empresa'),
            telefono_solicitante=client_info.get('telefono_solicitante'),
            telefono_empresa=client_info.get('telefono_empresa'),
            cargo_solicitante=client_info.get('cargo_solicitante'),
            
            # Informaci√≥n del evento
            fecha=event_info.get('fecha'),
            hora_entrega=event_info.get('hora_entrega'),
            lugar=event_info.get('lugar'),
            tipo_solicitud=event_info.get('tipo_solicitud'),
            
            # Productos validados
            productos=productos,
            
            # Texto original
            texto_original_relevante=raw_result.get('texto_original_relevante'),
            
            # Metadata de debugging
            confidence_scores=all_confidence_scores,
            chunk_index=chunk_index,
            extraction_metadata={
                'strategy': self.strategy.value,
                'debug_mode': self.debug_mode,
                'productos_count': len(productos),
                'client_fields_found': len([k for k, v in client_info.items() if v and k != 'confidence_scores']),
                'event_fields_found': len([k for k, v in event_info.items() if v and k != 'confidence_scores']),
                'raw_result_keys': list(raw_result.keys()) if self.debug_mode else None
            }
        )
        
        return result
    
    def get_extraction_summary(self) -> ExtractionDebugInfo:
        """Retorna resumen de extracci√≥n para debugging"""
        
        # Calcular confidence promedio
        avg_confidence = 0.0
        if self.extraction_stats['chunks_processed'] > 0:
            avg_confidence = 0.8  # Placeholder, calcular de confidence_scores reales
        
        # Determinar calidad de extracci√≥n
        quality = "high"
        if avg_confidence < 0.5:
            quality = "low"
        elif avg_confidence < 0.7:
            quality = "medium"
        
        return ExtractionDebugInfo(
            chunk_count=self.extraction_stats['chunks_processed'],
            total_characters=0,  # Se calcular√° en el uso
            processing_time_seconds=self.extraction_stats['total_processing_time'],
            ai_calls_made=self.extraction_stats['ai_calls_made'],
            retries_attempted=self.extraction_stats['retries_attempted'],
            confidence_summary={'average': avg_confidence},
            extraction_quality=quality
        )


class RFXProcessorService:
    """Service for processing RFX documents from PDF to structured data"""
    
    def __init__(self):
        self.openai_config = get_openai_config()
        self.openai_client = OpenAI(api_key=self.openai_config.api_key)
        self.db_client = get_database_client()
        
        # Validation helpers (legacy - mantenemos para compatibilidad)
        self.email_validator = EmailValidator()
        self.date_validator = DateValidator()
        self.time_validator = TimeValidator()
        
        # üÜï NUEVO SISTEMA MODULAR
        # Inicializar extractor modular con configuraci√≥n din√°mica
        debug_mode = FeatureFlags.eval_debug_enabled()
        extraction_strategy = self._get_extraction_strategy()
        
        self.modular_extractor = ModularRFXExtractor(
            strategy=extraction_strategy,
            debug_mode=debug_mode
        )
        
        # Estad√≠sticas de procesamiento para debugging
        self.processing_stats = {
            'total_documents_processed': 0,
            'chunks_processed': 0,
            'average_confidence': 0.0,
            'fallback_usage_count': 0
        }
        
        logger.info(f"üöÄ RFXProcessorService inicializado - Estrategia: {extraction_strategy.value}, Debug: {debug_mode}")
    
    def _get_extraction_strategy(self) -> ExtractionStrategy:
        """Determina estrategia de extracci√≥n basada en configuraci√≥n"""
        # Puedes a√±adir feature flags para controlar la estrategia
        if FeatureFlags.eval_debug_enabled():
            return ExtractionStrategy.AGGRESSIVE  # M√°s sensible en modo debug
        else:
            return ExtractionStrategy.BALANCED    # Balance por defecto
    
    def process_rfx_document(self, rfx_input: RFXInput, pdf_content: bytes) -> RFXProcessed:
        """
        Main processing pipeline: PDF ‚Üí Text ‚Üí AI Analysis ‚Üí Structured Data
        """
        try:
            logger.info(f"üöÄ Starting RFX processing for: {rfx_input.id}")
            
            # Step 1: Extract text from document
            extracted_text = self._extract_text_from_document(pdf_content)
            logger.info(f"üìÑ Text extracted: {len(extracted_text)} characters")
            
            # Store extracted text in RFXInput for later use
            rfx_input.extracted_content = extracted_text
            
            # Step 2: Process with AI
            raw_data = self._process_with_ai(extracted_text)
            logger.info(f"ü§ñ AI processing completed")
            
            # Step 3: Validate and clean data (enhanced with modular data)
            validated_data = self._validate_and_clean_data(raw_data, rfx_input.id)
            logger.info(f"‚úÖ Data validated successfully")
            
            # Log modular processing statistics if available
            if self.modular_extractor.debug_mode and 'modular_debug_info' in raw_data:
                debug_info = raw_data['modular_debug_info']
                logger.info(f"üìä Modular processing stats: Strategy={debug_info['extraction_strategy']}, Time={debug_info['total_processing_time']:.3f}s")
                
                # Update service-level statistics
                self.processing_stats['total_documents_processed'] += 1
                if debug_info['extraction_summary']['extraction_quality'] == 'fallback':
                    self.processing_stats['fallback_usage_count'] += 1
            
            # Step 3.5: Intelligent RFX evaluation (if enabled)
            evaluation_metadata = self._evaluate_rfx_intelligently(validated_data, rfx_input.id)
            
            # Step 4: Create structured RFX object
            rfx_processed = self._create_rfx_processed(validated_data, rfx_input, evaluation_metadata)
            
            # Step 5: Save to database
            self._save_rfx_to_database(rfx_processed)
            
            # Step 6: RFX processing completed - Proposal generation will be handled separately by user request
            
            logger.info(f"‚úÖ RFX processing completed successfully: {rfx_input.id}")
            return rfx_processed
            
        except Exception as e:
            logger.error(f"‚ùå RFX processing failed for {rfx_input.id}: {e}")
            raise
    
    def _extract_text_from_document(self, pdf_content: bytes) -> str:
        """Extract text content from PDF bytes or other file types"""
        try:
            logger.info(f"üìÑ Starting document text extraction, file size: {len(pdf_content)} bytes")
            
            # Try to detect file type from content
            if pdf_content.startswith(b'%PDF'):
                logger.info("üìÑ Detected PDF file")
                # This is a PDF file
                pdf_file = io.BytesIO(pdf_content)
                pdf_reader = PyPDF2.PdfReader(pdf_file)
                
                text_pages = []
                for i, page in enumerate(pdf_reader.pages):
                    page_text = page.extract_text()
                    text_pages.append(page_text)
                    logger.debug(f"üìÑ Page {i+1} extracted {len(page_text)} characters")
                
                full_text = "\n".join(text_pages)
                
                if not full_text.strip():
                    logger.error("‚ùå No text extracted from PDF")
                    raise ValueError("No text could be extracted from PDF")
                
                logger.info(f"‚úÖ PDF extraction successful: {len(full_text)} total characters")
                logger.debug(f"üìÑ PDF text preview: {full_text[:500]}...")
                return full_text
                
            elif pdf_content.startswith(b'PK'):
                logger.info("üìÑ Detected DOCX file (ZIP-based)")
                # This looks like a DOCX file (ZIP format)
                try:
                    from docx import Document
                    import io
                    
                    # Create a document from the bytes
                    doc_file = io.BytesIO(pdf_content)
                    doc = Document(doc_file)
                    
                    # Extract text from all paragraphs
                    paragraphs = []
                    for paragraph in doc.paragraphs:
                        if paragraph.text.strip():
                            paragraphs.append(paragraph.text.strip())
                    
                    # Extract text from tables
                    table_texts = []
                    for table in doc.tables:
                        for row in table.rows:
                            for cell in row.cells:
                                if cell.text.strip():
                                    table_texts.append(cell.text.strip())
                    
                    # Combine all text
                    all_text = "\n".join(paragraphs + table_texts)
                    
                    if not all_text.strip():
                        logger.error("‚ùå No text extracted from DOCX")
                        raise ValueError("No text could be extracted from DOCX")
                    
                    logger.info(f"‚úÖ DOCX extraction successful: {len(all_text)} total characters")
                    logger.info(f"üìä Extracted {len(paragraphs)} paragraphs and {len(table_texts)} table cells")
                    logger.debug(f"üìÑ DOCX text preview: {all_text[:500]}...")
                    
                    return all_text
                    
                except ImportError:
                    logger.error("‚ùå python-docx not installed, falling back to text decode")
                    # Fallback to text decoding
                except Exception as e:
                    logger.error(f"‚ùå DOCX extraction failed: {e}, falling back to text decode")
            
            # Try to decode as text file
            logger.info("üìÑ Attempting text file decoding")
            try:
                text_content = pdf_content.decode('utf-8')
                if not text_content.strip():
                    raise ValueError("Text file is empty")
                logger.info(f"‚úÖ UTF-8 text extraction successful: {len(text_content)} characters")
                logger.debug(f"üìÑ Text preview: {text_content[:500]}...")
                return text_content
            except UnicodeDecodeError:
                logger.warning("‚ö†Ô∏è UTF-8 decode failed, trying other encodings")
                # Try other encodings
                for encoding in ['latin-1', 'cp1252', 'iso-8859-1']:
                    try:
                        text_content = pdf_content.decode(encoding)
                        if text_content.strip():
                            logger.info(f"‚úÖ Text extraction successful with {encoding}: {len(text_content)} characters")
                            logger.debug(f"üìÑ Text preview: {text_content[:500]}...")
                            return text_content
                    except UnicodeDecodeError:
                        continue
                        
                logger.error("‚ùå All text decoding attempts failed")
                raise ValueError("Unable to decode file content")
            
        except Exception as e:
            logger.error(f"‚ùå Text extraction failed: {e}")
            raise ValueError(f"Failed to extract text from file: {e}")
    
    def _process_with_ai(self, text: str) -> Dict[str, Any]:
        """üÜï REFACTORIZADO: Process extracted text with modular AI system"""
        try:
            start_time = time.time()
            logger.info(f"ü§ñ Starting MODULAR AI processing for text of {len(text)} characters")
            
            if self.modular_extractor.debug_mode:
                logger.debug(f"üìÑ Full text to process: {text[:1000]}..." if len(text) > 1000 else f"üìÑ Full text: {text}")
            
            # Split text into manageable chunks
            chunks = chunk_text(text, max_tokens=1000)
            logger.info(f"üìù Text split into {len(chunks)} chunks")
            
            # üÜï Process each chunk using modular extractor
            chunk_results = []
            for i, chunk in enumerate(chunks):
                logger.info(f"ü§ñ Processing chunk {i+1}/{len(chunks)} ({len(chunk)} characters)")
                
                if self.modular_extractor.debug_mode:
                    logger.debug(f"üìÑ Chunk {i+1} content: {chunk[:300]}...")
                
                # Usar el nuevo sistema modular
                chunk_result = self.modular_extractor.extract_from_chunk(
                    chunk_text=chunk,
                    chunk_index=i,
                    openai_client=self.openai_client,
                    openai_config=self.openai_config
                )
                
                chunk_results.append(chunk_result)
                
                # Log what was found in this chunk (enhanced logging)
                if chunk_result.productos:
                    product_names = [p.nombre for p in chunk_result.productos]
                    confidences = [f"{p.confidence:.2f}" for p in chunk_result.productos]
                    logger.info(f"‚úÖ Chunk {i+1} found {len(chunk_result.productos)} productos: {product_names}")
                    if self.modular_extractor.debug_mode:
                        logger.debug(f"   Confidence scores: {confidences}")
                else:
                    logger.warning(f"‚ö†Ô∏è Chunk {i+1} found NO productos")
                
                # Log extraction metadata if available
                if self.modular_extractor.debug_mode and chunk_result.extraction_metadata:
                    metadata = chunk_result.extraction_metadata
                    logger.debug(f"üìä Chunk {i+1} metadata: Strategy={metadata.get('strategy')}, Fields={metadata.get('client_fields_found', 0)}+{metadata.get('event_fields_found', 0)}")
            
            # üÜï Combine results using enhanced combination logic
            logger.info(f"üîÑ Combining results from {len(chunk_results)} chunks using modular system")
            combined_data = self._combine_modular_chunk_results(chunk_results)
            
            # Update processing statistics
            processing_time = time.time() - start_time
            self.processing_stats['chunks_processed'] += len(chunks)
            
            # Enhanced logging with confidence information
            logger.info(f"‚úÖ MODULAR AI processing completed in {processing_time:.3f}s")
            logger.info(f"üìä Final combined products: {len(combined_data.get('productos', []))} total")
            
            if combined_data.get("productos"):
                product_names = [p['nombre'] for p in combined_data['productos']]
                logger.info(f"üì¶ Product names found: {product_names}")
                
                if self.modular_extractor.debug_mode:
                    avg_confidence = sum(p.get('confidence', 0) for p in combined_data['productos']) / len(combined_data['productos'])
                    logger.debug(f"üìä Average product confidence: {avg_confidence:.3f}")
                    
                    # Log extraction summary
                    extraction_summary = self.modular_extractor.get_extraction_summary()
                    logger.debug(f"üìà Extraction Summary: Quality={extraction_summary.extraction_quality}, AI Calls={extraction_summary.ai_calls_made}, Retries={extraction_summary.retries_attempted}")
            else:
                logger.error(f"‚ùå NO PRODUCTS found in final combined data!")
                self.processing_stats['fallback_usage_count'] += 1
            
            # Store debug information in the combined data if in debug mode
            if self.modular_extractor.debug_mode:
                combined_data['modular_debug_info'] = {
                    'extraction_strategy': self.modular_extractor.strategy.value,
                    'total_processing_time': processing_time,
                    'extraction_summary': self.modular_extractor.get_extraction_summary().dict(),
                    'chunks_metadata': [result.extraction_metadata for result in chunk_results]
                }
            
            logger.debug(f"üì¶ Combined data keys: {list(combined_data.keys())}")
            return combined_data
            
        except Exception as e:
            logger.error(f"‚ùå MODULAR AI processing failed: {e}")
            self.processing_stats['fallback_usage_count'] += 1
            
            # En caso de error, usar el sistema legacy como fallback
            logger.warning(f"üîÑ Fallback to legacy system due to modular processing error")
            return self._process_with_ai_legacy(text)
    
    def _combine_modular_chunk_results(self, chunk_results: List[ChunkExtractionResult]) -> Dict[str, Any]:
        """üÜï Combina resultados de chunks usando el sistema modular mejorado"""
        combined = {
            "email": "",
            "email_empresa": "",
            "nombre_solicitante": "",
            "nombre_empresa": "",
            "telefono_solicitante": "",
            "telefono_empresa": "",
            "cargo_solicitante": "",
            "tipo_solicitud": "",
            "productos": [],
            "hora_entrega": "",
            "fecha": "",
            "lugar": "",
            "texto_original_relevante": ""
        }
        
        # Estad√≠sticas de combinaci√≥n
        confidence_scores = []
        texto_fragments = []
        extraction_metadata = []
        
        logger.info(f"üîÑ Combining {len(chunk_results)} modular chunk results")
        
        for i, chunk_result in enumerate(chunk_results):
            if self.modular_extractor.debug_mode:
                logger.debug(f"üì¶ Processing chunk result {i+1}: {chunk_result.chunk_index}")
            
            # Combinar campos usando prioridad de confidence si est√°n disponibles
            fields_to_combine = [
                ("email", chunk_result.email),
                ("email_empresa", chunk_result.email_empresa),
                ("nombre_solicitante", chunk_result.nombre_solicitante),
                ("nombre_empresa", chunk_result.nombre_empresa),
                ("telefono_solicitante", chunk_result.telefono_solicitante),
                ("telefono_empresa", chunk_result.telefono_empresa),
                ("cargo_solicitante", chunk_result.cargo_solicitante),
                ("tipo_solicitud", chunk_result.tipo_solicitud),
                ("hora_entrega", chunk_result.hora_entrega),
                ("fecha", chunk_result.fecha),
                ("lugar", chunk_result.lugar)
            ]
            
            for field_name, field_value in fields_to_combine:
                if field_value and not combined[field_name]:
                    combined[field_name] = field_value
                    
                    if self.modular_extractor.debug_mode:
                        # Buscar confidence score para este campo
                        field_confidence = next(
                            (cs.confidence for cs in chunk_result.confidence_scores 
                             if cs.field_name == field_name), 
                            0.8
                        )
                        logger.debug(f"üìß Found {field_name} in chunk {i+1}: {field_value} (confidence: {field_confidence:.2f})")
            
            # Combinar productos con validaci√≥n Pydantic ya aplicada
            if chunk_result.productos:
                productos_count = len(chunk_result.productos)
                
                # Convertir ProductExtraction a dict para compatibilidad
                for producto in chunk_result.productos:
                    producto_dict = {
                        "nombre": producto.nombre,
                        "cantidad": producto.cantidad,
                        "unidad": producto.unidad
                    }
                    
                    # A√±adir confidence si est√° en modo debug
                    if self.modular_extractor.debug_mode:
                        producto_dict["confidence"] = producto.confidence
                    
                    combined["productos"].append(producto_dict)
                
                logger.debug(f"üì¶ Added {productos_count} productos from chunk {i+1}")
            
            # Recopilar texto relevante
            if chunk_result.texto_original_relevante:
                texto_fragments.append(f"Chunk {i+1}: {chunk_result.texto_original_relevante}")
            
            # Recopilar confidence scores
            confidence_scores.extend(chunk_result.confidence_scores)
            
            # Recopilar metadata de extracci√≥n
            if chunk_result.extraction_metadata:
                extraction_metadata.append({
                    'chunk_index': chunk_result.chunk_index,
                    'metadata': chunk_result.extraction_metadata
                })
        
        # Combinar fragmentos de texto
        if texto_fragments:
            combined["texto_original_relevante"] = " | ".join(texto_fragments)
        
        # A√±adir metadata de debugging si est√° habilitado
        if self.modular_extractor.debug_mode:
            combined["modular_extraction_metadata"] = {
                'total_confidence_scores': len(confidence_scores),
                'chunks_with_products': len([cr for cr in chunk_results if cr.productos]),
                'average_products_per_chunk': len(combined["productos"]) / len(chunk_results) if chunk_results else 0,
                'extraction_metadata': extraction_metadata,
                'combined_confidence_scores': [cs.dict() for cs in confidence_scores]
            }
        
        # Log final combined result
        logger.info(f"‚úÖ Combined result: {len(combined['productos'])} productos total")
        
        if self.modular_extractor.debug_mode:
            # Calcular estad√≠sticas de confidence
            if confidence_scores:
                avg_confidence = sum(cs.confidence for cs in confidence_scores) / len(confidence_scores)
                logger.debug(f"üìä Overall average confidence: {avg_confidence:.3f}")
            
            # Log breakdown by field type with empresa information
            solicitante_fields = [k for k in combined.keys() if combined[k] and k in ['email', 'nombre_solicitante', 'telefono_solicitante', 'cargo_solicitante']]
            empresa_fields = [k for k in combined.keys() if combined[k] and k in ['email_empresa', 'nombre_empresa', 'telefono_empresa']]
            event_fields = [k for k in combined.keys() if combined[k] and k in ['fecha', 'hora_entrega', 'lugar']]
            logger.info(f"üìã Fields found - Solicitante: {len(solicitante_fields)}, Empresa: {len(empresa_fields)}, Event: {len(event_fields)}, Products: {len(combined['productos'])}")
            
            # Log specific empresa and solicitante info if found
            if empresa_fields:
                empresa_details = [f"{k}: {combined[k]}" for k in empresa_fields if combined[k]]
                logger.info(f"üè¢ Empresa info: {', '.join(empresa_details)}")
            
            if solicitante_fields:
                solicitante_details = [f"{k}: {combined[k]}" for k in solicitante_fields if combined[k]]
                logger.info(f"üë§ Solicitante info: {', '.join(solicitante_details)}")
        
        return combined
    
    def _process_with_ai_legacy(self, text: str) -> Dict[str, Any]:
        """üîß Sistema legacy como fallback para compatibilidad"""
        logger.warning(f"‚ö†Ô∏è Using LEGACY processing system as fallback")
        
        try:
            # Split text into manageable chunks
            chunks = chunk_text(text, max_tokens=1000)
            logger.info(f"üìù Legacy: Text split into {len(chunks)} chunks")
            
            # Process each chunk using legacy method
            chunk_results = []
            for i, chunk in enumerate(chunks):
                result = self._extract_info_from_chunk_legacy(chunk)
                chunk_results.append(result)
            
            # Combine results using legacy method
            combined_data = self._combine_chunk_results_legacy(chunk_results)
            return combined_data
            
        except Exception as e:
            logger.error(f"‚ùå Legacy processing also failed: {e}")
            # Return minimal fallback data
            return {
                "email": "",
                "nombre_solicitante": "",
                "productos": [],
                "hora_entrega": "",
                "fecha": "",
                "lugar": "",
                "texto_original_relevante": ""
            }
    
    def _extract_info_from_chunk_legacy(self, chunk: str, max_retries: int = 2) -> Dict[str, Any]:
        """üîß LEGACY: Extract information from a single text chunk using OpenAI"""
        
        system_prompt = """Eres un especialista experto en an√°lisis de documentos RFX, catering y eventos 
        corporativos con especializaci√≥n en identificaci√≥n de requisitos espec√≠ficos del cliente.

Tu trabajo es extraer informaci√≥n EXACTA del texto proporcionado, prestando especial atenci√≥n a distinguir entre:
- DESCRIPCI√ìN GENERAL del servicio vs REQUISITOS ESPEC√çFICOS del cliente
- INFORMACI√ìN FACTUAL vs PREFERENCIAS/RESTRICCIONES del solicitante

REGLAS FUNDAMENTALES:
- Si NO encuentras un dato espec√≠fico, usa null (no inventes informaci√≥n)
- Extrae solo lo que est√° expl√≠citamente mencionado en el texto
- Para requisitos, busca INSTRUCCIONES ESPEC√çFICAS, no descripciones generales
- Mant√©n el contexto original para validaci√≥n posterior
- Responde √öNICAMENTE con JSON v√°lido, sin texto adicional

DEFINICIONES CR√çTICAS:
- REQUIREMENTS = Instrucciones espec√≠ficas, restricciones, preferencias o condiciones especiales mencionadas por el cliente
- DESCRIPCI√ìN = Informaci√≥n general sobre qu√© tipo de servicio se necesita
- DATOS EMPRESARIALES = Informaci√≥n de contacto y organizacional"""
        
        user_prompt = f"""Analiza cuidadosamente este texto de un documento de catering/evento y extrae la siguiente informaci√≥n en formato JSON:

        {{
        "nombre_empresa": "EMPRESA: nombre de la compa√±√≠a/organizaci√≥n (ej: Chevron, Microsoft). Si solo hay email como juan@chevron.com, extrae 'Chevron' del dominio (null si no se encuentra)",
        "email_empresa": "EMPRESA: email corporativo general de la empresa. NO el email personal del solicitante (null si no se encuentra)",
        "nombre_solicitante": "PERSONA: nombre y apellido de la persona individual que hace la solicitud (null si no se encuentra)",
        "email_solicitante": "PERSONA: email personal/de trabajo de la persona espec√≠fica que solicita (null si no se encuentra)",
        "telefono_solicitante": "PERSONA: n√∫mero telef√≥nico personal de la persona que solicita (null si no se encuentra)",
        "telefono_empresa": "EMPRESA: n√∫mero telef√≥nico principal/general de la empresa (null si no se encuentra)",
        "cargo_solicitante": "PERSONA: puesto/cargo que ocupa la persona en la empresa (null si no se encuentra)",
        "tipo_solicitud": "tipo de solicitud de catering/evento (null si no se encuentra)",
        "productos": [
            {{
            "nombre": "nombre exacto del producto/servicio",
            "cantidad": n√∫mero_entero,
            "unidad": "unidades/personas/kg/litros/etc"
            }}
        ],
        "fecha": "fecha de entrega en formato YYYY-MM-DD (null si no se encuentra)",
        "hora_entrega": "hora de entrega en formato HH:MM (null si no se encuentra)",
        "lugar": "direcci√≥n completa o ubicaci√≥n del evento (null si no se encuentra)",
        "requirements": "INSTRUCCIONES ESPEC√çFICAS, preferencias o restricciones mencionadas por el cliente. NO incluir descripci√≥n general del servicio (null si no se encuentra)",
        "texto_original_relevante": "fragmento del texto donde encontraste la informaci√≥n principal",
        "confidence_score": n√∫mero_decimal_entre_0_y_1_indicando_confianza_en_extracci√≥n
        }}

        ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        üéØ INSTRUCCIONES CR√çTICAS PARA REQUIREMENTS
        ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

        **QU√â SON REQUIREMENTS (Buscar estas frases/patrones):**
        ‚úÖ Instrucciones espec√≠ficas: "Necesito que...", "Debe incluir...", "Es importante que..."
        ‚úÖ Preferencias expl√≠citas: "Prefiero...", "Me gustar√≠a...", "Ser√≠a ideal..."
        ‚úÖ Restricciones: "No queremos...", "Evitar...", "Sin...", "Prohibido..."
        ‚úÖ Condiciones especiales: "Solo si...", "Siempre que...", "Mientras..."
        ‚úÖ Experiencia/calificaciones: "Con experiencia de...", "Certificado en...", "Que tenga..."
        ‚úÖ Presupuesto/calidad: "Econ√≥mico", "Premium", "M√°ximo $...", "Presupuesto de..."
        ‚úÖ Timing espec√≠fico: "Antes de las...", "Exactamente a...", "No despu√©s de..."
        ‚úÖ Especificaciones t√©cnicas: "Vegetariano", "Sin gluten", "Halal", "Org√°nico"
        ‚úÖ Requisitos de servicio: "Con meseros", "Auto-servicio", "Servicio completo"
        ‚úÖ Log√≠stica espec√≠fica: "Montaje 2 horas antes", "Incluir vajilla", "Desmontaje incluido"

        **QU√â NO SON REQUIREMENTS (No incluir):**
        ‚ùå Descripci√≥n general: "Necesitamos catering" (esto es tipo_solicitud)
        ‚ùå Informaci√≥n b√°sica: "Para 50 personas" (esto va en productos)
        ‚ùå Datos de contacto: "Llamar a Juan" (esto va en otros campos)
        ‚ùå Ubicaci√≥n b√°sica: "En nuestra oficina" (esto va en lugar)
        ‚ùå Fecha/hora est√°ndar: "El viernes 15" (esto va en fecha)

        **EJEMPLOS PR√ÅCTICOS:**

        üìã **EJEMPLO 1 - CATERING CORPORATIVO:**
        Texto: "Necesitamos catering para 60 personas el viernes. Queremos opciones vegetarianas y que no incluya frutos secos por alergias. Preferimos meseros uniformados y montaje 2 horas antes del evento."

        ‚úÖ CORRECTO requirements: "Opciones vegetarianas, no incluir frutos secos por alergias, meseros uniformados, montaje 2 horas antes del evento"
        ‚ùå INCORRECTO: "Catering para 60 personas el viernes" (eso va en productos, cantidad, fecha)

        üìã **EJEMPLO 2 - EVENTO CORPORATIVO:**
        Texto: "Organizamos evento de fin de a√±o para 100 empleados. El presupuesto es m√°ximo $2000. Solo proveedores con m√°s de 5 a√±os de experiencia."

        ‚úÖ CORRECTO requirements: "Presupuesto m√°ximo $2000, solo proveedores con m√°s de 5 a√±os de experiencia"
        ‚ùå INCORRECTO: "Evento de fin de a√±o para 100 empleados" (eso va en tipo_solicitud y productos)

        üìã **EJEMPLO 3 - SIN REQUIREMENTS ESPEC√çFICOS:**
        Texto: "Hola, necesitamos catering para reuni√≥n de 30 personas ma√±ana a las 12pm en sala de juntas."

        ‚úÖ CORRECTO requirements: null (solo informaci√≥n b√°sica, sin instrucciones espec√≠ficas)

        ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        üîç VALIDACI√ìN DE CONTEXTO PARA REQUIREMENTS
        ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

        **ANTES DE INCLUIR COMO REQUIREMENT, PREG√öNTATE:**
        1. ¬øEs esto una instrucci√≥n espec√≠fica del cliente?
        2. ¬øVa m√°s all√° de la informaci√≥n b√°sica del servicio?
        3. ¬øAfecta c√≥mo debe ejecutarse el servicio?
        4. ¬øEs una preferencia, restricci√≥n o condici√≥n especial?

        **SI RESPONDES S√ç A ALGUNA ‚Üí ES REQUIREMENT**
        **SI TODAS SON NO ‚Üí NO ES REQUIREMENT**

        ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

        REGLAS CR√çTICAS PARA EMPRESA vs SOLICITANTE:
        - EMPRESA = compa√±√≠a/organizaci√≥n que solicita el servicio
        - SOLICITANTE = persona individual dentro de la empresa
        - Si ves "sofia.elena@chevron.com" ‚Üí nombre_empresa="Chevron", email_solicitante="sofia.elena@chevron.com"

        INSTRUCCIONES ESPEC√çFICAS PARA PRODUCTOS:
        - Busca CUALQUIER tipo de comida, bebida o servicio de catering
        - Incluye cantidades: n√∫meros seguidos de "personas", "pax", "unidades", "kg", "litros"
        - Si solo encuentras "para X personas" sin productos espec√≠ficos, usa "Catering para X personas"
        - SIEMPRE incluye al menos un producto si el texto menciona comida o catering

        CONFIDENCE SCORE (0.0 - 1.0):
        - 0.9-1.0: Informaci√≥n muy clara y expl√≠cita
        - 0.7-0.8: Informaci√≥n clara con interpretaci√≥n m√≠nima
        - 0.5-0.6: Informaci√≥n impl√≠cita o requiere interpretaci√≥n
        - 0.3-0.4: Informaci√≥n ambigua o parcial
        - 0.0-0.2: Informaci√≥n muy incierta o extrapolada

        TEXTO A ANALIZAR:
        {chunk}

        Responde SOLO con el JSON solicitado:"""
        
        try:
            # üîç DEBUG: Log the chunk being processed
            logger.info(f"ü§ñ Processing chunk of {len(chunk)} characters")
            logger.debug(f"üìÑ Chunk content preview: {chunk[:200]}...")
            
            # Retry logic with exponential backoff
            response = self._call_openai_with_retry(
                model=self.openai_config.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=self.openai_config.temperature,
                max_tokens=self.openai_config.max_tokens,
                timeout=30  # 30s timeout as specified
            )
            
            output = response.choices[0].message.content.strip()
            
            # üîç DEBUG: Log AI response
            logger.info(f"ü§ñ AI Response received: {len(output)} characters")
            logger.debug(f"üìù AI Response preview: {output[:300]}...")
            
            # Extract JSON from response
            json_start = output.find('{')
            json_end = output.rfind('}') + 1
            
            if json_start >= 0 and json_end > json_start:
                json_str = output[json_start:json_end]
                
                # Try to parse JSON with retries
                for attempt in range(max_retries):
                    try:
                        parsed_data = json.loads(json_str)
                        
                        # üîç DEBUG: Log successfully parsed data
                        logger.info(f"‚úÖ Successfully parsed AI response on attempt {attempt + 1}")
                        logger.debug(f"üì¶ Parsed data: {parsed_data}")
                        
                        return parsed_data
                    except json.JSONDecodeError as e:
                        logger.warning(f"‚ö†Ô∏è JSON parse attempt {attempt + 1} failed: {e}")
                        if attempt < max_retries - 1:
                            json_str = clean_json_string(json_str)
                            logger.info(f"üîÑ Retrying with cleaned JSON...")
                        else:
                            logger.error(f"‚ùå Failed to parse JSON after {max_retries} attempts")
                            logger.error(f"üîç Raw JSON: {json_str}")
                            break
            else:
                logger.error(f"‚ùå No valid JSON structure found in AI response")
                logger.error(f"üîç Full response: {output}")
            
            # Return empty structure if parsing fails
            logger.warning(f"‚ö†Ô∏è Returning empty result due to parsing failure")
            return self._get_empty_extraction_result()
            
        except Exception as e:
            logger.error(f"‚ùå OpenAI API call failed: {e}")
            return self._get_empty_extraction_result()
    
    def _combine_chunk_results_legacy(self, chunk_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """üîß LEGACY: Combine results from multiple text chunks into a single data structure"""
        combined = {
            "email": "",
            "nombre_solicitante": "",
            "productos": [],
            "hora_entrega": "",
            "fecha": "",
            "lugar": "",
            "texto_original_relevante": ""
        }
        
        # üîç DEBUG: Log chunk combination process
        logger.info(f"üîÑ Combining results from {len(chunk_results)} chunks")
        
        texto_fragments = []
        
        for i, result in enumerate(chunk_results):
            logger.debug(f"üì¶ Processing chunk {i+1} result: {result}")
            
            # Take the first non-empty value for each field
            if result.get("email") and not combined["email"]:
                combined["email"] = result["email"]
                logger.debug(f"üìß Found email in chunk {i+1}: {result['email']}")
                
            if result.get("nombre_solicitante") and not combined["nombre_solicitante"]:
                combined["nombre_solicitante"] = result["nombre_solicitante"]
                logger.debug(f"üë§ Found solicitante in chunk {i+1}: {result['nombre_solicitante']}")
                
            if result.get("hora_entrega") and not combined["hora_entrega"]:
                combined["hora_entrega"] = result["hora_entrega"]
                logger.debug(f"üïê Found hora_entrega in chunk {i+1}: {result['hora_entrega']}")
                
            if result.get("fecha") and not combined["fecha"]:
                combined["fecha"] = result["fecha"]
                logger.debug(f"üìÖ Found fecha in chunk {i+1}: {result['fecha']}")
                
            if result.get("lugar") and not combined["lugar"]:
                combined["lugar"] = result["lugar"]
                logger.debug(f"üìç Found lugar in chunk {i+1}: {result['lugar']}")
            
            # Combine products from all chunks
            if result.get("productos"):
                productos_count = len(result["productos"])
                combined["productos"].extend(result["productos"])
                logger.debug(f"üì¶ Added {productos_count} productos from chunk {i+1}")
            
            # Collect text fragments
            if result.get("texto_original_relevante"):
                texto_fragments.append(f"Chunk {i+1}: {result['texto_original_relevante']}")
        
        # Combine text fragments
        if texto_fragments:
            combined["texto_original_relevante"] = " | ".join(texto_fragments)
        
        # üîç DEBUG: Log final combined result
        logger.info(f"‚úÖ Combined result: {len(combined['productos'])} productos total")
        logger.debug(f"üì¶ Final combined data: {combined}")
        
        return combined
    
    # ============================================================================
    # üÜï MVP: FUNCIONES DE VALIDACI√ìN DE REQUIREMENTS
    # ============================================================================
    
    def _validate_basic_requirements(self, requirements: str, confidence: float) -> Dict[str, Any]:
        """Validaci√≥n m√≠nima para MVP de requirements"""
        if not requirements:
            return {
                'validated_requirements': None,
                'adjusted_confidence': 0.0,
                'validation_issues': [],
                'needs_review': False
            }
        
        issues = []
        adjusted_confidence = confidence
        cleaned_requirements = requirements.strip()
        
        # Validaci√≥n 1: Longitud apropiada
        if len(cleaned_requirements) < 5:
            issues.append("requirements_too_short")
            adjusted_confidence *= 0.3
            logger.warning(f"‚ö†Ô∏è Requirements muy cortos: '{cleaned_requirements}'")
        
        if len(cleaned_requirements) > 1500:
            issues.append("requirements_too_long")
            adjusted_confidence *= 0.5
            cleaned_requirements = cleaned_requirements[:1500] + "..."
            logger.warning(f"‚ö†Ô∏è Requirements truncados por longitud")
        
        # Validaci√≥n 2: Detectar si son demasiado gen√©ricos
        generic_phrases = [
            "necesitamos catering", "queremos evento", "solicito servicio",
            "buen servicio", "servicio de calidad", "excelente atenci√≥n"
        ]
        
        requirements_lower = cleaned_requirements.lower()
        has_generic = any(phrase in requirements_lower for phrase in generic_phrases)
        
        # Detectar si hay requirements espec√≠ficos
        specific_indicators = [
            "preferimos", "debe", "sin", "con experiencia", "m√≠nimo", "m√°ximo",
            "a√±os", "alergia", "vegetariano", "vegano", "halal", "kosher",
            "presupuesto", "horario", "personal", "certificaci√≥n"
        ]
        
        has_specific = any(indicator in requirements_lower for indicator in specific_indicators)
        
        if has_generic and not has_specific:
            adjusted_confidence *= 0.4
            issues.append("seems_too_generic")
            logger.warning(f"‚ö†Ô∏è Requirements parecen demasiado gen√©ricos: {confidence:.2f} ‚Üí {adjusted_confidence:.2f}")
        
        # Validaci√≥n 3: Detectar si contiene informaci√≥n de empresa/contacto (error de extracci√≥n)
        contact_indicators = ["@", "tel:", "email:", "tel√©fono", "contacto:"]
        if any(indicator in requirements_lower for indicator in contact_indicators):
            adjusted_confidence *= 0.3
            issues.append("contains_contact_info")
            logger.warning(f"‚ö†Ô∏è Requirements contienen informaci√≥n de contacto")
        
        # Validaci√≥n 4: Score demasiado bajo necesita revisi√≥n
        needs_review = adjusted_confidence < 0.4
        
        if needs_review:
            logger.warning(f"‚ö†Ô∏è Requirements necesitan revisi√≥n - Confidence: {adjusted_confidence:.2f}")
        
        return {
            'validated_requirements': cleaned_requirements,
            'adjusted_confidence': round(adjusted_confidence, 3),
            'validation_issues': issues,
            'needs_review': needs_review,
            'original_confidence': confidence
        }
    
    def _log_requirements_extraction(self, rfx_id: str, validation_result: Dict) -> None:
        """Log detallado para analizar y mejorar requirements extraction"""
        log_data = {
            'rfx_id': rfx_id,
            'timestamp': datetime.now(),
            'requirements_extracted': validation_result.get('validated_requirements'),
            'confidence_score': validation_result.get('adjusted_confidence', 0.0),
            'original_confidence': validation_result.get('original_confidence', 0.0),
            'validation_issues': validation_result.get('validation_issues', []),
            'needs_review': validation_result.get('needs_review', False),
            'requirements_length': len(validation_result.get('validated_requirements', '') or '')
        }
        
        # Log b√°sico para monitoring
        logger.info(f"üìã Requirements extracted for {rfx_id}: "
                   f"Length={log_data['requirements_length']}, "
                   f"Confidence={log_data['confidence_score']:.3f}, "
                   f"Issues={len(log_data['validation_issues'])}")
        
        # Log detallado en debug
        if logger.isEnabledFor(logging.DEBUG):
            logger.debug(f"üìä Requirements log data: {log_data}")
        
        # TODO: En el futuro, guardar en tabla de logs para an√°lisis
        # self.requirements_logs.insert(log_data)
    
    def _validate_and_clean_data(self, raw_data: Dict[str, Any], rfx_id: str) -> Dict[str, Any]:
        """Validate and clean extracted data with fallbacks for invalid values"""
        # üîç DEBUG: Log validation process
        logger.info(f"üîç Starting validation for RFX: {rfx_id}")
        logger.debug(f"üì¶ Raw data to validate: {raw_data}")
        
        cleaned_data = raw_data.copy()
        validation_status = {
            "email_valid": False,
            "fecha_valid": False,
            "hora_valid": False,
            "has_original_data": True
        }
        
        # Validate and clean email - PRESERVE original even if invalid
        email = raw_data.get("email", "").strip()
        if email:
            if self.email_validator.validate(email):
                cleaned_data["email"] = email
                validation_status["email_valid"] = True
                logger.info(f"‚úÖ Email validated successfully: {email}")
            else:
                # Keep original email but mark as invalid
                cleaned_data["email"] = email
                validation_status["email_valid"] = False
                logger.warning(f"‚ö†Ô∏è Email format invalid but preserved: {email}")
        else:
            # Only use fallback if completely empty
            cleaned_data["email"] = "cliente@example.com"
            validation_status["email_valid"] = False
            validation_status["has_original_data"] = False
            logger.warning(f"‚ö†Ô∏è No email found, using fallback")
        
        # Validate and clean date - PRESERVE original even if invalid
        fecha = raw_data.get("fecha", "").strip()
        if fecha:
            if self.date_validator.validate(fecha):
                cleaned_data["fecha"] = fecha
                validation_status["fecha_valid"] = True
                logger.info(f"‚úÖ Date validated successfully: {fecha}")
            else:
                # Try to preserve original date format
                cleaned_data["fecha"] = fecha
                validation_status["fecha_valid"] = False
                logger.warning(f"‚ö†Ô∏è Date format may be invalid but preserved: {fecha}")
        else:
            # Only use fallback if completely empty
            cleaned_data["fecha"] = datetime.now().strftime('%Y-%m-%d')
            validation_status["fecha_valid"] = False
            logger.warning(f"‚ö†Ô∏è No date found, using current date")
        
        # Validate and clean time - PRESERVE original even if invalid
        hora = raw_data.get("hora_entrega", "").strip()
        if hora:
            if self.time_validator.validate(hora):
                cleaned_data["hora_entrega"] = hora
                validation_status["hora_valid"] = True
                logger.info(f"‚úÖ Time validated successfully: {hora}")
            else:
                # Keep original time but mark as invalid
                cleaned_data["hora_entrega"] = hora
                validation_status["hora_valid"] = False
                logger.warning(f"‚ö†Ô∏è Time format may be invalid but preserved: {hora}")
        else:
            # Only use fallback if completely empty
            cleaned_data["hora_entrega"] = "12:00"
            validation_status["hora_valid"] = False
            logger.warning(f"‚ö†Ô∏è No time found, using default")
        
        # Clean and validate client name - MORE PERMISSIVE
        nombre_solicitante = raw_data.get("nombre_solicitante", "").strip()
        if nombre_solicitante and nombre_solicitante.lower() not in ["null", "none", ""]:
            cleaned_data["nombre_solicitante"] = nombre_solicitante.title()
            logger.info(f"‚úÖ Solicitante name processed: {nombre_solicitante}")
        else:
            cleaned_data["nombre_solicitante"] = f"Solicitante-{rfx_id}"
            validation_status["has_original_data"] = False
            logger.warning(f"‚ö†Ô∏è No client name found, using fallback")
        
        # Clean place - MORE PERMISSIVE
        lugar = raw_data.get("lugar", "").strip()
        if lugar and lugar.lower() not in ["null", "none", ""]:
            cleaned_data["lugar"] = lugar
            logger.info(f"‚úÖ Location preserved: {lugar}")
        else:
            cleaned_data["lugar"] = "Ubicaci√≥n por definir"
            logger.warning(f"‚ö†Ô∏è No location found, using fallback")
        
        # Validate products - VERY PERMISSIVE
        productos = raw_data.get("productos", [])
        logger.info(f"üîç Validating {len(productos)} products from AI extraction")
        logger.debug(f"üì¶ Raw products data: {productos}")
        
        if not productos:
            logger.error(f"‚ùå No products array found in AI response")
            logger.debug(f"üì¶ Full raw_data keys: {list(raw_data.keys())}")
            raise ValueError("No se encontraron productos en el documento")
        
        # Clean and validate each product - VERY PERMISSIVE
        cleaned_productos = []
        for i, producto in enumerate(productos):
            logger.debug(f"üîç Processing product {i+1}: {producto}")
            
            # Handle different formats of product data
            if isinstance(producto, dict):
                nombre = None
                cantidad = 1
                unidad = "unidades"
                
                # Try to extract name from various possible keys
                for name_key in ["nombre", "name", "product", "producto", "item"]:
                    if producto.get(name_key):
                        nombre = str(producto[name_key]).strip()
                        break
                
                # Try to extract quantity
                for qty_key in ["cantidad", "quantity", "qty", "count", "numero"]:
                    if producto.get(qty_key):
                        try:
                            cantidad = max(1, int(float(producto[qty_key])))
                            break
                        except (ValueError, TypeError):
                            logger.warning(f"‚ö†Ô∏è Invalid quantity for product {i+1}: {producto.get(qty_key)}")
                            continue
                
                # Try to extract unit
                for unit_key in ["unidad", "unit", "units", "medida"]:
                    if producto.get(unit_key):
                        unidad = str(producto[unit_key]).strip().lower()
                        break
                
                # Accept product if it has any meaningful name
                if nombre and len(nombre.strip()) > 0 and nombre.lower() not in ["null", "none", "", "undefined"]:
                    cleaned_producto = {
                        "nombre": nombre.title(),
                        "cantidad": cantidad,
                        "unidad": unidad or "unidades"
                    }
                    cleaned_productos.append(cleaned_producto)
                    logger.info(f"‚úÖ Product {i+1} accepted: {cleaned_producto}")
                else:
                    logger.warning(f"‚ö†Ô∏è Product {i+1} rejected - invalid name: {nombre}")
                    
            elif isinstance(producto, str):
                # Handle product as simple string
                nombre = producto.strip()
                if nombre and len(nombre) > 0 and nombre.lower() not in ["null", "none", "", "undefined"]:
                    cleaned_producto = {
                        "nombre": nombre.title(),
                        "cantidad": 1,
                        "unidad": "unidades"
                    }
                    cleaned_productos.append(cleaned_producto)
                    logger.info(f"‚úÖ Product {i+1} accepted as string: {cleaned_producto}")
                else:
                    logger.warning(f"‚ö†Ô∏è Product {i+1} rejected - invalid string: {nombre}")
            else:
                logger.warning(f"‚ö†Ô∏è Product {i+1} skipped - unrecognized format: {type(producto)}")
        
        logger.info(f"üìä Product validation completed: {len(cleaned_productos)} valid products from {len(productos)} raw products")
        
        if not cleaned_productos:
            logger.error(f"‚ùå No valid products could be processed from AI extraction")
            logger.error(f"üîç Original products data: {productos}")
            # Create a fallback product to avoid complete failure
            logger.warning(f"‚ö†Ô∏è Creating fallback product to prevent complete failure")
            cleaned_productos = [{
                "nombre": "Producto No Especificado",
                "cantidad": 1,
                "unidad": "unidades"
            }]
            logger.info(f"‚úÖ Fallback product created: {cleaned_productos[0]}")
        
        cleaned_data["productos"] = cleaned_productos
        
        # ‚ú® PRESERVE: Datos de empresa sin validaci√≥n (son opcionales)
        empresa_fields = ["nombre_empresa", "email_empresa", "telefono_empresa", "telefono_solicitante", "cargo_solicitante"]
        for field in empresa_fields:
            if field in raw_data and raw_data[field]:
                cleaned_data[field] = str(raw_data[field]).strip()
                logger.info(f"‚úÖ Empresa field preserved: {field} = {cleaned_data[field]}")
            else:
                cleaned_data[field] = ""
                logger.debug(f"üìù Empresa field empty: {field}")
        
        # üÜï MVP: Validate and clean requirements
        requirements = raw_data.get("requirements", "")
        requirements_confidence = raw_data.get("requirements_confidence", 0.0)
        
        if requirements and requirements.strip():
            # Aplicar validaci√≥n b√°sica
            validation_result = self._validate_basic_requirements(
                requirements.strip(), 
                float(requirements_confidence) if requirements_confidence else 0.0
            )
            
            # Guardar resultados validados
            cleaned_data["requirements"] = validation_result['validated_requirements']
            cleaned_data["requirements_confidence"] = validation_result['adjusted_confidence']
            
            # Actualizar validation status
            validation_status["requirements_valid"] = not validation_result['needs_review']
            validation_status["requirements_issues"] = validation_result['validation_issues']
            
            # Log extraction para mejora continua
            self._log_requirements_extraction(rfx_id, validation_result)
            
            logger.info(f"‚úÖ Requirements processed: confidence={validation_result['adjusted_confidence']:.3f}, "
                       f"issues={len(validation_result['validation_issues'])}")
        else:
            # No hay requirements extra√≠dos
            cleaned_data["requirements"] = None
            cleaned_data["requirements_confidence"] = 0.0
            validation_status["requirements_valid"] = True  # V√°lido que no haya requirements
            validation_status["requirements_issues"] = []
            logger.debug(f"üìù No requirements extracted for RFX: {rfx_id}")
        
        # Add validation metadata
        cleaned_data["validation_metadata"] = validation_status
        
        # üîç DEBUG: Log final validation result
        logger.info(f"‚úÖ Validation completed for {len(cleaned_productos)} products")
        logger.info(f"üìä Validation status: {validation_status}")
        logger.debug(f"üì¶ Final cleaned data: {cleaned_data}")
        
        return cleaned_data
    
    def _create_rfx_processed(self, validated_data: Dict[str, Any], rfx_input: RFXInput, evaluation_metadata: Optional[Dict[str, Any]] = None) -> RFXProcessed:
        """Create RFXProcessed object from validated data and evaluation results"""
        try:
            # üîç DEBUG: Log object creation
            logger.info(f"üî® Creating RFXProcessed object for: {rfx_input.id}")
            
            # Convert productos to ProductoRFX objects (map Spanish to English fields)
            productos = [
                ProductoRFX(
                    product_name=p["nombre"],
                    quantity=p["cantidad"],
                    unit=p["unidad"]
                )
                for p in validated_data["productos"]
            ]
            
            # Prepare enhanced metadata including empresa data
            metadata = {
                "original_rfx_id": rfx_input.id,  # Preserve original string ID for frontend
                "texto_original_length": len(rfx_input.extracted_content or ""),
                "productos_count": len(productos),
                "processing_version": "2.1",  # Upgraded version with intelligent evaluation
                "validation_status": validated_data.get("validation_metadata", {}),
                "texto_original_relevante": validated_data.get("texto_original_relevante", ""),
                "ai_extraction_quality": "high" if validated_data.get("validation_metadata", {}).get("has_original_data", False) else "fallback",
                # ‚ú® A√ëADIR: Datos de empresa en metadatos para acceso del frontend
                "nombre_empresa": validated_data.get("nombre_empresa", ""),
                "email_empresa": validated_data.get("email_empresa", ""),
                "telefono_empresa": validated_data.get("telefono_empresa", ""),
                "telefono_solicitante": validated_data.get("telefono_solicitante", ""),
                "cargo_solicitante": validated_data.get("cargo_solicitante", "")
            }
            
            # Integrate intelligent evaluation metadata if available
            if evaluation_metadata:
                metadata["intelligent_evaluation"] = evaluation_metadata
                
                # Log evaluation integration
                if evaluation_metadata.get('evaluation_enabled'):
                    if 'evaluation_error' in evaluation_metadata:
                        logger.warning(f"‚ö†Ô∏è Evaluation error included in metadata for {rfx_input.id}")
                    else:
                        score = evaluation_metadata.get('execution_summary', {}).get('consolidated_score')
                        quality = evaluation_metadata.get('execution_summary', {}).get('overall_quality')
                        domain = evaluation_metadata.get('domain_detection', {}).get('primary_domain')
                        logger.info(f"üìä Evaluation metadata integrated - Score: {score}, Quality: {quality}, Domain: {domain}")
                else:
                    logger.debug(f"üîß Evaluation disabled - metadata reflects feature flag status")
            else:
                logger.debug(f"‚ÑπÔ∏è No evaluation metadata provided for {rfx_input.id}")
            
            # üîç DEBUG: Log metadata
            logger.debug(f"üìä Metadata prepared: {metadata}")
            
            # Generate UUID for database, but keep original ID in metadata
            from uuid import uuid4
            rfx_uuid = uuid4()
            
            rfx_processed = RFXProcessed(
                id=rfx_uuid,
                rfx_type=rfx_input.rfx_type,
                title=f"RFX Request - {validated_data.get('nombre_solicitante', 'Unknown')} - {rfx_input.rfx_type.value if hasattr(rfx_input.rfx_type, 'value') else str(rfx_input.rfx_type)}",
                location=validated_data["lugar"],
                delivery_date=validated_data["fecha"],
                delivery_time=validated_data["hora_entrega"],
                status=RFXStatus.IN_PROGRESS,
                original_pdf_text=rfx_input.extracted_content,
                requested_products=[p.dict() for p in productos] if productos else [],
                metadata_json=metadata,
                received_at=datetime.now(),
                
                # Legacy/extracted fields for compatibility
                email=validated_data["email"],
                requester_name=validated_data["nombre_solicitante"],
                company_name=validated_data.get("nombre_empresa", ""),
                products=productos,  # productos is already a list of RFXProductRequest objects
                
                # üÜï MVP: Requirements espec√≠ficos del cliente
                requirements=validated_data.get("requirements"),
                requirements_confidence=validated_data.get("requirements_confidence")
            )
            
            # üîç DEBUG: Log successful creation
            logger.info(f"‚úÖ RFXProcessed object created successfully")
            empresa_info = metadata or {}
            empresa_nombre = empresa_info.get("nombre_empresa", "No especificada")
            original_id = empresa_info.get("original_rfx_id", str(rfx_processed.id))
            logger.info(f"üì¶ RFX Object: Original ID={original_id}, UUID={rfx_processed.id}, Solicitante={rfx_processed.requester_name}, Empresa={empresa_nombre}, Productos={len(rfx_processed.products)}")
            
            # Log empresa details if available
            if empresa_info.get("nombre_empresa"):
                logger.info(f"üè¢ Empresa: {empresa_info.get('nombre_empresa')} | Email: {empresa_info.get('email_empresa', 'N/A')} | Tel: {empresa_info.get('telefono_empresa', 'N/A')}")
            
            # Log solicitante details if available  
            if empresa_info.get("telefono_solicitante") or empresa_info.get("cargo_solicitante"):
                logger.info(f"üë§ Solicitante: {rfx_processed.requester_name} | Tel: {empresa_info.get('telefono_solicitante', 'N/A')} | Cargo: {empresa_info.get('cargo_solicitante', 'N/A')}")
            
            # üÜï MVP: Log requirements info if available
            if rfx_processed.requirements:
                req_preview = rfx_processed.requirements[:100] + "..." if len(rfx_processed.requirements) > 100 else rfx_processed.requirements
                logger.info(f"üìã Requirements: '{req_preview}' | Confidence: {rfx_processed.requirements_confidence:.3f}")
            else:
                logger.debug(f"üìù No requirements extracted for RFX")
                
            return rfx_processed
            
        except Exception as e:
            logger.error(f"‚ùå Failed to create RFXProcessed object: {e}")
            raise
    
    def _save_rfx_to_database(self, rfx_processed: RFXProcessed) -> None:
        """Save processed RFX to database V2.0 with normalized structure"""
        try:
            # Extract company and requester information from metadata
            metadata = rfx_processed.metadata_json or {}
            
            # 1. Create or find company
            # Clean email to meet database constraints (no spaces allowed)
            email_empresa = metadata.get("email_empresa", "")
            if email_empresa:
                email_empresa = email_empresa.replace(" ", "").strip()
                if not email_empresa or "@" not in email_empresa:
                    email_empresa = None
            else:
                email_empresa = None
                
            company_data = {
                "name": metadata.get("nombre_empresa", "Unknown Company"),
                "email": email_empresa,
                "phone": metadata.get("telefono_empresa")
            }
            company_record = self.db_client.insert_company(company_data)
            
            # 2. Create or find requester
            # Clean requester email to meet database constraints
            requester_email = rfx_processed.email or ""
            if requester_email:
                requester_email = requester_email.replace(" ", "").strip()
                if not requester_email or "@" not in requester_email:
                    requester_email = None
            else:
                requester_email = None
                
            requester_data = {
                "company_id": company_record.get("id"),
                "name": rfx_processed.requester_name or "Unknown Requester",
                "email": requester_email,
                "phone": metadata.get("telefono_solicitante"),
                "position": metadata.get("cargo_solicitante")
            }
            requester_record = self.db_client.insert_requester(requester_data)
            
            # 3. Prepare RFX data for database (V2.0 schema)
            rfx_data = {
                "id": str(rfx_processed.id),
                "company_id": company_record.get("id"),
                "requester_id": requester_record.get("id"),
                "rfx_type": rfx_processed.rfx_type.value if hasattr(rfx_processed.rfx_type, 'value') else str(rfx_processed.rfx_type),
                "title": f"RFX Request - {rfx_processed.rfx_type}",
                "location": rfx_processed.location,
                "delivery_date": rfx_processed.delivery_date.isoformat() if rfx_processed.delivery_date else None,
                "delivery_time": rfx_processed.delivery_time.isoformat() if rfx_processed.delivery_time else None,
                "status": rfx_processed.status.value if hasattr(rfx_processed.status, 'value') else str(rfx_processed.status),
                "original_pdf_text": rfx_processed.original_pdf_text,
                "requested_products": rfx_processed.requested_products or [],
                "received_at": rfx_processed.received_at.isoformat() if rfx_processed.received_at else None,
                "metadata_json": rfx_processed.metadata_json,
                
                # üÜï MVP: Requirements espec√≠ficos del cliente
                "requirements": rfx_processed.requirements,
                "requirements_confidence": rfx_processed.requirements_confidence
            }
            
            # 4. Insert RFX data
            rfx_record = self.db_client.insert_rfx(rfx_data)
            
            # 5. Insert structured products if available
            if rfx_processed.products:
                structured_products = []
                for product in rfx_processed.products:
                    product_data = {
                        "product_name": product.product_name if hasattr(product, 'product_name') else product.nombre,
                        "quantity": product.quantity if hasattr(product, 'quantity') else product.cantidad,
                        "unit": product.unit if hasattr(product, 'unit') else product.unidad,
                        "estimated_unit_price": getattr(product, 'estimated_unit_price', None),
                        "notes": f"Extracted from RFX processing"
                    }
                    structured_products.append(product_data)
                
                self.db_client.insert_rfx_products(rfx_record["id"], structured_products)
                logger.info(f"‚úÖ {len(structured_products)} structured products saved")
            
            # 6. Create history event
            history_event = {
                "rfx_id": str(rfx_processed.id),
                "event_type": "rfx_processed",
                "description": f"RFX processed successfully with {len(rfx_processed.products or [])} products",
                "new_values": {
                    "company_name": company_data["name"],
                    "requester_name": requester_data["name"],
                    "processing_version": metadata.get("processing_version", "2.0"),
                    "original_rfx_id": metadata.get("original_rfx_id"),
                    "product_count": len(rfx_processed.products or [])
                },
                "performed_by": "system_ai"
            }
            self.db_client.insert_rfx_history(history_event)
            
            logger.info(f"‚úÖ RFX saved to database V2.0: {rfx_processed.id}")
            
        except Exception as e:
            logger.error(f"‚ùå Failed to save RFX to database: {e}")
            raise
    
    # REMOVED: _generate_proposal_automatically
    # La generaci√≥n de propuestas ahora se maneja por separado cuando el usuario lo solicite
    # mediante el endpoint /api/proposals/generate despu√©s de revisar datos y establecer costos
    
    def _map_rfx_data_for_proposal(self, rfx_data_raw: Dict[str, Any]) -> Dict[str, Any]:
        """
        üîß Mapea estructura BD V2.0 (ingl√©s, normalizada) ‚Üí ProposalGenerationService (espa√±ol, plana)
        
        Args:
            rfx_data_raw: Datos del RFX desde BD V2.0 con companies, requesters, etc.
            
        Returns:
            Dict con estructura esperada por ProposalGenerationService
        """
        try:
            # Extraer datos de BD V2.0
            company_data = rfx_data_raw.get("companies", {}) or {}
            requester_data = rfx_data_raw.get("requesters", {}) or {}
            requested_products = rfx_data_raw.get("requested_products", [])
            metadata = rfx_data_raw.get("metadata_json", {}) or {}
            
            # Mapear productos con fallback a metadata si es necesario
            productos_mapped = []
            if requested_products:
                # Usar productos estructurados de BD
                for producto in requested_products:
                    productos_mapped.append({
                        "nombre": producto.get("product_name", producto.get("nombre", "Producto")),
                        "cantidad": producto.get("quantity", producto.get("cantidad", 1)),
                        "unidad": producto.get("unit", producto.get("unidad", "unidades"))
                    })
            else:
                # Fallback a metadata si no hay productos estructurados
                metadata_productos = metadata.get("productos", [])
                for producto in metadata_productos:
                    if isinstance(producto, dict):
                        productos_mapped.append({
                            "nombre": producto.get("nombre", "Producto"),
                            "cantidad": producto.get("cantidad", 1),
                            "unidad": producto.get("unidad", "unidades")
                        })
            
            # Si a√∫n no hay productos, crear uno por defecto
            if not productos_mapped:
                productos_mapped = [{
                    "nombre": "Servicio de Catering",
                    "cantidad": 1,
                    "unidad": "servicio"
                }]
            
            # Estructura mapeada esperada por ProposalGenerationService
            mapped_data = {
                # ‚úÖ Mapear informaci√≥n del cliente (combinando empresa + solicitante)
                "clientes": {
                    "nombre": requester_data.get("name", metadata.get("nombre_solicitante", "Cliente")),
                    "email": requester_data.get("email", metadata.get("email", "")),
                    "empresa": company_data.get("name", metadata.get("nombre_empresa", "")),
                    "telefono": requester_data.get("phone", metadata.get("telefono_solicitante", "")),
                    "cargo": requester_data.get("position", metadata.get("cargo_solicitante", "")),
                    # Informaci√≥n adicional de empresa
                    "email_empresa": company_data.get("email", metadata.get("email_empresa", "")),
                    "telefono_empresa": company_data.get("phone", metadata.get("telefono_empresa", ""))
                },
                
                # ‚úÖ Mapear productos
                "productos": productos_mapped,
                
                # ‚úÖ Mapear informaci√≥n del evento
                "lugar": rfx_data_raw.get("location", metadata.get("lugar", "Por definir")),
                "fecha_entrega": rfx_data_raw.get("delivery_date", metadata.get("fecha", "")),
                "hora_entrega": rfx_data_raw.get("delivery_time", metadata.get("hora_entrega", "")),
                "tipo": rfx_data_raw.get("rfx_type", metadata.get("tipo_solicitud", "catering")),
                
                # ‚úÖ Informaci√≥n adicional
                "id": str(rfx_data_raw.get("id", "")),
                "title": rfx_data_raw.get("title", ""),
                "received_at": rfx_data_raw.get("received_at", ""),
                "texto_original_relevante": metadata.get("texto_original_relevante", "")
            }
            
            logger.debug(f"üîß Datos mapeados para propuesta: Cliente={mapped_data['clientes']['nombre']}, "
                        f"Empresa={mapped_data['clientes']['empresa']}, Productos={len(mapped_data['productos'])}")
            
            return mapped_data
            
        except Exception as e:
            logger.error(f"‚ùå Error mapeando datos para propuesta: {e}")
            # Fallback con estructura m√≠nima
            return {
                "clientes": {"nombre": "Cliente", "email": ""},
                "productos": [{"nombre": "Servicio de Catering", "cantidad": 1, "unidad": "servicio"}],
                "lugar": "Por definir",
                "fecha_entrega": "",
                "hora_entrega": "",
                "tipo": "catering"
            }
    
    # REMOVED: _log_proposal_generation_event and _log_proposal_generation_error
    # Estas funciones ya no se necesitan porque la generaci√≥n de propuestas es manual

    def _call_openai_with_retry(self, max_retries: int = 3, **kwargs) -> Any:
        """Call OpenAI API with exponential backoff retry logic"""
        for attempt in range(max_retries):
            try:
                logger.info(f"üîÑ OpenAI API call attempt {attempt + 1}/{max_retries}")
                response = self.openai_client.chat.completions.create(**kwargs)
                logger.info(f"‚úÖ OpenAI API call successful on attempt {attempt + 1}")
                return response
            except Exception as e:
                wait_time = (2 ** attempt) + 1  # Exponential backoff: 2s, 3s, 5s
                logger.warning(f"‚ö†Ô∏è OpenAI API call failed on attempt {attempt + 1}: {e}")
                
                if attempt < max_retries - 1:
                    logger.info(f"üîÑ Retrying in {wait_time} seconds...")
                    time.sleep(wait_time)
                else:
                    logger.error(f"‚ùå OpenAI API call failed after {max_retries} attempts")
                    raise

    def _evaluate_rfx_intelligently(self, validated_data: Dict[str, Any], rfx_id: str) -> Dict[str, Any]:
        """
        Ejecuta evaluaci√≥n inteligente del RFX usando el orquestador.
        
        Args:
            validated_data: Datos del RFX ya validados y limpios
            rfx_id: ID √∫nico del RFX para logging
            
        Returns:
            Dict con metadata de evaluaci√≥n para incluir en RFXProcessed
        """
        # Verificar si la evaluaci√≥n est√° habilitada
        if not FeatureFlags.evals_enabled():
            logger.debug(f"üîß Evaluaci√≥n inteligente deshabilitada por feature flag para RFX: {rfx_id}")
            return {
                'evaluation_enabled': False,
                'reason': 'Feature flag disabled'
            }
        
        try:
            # Import lazy para evitar circular imports y mejorar startup time
            from backend.services.evaluation_orchestrator import evaluate_rfx_intelligently
            
            logger.info(f"üîç Iniciando evaluaci√≥n inteligente para RFX: {rfx_id}")
            start_time = datetime.now()
            
            # Ejecutar evaluaci√≥n completa
            eval_result = evaluate_rfx_intelligently(validated_data)
            execution_time = (datetime.now() - start_time).total_seconds()
            
            # Log resultados principales
            domain = eval_result['domain_detection']['primary_domain']
            confidence = eval_result['domain_detection']['confidence']
            score = eval_result['consolidated_score']
            quality = eval_result['execution_summary']['overall_quality']
            
            logger.info(f"‚úÖ Evaluaci√≥n completada para {rfx_id} - Dominio: {domain} ({confidence:.3f}), Score: {score:.3f} ({quality}), Tiempo: {execution_time:.3f}s")
            
            # Extraer recomendaciones cr√≠ticas y de alta prioridad
            critical_recommendations = [
                {
                    'title': rec['title'],
                    'description': rec['description'],
                    'priority': rec['priority'],
                    'category': rec['category'],
                    'type': rec['type']
                }
                for rec in eval_result['recommendations'] 
                if rec.get('priority') in ['critical', 'high']
            ]
            
            # Log recomendaciones cr√≠ticas
            if critical_recommendations:
                logger.warning(f"‚ö†Ô∏è RFX {rfx_id} tiene {len(critical_recommendations)} recomendaciones cr√≠ticas/alta prioridad")
                for rec in critical_recommendations[:2]:  # Log solo las primeras 2
                    logger.warning(f"   - [{rec['priority'].upper()}] {rec['title']}")
            
            # Estructura optimizada de metadata (solo datos esenciales)
            evaluation_metadata = {
                'evaluation_enabled': True,
                'execution_summary': {
                    'consolidated_score': score,
                    'overall_quality': quality,
                    'execution_time_seconds': round(execution_time, 3),
                    'evaluators_executed': eval_result['execution_summary']['evaluators_executed'],
                    'timestamp': datetime.now().isoformat()
                },
                'domain_detection': {
                    'primary_domain': domain,
                    'confidence': confidence,
                    'secondary_domains': eval_result['domain_detection'].get('secondary_domains', [])
                },
                'evaluation_scores': {
                    'generic_score': eval_result['generic_evaluation']['score'],
                    'domain_specific_score': eval_result['domain_specific_evaluation']['score'],
                    'generic_evaluators_count': eval_result['generic_evaluation']['count'],
                    'domain_specific_evaluators_count': eval_result['domain_specific_evaluation']['count']
                },
                'critical_recommendations': critical_recommendations,
                'recommendations_summary': {
                    'total_count': len(eval_result['recommendations']),
                    'critical_count': len([r for r in eval_result['recommendations'] if r.get('priority') == 'critical']),
                    'high_priority_count': len([r for r in eval_result['recommendations'] if r.get('priority') == 'high']),
                    'by_category': {
                        cat: len([r for r in eval_result['recommendations'] if r.get('category') == cat])
                        for cat in set(r.get('category', 'other') for r in eval_result['recommendations'])
                    }
                }
            }
            
            # Log opcional de debugging si est√° habilitado
            if FeatureFlags.eval_debug_enabled():
                logger.debug(f"üîç Evaluaci√≥n detallada para {rfx_id}: {evaluation_metadata}")
            
            return evaluation_metadata
            
        except Exception as e:
            logger.error(f"‚ùå Error ejecutando evaluaci√≥n inteligente para {rfx_id}: {str(e)}")
            
            # NO fallar el procesamiento principal - solo log el error
            # Retornar metadata de error para debugging
            return {
                'evaluation_enabled': True,
                'evaluation_error': {
                    'error_message': str(e),
                    'error_type': type(e).__name__,
                    'timestamp': datetime.now().isoformat(),
                    'rfx_id': rfx_id
                },
                'execution_summary': {
                    'consolidated_score': None,
                    'overall_quality': 'evaluation_failed',
                    'execution_time_seconds': 0.0,
                    'evaluators_executed': 0
                }
            }

    def _get_empty_extraction_result(self) -> Dict[str, Any]:
        """Return empty result structure for failed extractions"""
        return {
            "email": "",
            "nombre_solicitante": "",
            "productos": [],
            "hora_entrega": "",
            "fecha": "",
            "lugar": "",
            "texto_original_relevante": ""
        }
    
    # ============================================================================
    # üîç M√âTODOS DE DEBUGGING Y ESTAD√çSTICAS
    # ============================================================================
    
    def get_processing_statistics(self) -> Dict[str, Any]:
        """üÜï Retorna estad√≠sticas de procesamiento para monitoring y debugging"""
        base_stats = self.processing_stats.copy()
        
        # A√±adir estad√≠sticas del extractor modular
        if hasattr(self, 'modular_extractor'):
            extraction_summary = self.modular_extractor.get_extraction_summary()
            base_stats.update({
                'modular_extractor_stats': extraction_summary.dict(),
                'modular_extraction_quality': extraction_summary.extraction_quality,
                'modular_chunks_processed': extraction_summary.chunk_count,
                'modular_ai_calls': extraction_summary.ai_calls_made,
                'modular_retries': extraction_summary.retries_attempted
            })
        
        # Calcular ratios y m√©tricas derivadas
        if base_stats['total_documents_processed'] > 0:
            base_stats['fallback_usage_ratio'] = base_stats['fallback_usage_count'] / base_stats['total_documents_processed']
            base_stats['average_chunks_per_document'] = base_stats['chunks_processed'] / base_stats['total_documents_processed']
        else:
            base_stats['fallback_usage_ratio'] = 0.0
            base_stats['average_chunks_per_document'] = 0.0
        
        return base_stats
    
    def reset_processing_statistics(self) -> None:
        """üÜï Resetea estad√≠sticas de procesamiento"""
        self.processing_stats = {
            'total_documents_processed': 0,
            'chunks_processed': 0,
            'average_confidence': 0.0,
            'fallback_usage_count': 0
        }
        
        if hasattr(self, 'modular_extractor'):
            # Resetear estad√≠sticas del extractor modular
            self.modular_extractor.extraction_stats = {
                'chunks_processed': 0,
                'ai_calls_made': 0,
                'retries_attempted': 0,
                'total_processing_time': 0.0
            }
        
        logger.info("üìä Processing statistics reset")
    
    def get_debug_mode_status(self) -> Dict[str, Any]:
        """üÜï Retorna estado del modo debug y configuraci√≥n actual"""
        return {
            'debug_mode_enabled': getattr(self.modular_extractor, 'debug_mode', False),
            'extraction_strategy': getattr(self.modular_extractor, 'strategy', ExtractionStrategy.BALANCED).value,
            'feature_flags': {
                'evals_enabled': FeatureFlags.evals_enabled(),
                'eval_debug_enabled': FeatureFlags.eval_debug_enabled(),
                'meta_prompting_enabled': FeatureFlags.meta_prompting_enabled() if hasattr(FeatureFlags, 'meta_prompting_enabled') else False,
                'vertical_agent_enabled': FeatureFlags.vertical_agent_enabled() if hasattr(FeatureFlags, 'vertical_agent_enabled') else False
            },
            'processing_version': '2.1_modular',
            'available_extractors': ['ProductExtractor', 'ClientExtractor', 'EventExtractor'],
            'template_manager_initialized': hasattr(self.modular_extractor, 'template_manager')
        }
