"""
ðŸŽ¯ RFX Processor Service - Core business logic for RFX document processing
Extracts logic from rfx_webhook.py and improves it with better practices

REFACTORIZADO: Sistema modular con validaciones Pydantic, templates Jinja2,
modo debug con confidence scores y arquitectura extensible.
"""
import io
import re
import json
import PyPDF2
import time
from datetime import datetime
from typing import List, Dict, Any, Optional, Union, Tuple
from enum import Enum
from openai import OpenAI

# Pydantic imports for robust validation
from pydantic import BaseModel, Field, validator, ValidationError

# Jinja2 imports for dynamic templates
from jinja2 import Template, Environment, BaseLoader

from backend.models.rfx_models import (
    RFXInput, RFXProcessed, RFXProductRequest, RFXType, RFXStatus,
    CompanyModel, RequesterModel, RFXHistoryEvent,
    # Legacy aliases for backwards compatibility
    ProductoRFX, TipoRFX, EstadoRFX
)
from backend.models.proposal_models import ProposalRequest, ProposalNotes
from backend.core.config import get_openai_config
from backend.core.database import get_database_client
from backend.utils.validators import EmailValidator, DateValidator, TimeValidator
from backend.utils.text_utils import chunk_text, clean_json_string
from backend.core.feature_flags import FeatureFlags

import logging

logger = logging.getLogger(__name__)


# ============================================================================
# ðŸŽ¯ MODELOS PYDANTIC PARA VALIDACIÃ“N ESTRUCTURADA
# ============================================================================

class ExtractionConfidence(BaseModel):
    """Modelo para tracking de confidence scores en extracciones"""
    field_name: str = Field(..., description="Nombre del campo extraÃ­do")
    confidence: float = Field(..., ge=0.0, le=1.0, description="Confidence score 0-1")
    source: str = Field(..., description="Fuente de la extracciÃ³n (AI, manual, fallback)")
    metadata: Dict[str, Any] = Field(default_factory=dict, description="Metadata adicional")

class ProductExtraction(BaseModel):
    """Modelo Pydantic para validaciÃ³n robusta de productos extraÃ­dos"""
    nombre: str = Field(..., min_length=1, max_length=200, description="Nombre del producto")
    cantidad: int = Field(..., ge=1, le=10000, description="Cantidad del producto")
    unidad: str = Field(..., min_length=1, max_length=50, description="Unidad de medida")
    confidence: float = Field(default=0.8, ge=0.0, le=1.0, description="Confidence score")
    
    @validator('nombre')
    def validate_nombre(cls, v):
        """Validador para nombre de producto"""
        if v.lower() in ['null', 'none', 'undefined', '']:
            raise ValueError('Nombre de producto no puede ser nulo o vacÃ­o')
        return v.strip().title()
    
    @validator('unidad')
    def validate_unidad(cls, v):
        """Validador para unidad de medida"""
        valid_units = ['unidades', 'personas', 'kg', 'litros', 'porciones', 'servicios', 'pax']
        v_clean = v.lower().strip()
        if v_clean not in valid_units:
            # Auto-map common variants
            unit_mapping = {
                'unidad': 'unidades', 'pcs': 'unidades', 'units': 'unidades',
                'persona': 'personas', 'ppl': 'personas', 'people': 'personas',
                'litro': 'litros', 'l': 'litros', 'lts': 'litros',
                'porcion': 'porciones', 'servicio': 'servicios'
            }
            v_clean = unit_mapping.get(v_clean, 'unidades')
        return v_clean

class ChunkExtractionResult(BaseModel):
    """Modelo para resultado de extracciÃ³n por chunk con confidence tracking"""
    email: Optional[str] = Field(None, description="Email extraÃ­do")
    email_empresa: Optional[str] = Field(None, description="Email de la empresa")
    nombre_solicitante: Optional[str] = Field(None, description="Nombre del solicitante")
    nombre_empresa: Optional[str] = Field(None, description="Nombre de la empresa") 
    telefono_solicitante: Optional[str] = Field(None, description="TelÃ©fono del solicitante")
    telefono_empresa: Optional[str] = Field(None, description="TelÃ©fono de la empresa")
    cargo_solicitante: Optional[str] = Field(None, description="Cargo del solicitante")
    tipo_solicitud: Optional[str] = Field(None, description="Tipo de solicitud")
    productos: List[ProductExtraction] = Field(default_factory=list, description="Lista de productos")
    fecha: Optional[str] = Field(None, description="Fecha de entrega")
    hora_entrega: Optional[str] = Field(None, description="Hora de entrega")
    lugar: Optional[str] = Field(None, description="Lugar del evento")
    texto_original_relevante: Optional[str] = Field(None, description="Texto original relevante")
    
    # Campos de confidence y debugging
    confidence_scores: List[ExtractionConfidence] = Field(default_factory=list)
    extraction_metadata: Dict[str, Any] = Field(default_factory=dict)
    chunk_index: int = Field(default=0, description="Ãndice del chunk procesado")

class ExtractionDebugInfo(BaseModel):
    """InformaciÃ³n de debug para el modo debug"""
    chunk_count: int
    total_characters: int
    processing_time_seconds: float
    ai_calls_made: int
    retries_attempted: int
    confidence_summary: Dict[str, float]
    extraction_quality: str  # 'high', 'medium', 'low', 'fallback'

# ============================================================================
# ðŸ­ SISTEMA MODULAR DE EXTRACTORES
# ============================================================================

class ExtractionStrategy(Enum):
    """Estrategias de extracciÃ³n disponibles"""
    CONSERVATIVE = "conservative"  # Alta precisiÃ³n, baja sensibilidad
    BALANCED = "balanced"         # Balance entre precisiÃ³n y sensibilidad 
    AGGRESSIVE = "aggressive"     # Alta sensibilidad, menor precisiÃ³n

class BaseExtractor:
    """Extractor base con funcionalidad comÃºn"""
    
    def __init__(self, debug_mode: bool = False):
        self.debug_mode = debug_mode
        self.confidence_threshold = 0.7
        
    def calculate_confidence(self, extracted_value: Any, raw_text: str) -> float:
        """Calcula confidence score basado en heurÃ­sticas"""
        if not extracted_value:
            return 0.0
        
        # HeurÃ­sticas bÃ¡sicas para confidence
        confidence = 0.5  # Base confidence
        
        # Boost si el valor extraÃ­do aparece directamente en el texto
        if isinstance(extracted_value, str) and extracted_value.lower() in raw_text.lower():
            confidence += 0.3
            
        # Boost por longitud/format validity 
        if isinstance(extracted_value, str) and len(extracted_value) > 3:
            confidence += 0.1
            
        return min(confidence, 1.0)

class ProductExtractor(BaseExtractor):
    """Extractor especializado para productos con validaciÃ³n robusta"""
    
    def extract_products(self, chunk_data: Dict[str, Any], raw_text: str) -> List[ProductExtraction]:
        """Extrae y valida productos de los datos del chunk"""
        productos_raw = chunk_data.get("productos", [])
        productos_validated = []
        
        if self.debug_mode:
            logger.debug(f"ðŸ” ProductExtractor: procesando {len(productos_raw)} productos raw")
        
        for i, producto_raw in enumerate(productos_raw):
            try:
                # Intentar crear ProductExtraction con validaciÃ³n Pydantic
                if isinstance(producto_raw, dict):
                    # Extract and clean product data
                    nombre = self._extract_product_name(producto_raw)
                    cantidad = self._extract_product_quantity(producto_raw)
                    unidad = self._extract_product_unit(producto_raw)
                    
                    if nombre:  # Solo proceder si tenemos nombre vÃ¡lido
                        confidence = self.calculate_confidence(nombre, raw_text)
                        
                        producto = ProductExtraction(
                            nombre=nombre,
                            cantidad=cantidad,
                            unidad=unidad,
                            confidence=confidence
                        )
                        productos_validated.append(producto)
                        
                        if self.debug_mode:
                            logger.debug(f"âœ… Producto {i+1} validado: {producto.nombre} (confidence: {confidence:.2f})")
                    else:
                        if self.debug_mode:
                            logger.warning(f"âš ï¸ Producto {i+1} rechazado: nombre invÃ¡lido")
                            
                elif isinstance(producto_raw, str) and producto_raw.strip():
                    # Handle simple string products
                    confidence = self.calculate_confidence(producto_raw, raw_text)
                    producto = ProductExtraction(
                        nombre=producto_raw.strip(),
                        cantidad=1,
                        unidad="unidades",
                        confidence=confidence
                    )
                    productos_validated.append(producto)
                    
            except ValidationError as e:
                if self.debug_mode:
                    logger.warning(f"âš ï¸ ValidaciÃ³n fallida para producto {i+1}: {e}")
                continue
        
        return productos_validated
    
    def _extract_product_name(self, producto_dict: Dict[str, Any]) -> Optional[str]:
        """Extrae nombre del producto desde mÃºltiples claves posibles"""
        name_keys = ["nombre", "name", "product", "producto", "item"]
        for key in name_keys:
            if key in producto_dict and producto_dict[key]:
                name = str(producto_dict[key]).strip()
                if name and name.lower() not in ["null", "none", "undefined", ""]:
                    return name
        return None
    
    def _extract_product_quantity(self, producto_dict: Dict[str, Any]) -> int:
        """Extrae cantidad del producto con fallback a 1"""
        qty_keys = ["cantidad", "quantity", "qty", "count", "numero"]
        for key in qty_keys:
            if key in producto_dict and producto_dict[key]:
                try:
                    return max(1, int(float(producto_dict[key])))
                except (ValueError, TypeError):
                    continue
        return 1
    
    def _extract_product_unit(self, producto_dict: Dict[str, Any]) -> str:
        """Extrae unidad del producto con fallback a 'unidades'"""
        unit_keys = ["unidad", "unit", "units", "medida"]
        for key in unit_keys:
            if key in producto_dict and producto_dict[key]:
                return str(producto_dict[key]).strip().lower()
        return "unidades"

class SolicitanteExtractor(BaseExtractor):
    """Extractor especializado para informaciÃ³n del solicitante"""
    
    def extract_solicitante_info(self, chunk_data: Dict[str, Any], raw_text: str) -> Dict[str, Any]:
        """Extrae informaciÃ³n del solicitante con confidence tracking"""
        client_info = {}
        confidence_scores = []
        
        # Email extraction with confidence
        email = chunk_data.get("email_solicitante") or chunk_data.get("email")
        if email:
            confidence = self.calculate_confidence(email, raw_text)
            # Additional email validation boost
            if "@" in email and "." in email:
                confidence += 0.2
            client_info["email"] = email
            confidence_scores.append(ExtractionConfidence(
                field_name="email", confidence=min(confidence, 1.0), 
                source="AI", metadata={"format_valid": "@" in email}
            ))
        
        # Company email
        email_empresa = chunk_data.get("email_empresa")
        if email_empresa:
            confidence = self.calculate_confidence(email_empresa, raw_text)
            if "@" in email_empresa and "." in email_empresa:
                confidence += 0.2
            client_info["email_empresa"] = email_empresa
            confidence_scores.append(ExtractionConfidence(
                field_name="email_empresa", confidence=min(confidence, 1.0),
                source="AI", metadata={"format_valid": "@" in email_empresa}
            ))
        
        # Solicitante name extraction
        nombre_solicitante = chunk_data.get("nombre_solicitante")
        if nombre_solicitante:
            confidence = self.calculate_confidence(nombre_solicitante, raw_text)
            client_info["nombre_solicitante"] = nombre_solicitante
            confidence_scores.append(ExtractionConfidence(
                field_name="nombre_solicitante", confidence=confidence, source="AI"
            ))
        
        # Company name extraction
        nombre_empresa = chunk_data.get("nombre_empresa")
        if nombre_empresa:
            confidence = self.calculate_confidence(nombre_empresa, raw_text)
            client_info["nombre_empresa"] = nombre_empresa
            confidence_scores.append(ExtractionConfidence(
                field_name="nombre_empresa", confidence=confidence, source="AI"
            ))
        
        # Phone numbers
        for phone_field in ["telefono_solicitante", "telefono_empresa"]:
            phone = chunk_data.get(phone_field)
            if phone:
                confidence = self.calculate_confidence(phone, raw_text)
                client_info[phone_field] = phone
                confidence_scores.append(ExtractionConfidence(
                    field_name=phone_field, confidence=confidence, source="AI"
                ))
        
        # Position/cargo
        cargo = chunk_data.get("cargo_solicitante")
        if cargo:
            confidence = self.calculate_confidence(cargo, raw_text)
            client_info["cargo_solicitante"] = cargo
            confidence_scores.append(ExtractionConfidence(
                field_name="cargo_solicitante", confidence=confidence, source="AI"
            ))
        
        client_info["confidence_scores"] = confidence_scores
        return client_info

class EventExtractor(BaseExtractor):
    """Extractor especializado para informaciÃ³n del evento"""
    
    def extract_event_info(self, chunk_data: Dict[str, Any], raw_text: str) -> Dict[str, Any]:
        """Extrae informaciÃ³n del evento con validaciÃ³n"""
        event_info = {}
        confidence_scores = []
        
        # Date extraction
        fecha = chunk_data.get("fecha")
        if fecha:
            confidence = self.calculate_confidence(fecha, raw_text)
            # Boost confidence for date-like patterns
            if re.match(r'\d{4}-\d{2}-\d{2}', str(fecha)):
                confidence += 0.2
            event_info["fecha"] = fecha
            confidence_scores.append(ExtractionConfidence(
                field_name="fecha", confidence=min(confidence, 1.0), source="AI"
            ))
        
        # Time extraction  
        hora_entrega = chunk_data.get("hora_entrega")
        if hora_entrega:
            confidence = self.calculate_confidence(hora_entrega, raw_text)
            # Boost confidence for time-like patterns
            if re.match(r'\d{1,2}:\d{2}', str(hora_entrega)):
                confidence += 0.2
            event_info["hora_entrega"] = hora_entrega
            confidence_scores.append(ExtractionConfidence(
                field_name="hora_entrega", confidence=min(confidence, 1.0), source="AI"
            ))
        
        # Location extraction
        lugar = chunk_data.get("lugar")
        if lugar:
            confidence = self.calculate_confidence(lugar, raw_text)
            event_info["lugar"] = lugar
            confidence_scores.append(ExtractionConfidence(
                field_name="lugar", confidence=confidence, source="AI"
            ))
        
        # Request type
        tipo_solicitud = chunk_data.get("tipo_solicitud")
        if tipo_solicitud:
            confidence = self.calculate_confidence(tipo_solicitud, raw_text)
            event_info["tipo_solicitud"] = tipo_solicitud
            confidence_scores.append(ExtractionConfidence(
                field_name="tipo_solicitud", confidence=confidence, source="AI"
            ))
        
        event_info["confidence_scores"] = confidence_scores
        return event_info

# ============================================================================
# ðŸŽ¨ SISTEMA DE TEMPLATES JINJA2 PARA PROMPTS DINÃMICOS
# ============================================================================

class PromptTemplateManager:
    """Manager para templates de prompts usando Jinja2"""
    
    def __init__(self):
        self.jinja_env = Environment(loader=BaseLoader())
        self._init_templates()
    
    def _init_templates(self):
        """Inicializa templates predefinidos"""
        self.templates = {
            'system_prompt': self._get_system_prompt_template(),
            'extraction_prompt': self._get_extraction_prompt_template(),
            'debug_prompt': self._get_debug_prompt_template()
        }
    
    def _get_system_prompt_template(self) -> str:
        """Template del system prompt con configuraciÃ³n dinÃ¡mica"""
        return """Eres un especialista experto en anÃ¡lisis de documentos RFX, catering y eventos corporativos. 
Tu trabajo es extraer informaciÃ³n EXACTA del texto proporcionado.

CONFIGURACIÃ“N:
- Estrategia: {{ strategy }}
- Modo Debug: {{ debug_mode }}
- Confidence Threshold: {{ confidence_threshold }}

REGLAS IMPORTANTES:
- Si NO encuentras un dato especÃ­fico, usa null (no inventes informaciÃ³n)
- Extrae solo lo que estÃ¡ explÃ­citamente mencionado en el texto
- Para productos, incluye TODAS las cantidades y unidades mencionadas
- Preserva el texto original relevante para referencia
- Responde ÃšNICAMENTE con JSON vÃ¡lido, sin texto adicional

{% if debug_mode %}
MODO DEBUG ACTIVADO:
- Incluye confidence scores para cada campo extraÃ­do
- AÃ±ade metadata de debugging en el JSON
- Registra heurÃ­sticas usadas para extraction decisions
{% endif %}"""

    def _get_extraction_prompt_template(self) -> str:
        """Template del prompt de extracciÃ³n con campos dinÃ¡micos"""
        return """Analiza cuidadosamente este texto de un documento de catering/evento y extrae la siguiente informaciÃ³n en formato JSON:

{
{% for field in required_fields %}
  "{{ field.name }}": "{{ field.description }} ({{ field.format if field.format else 'null si no se encuentra' }})",
{% endfor %}
{% if include_products %}
  "productos": [
    {
      "nombre": "nombre exacto del producto/servicio",
      "cantidad": nÃºmero_entero,
      "unidad": "unidades/personas/kg/litros/etc"
      {% if debug_mode %},
      "confidence": "confidence score 0.0-1.0",
      "extraction_notes": "notas sobre la extracciÃ³n"
      {% endif %}
    }
  ],
{% endif %}
{% if debug_mode %}
  "extraction_metadata": {
    "chunk_quality": "calidad del chunk: high/medium/low",
    "extraction_strategy": "{{ strategy }}",
    "processing_notes": "notas del procesamiento"
  },
{% endif %}
  "texto_original_relevante": "fragmento del texto donde encontraste la informaciÃ³n principal"
}

REGLAS CRÃTICAS PARA EMPRESA vs SOLICITANTE:
- EMPRESA = compaÃ±Ã­a/organizaciÃ³n que solicita el servicio
- SOLICITANTE = persona individual dentro de la empresa
- Si ves "juan.perez@chevron.com" â†’ nombre_empresa="Chevron", email_solicitante="juan.perez@chevron.com"
- Si ves "Sofia Elena Camejo" â†’ nombre_solicitante="Sofia Elena Camejo"
- Si solo hay un email corporativo, extrae la empresa del dominio (@chevron.com â†’ Chevron)

EJEMPLOS DE DIFERENCIACIÃ“N:
- Email: "sofia.elena@chevron.com" + Nombre: "Sofia Elena"
  â†’ nombre_empresa: "Chevron"
  â†’ nombre_solicitante: "Sofia Elena"
  â†’ email_solicitante: "sofia.elena@chevron.com"
- "Contacto: Maria Rodriguez, Coordinadora de PDVSA"
  â†’ nombre_empresa: "PDVSA"
  â†’ nombre_solicitante: "Maria Rodriguez"
  â†’ cargo_solicitante: "Coordinadora"

INSTRUCCIONES ESPECÃFICAS PARA PRODUCTOS:
- Busca CUALQUIER tipo de comida, bebida o servicio de catering
- Incluye: {{ product_categories | join(', ') }}
- Busca cantidades: nÃºmeros seguidos de "personas", "pax", "unidades", "kg", "litros", "porciones"
- Si solo encuentras nombres sin cantidades, usa cantidad = 1
- Si encuentras "para X personas" pero no productos especÃ­ficos, crea producto "Catering para X personas"
- SIEMPRE incluye al menos un producto si el texto menciona cualquier tipo de comida o catering

{% if strategy == 'aggressive' %}
MODO AGRESIVO: Busca productos incluso en menciones indirectas o implÃ­citas
{% elif strategy == 'conservative' %}
MODO CONSERVADOR: Solo extrae productos explÃ­citamente mencionados con claridad
{% endif %}

INSTRUCCIONES CRÃTICAS PARA REQUIREMENTS:
- REQUIREMENTS = instrucciones especÃ­ficas o restricciones del cliente
- BUSCA: preferencias, restricciones, experiencia requerida, presupuesto, alergias, estÃ¡ndares
- NO EXTRAER: descripciones generales como "necesitamos catering" o "queremos buen servicio"
- SI NO HAY REQUIREMENTS ESPECÃFICOS: usar null y confidence 0.0

EJEMPLOS DE REQUIREMENTS VÃLIDOS:
- "Personal con mÃ¡s de 5 aÃ±os de experiencia" â†’ requirements: "Personal con mÃ¡s de 5 aÃ±os de experiencia", confidence: 0.9
- "Sin frutos secos por alergias" â†’ requirements: "Sin frutos secos por alergias", confidence: 0.95
- "Presupuesto mÃ¡ximo $500" â†’ requirements: "Presupuesto mÃ¡ximo $500", confidence: 1.0
- "Opciones vegetarianas Ãºnicamente" â†’ requirements: "Opciones vegetarianas Ãºnicamente", confidence: 0.9
- "Servicio debe estar listo 2 horas antes" â†’ requirements: "Servicio debe estar listo 2 horas antes", confidence: 0.85

EJEMPLOS DE NO-REQUIREMENTS (usar null):
- "Necesitamos catering para evento" â†’ requirements: null, confidence: 0.0
- "Queremos buen servicio" â†’ requirements: null, confidence: 0.0
- "Solicito cotizaciÃ³n" â†’ requirements: null, confidence: 0.0

EJEMPLOS DE PRODUCTOS VÃLIDOS:
{% for example in product_examples %}
- "{{ example.input }}" â†’ {"nombre": "{{ example.nombre }}", "cantidad": {{ example.cantidad }}, "unidad": "{{ example.unidad }}"}
{% endfor %}

TEXTO A ANALIZAR:
{{ chunk_text }}

Responde SOLO con el JSON solicitado:"""

    def _get_debug_prompt_template(self) -> str:
        """Template especÃ­fico para modo debug con informaciÃ³n detallada"""
        return """{{ base_prompt }}

INFORMACIÃ“N DE DEBUG ADICIONAL:
- Chunk Index: {{ chunk_index }}
- Chunk Size: {{ chunk_size }} caracteres
- Previous Extractions: {{ previous_extractions_count }}
- Context: {{ context_info }}

INSTRUCCIONES DE DEBUG:
1. Incluye confidence scores detallados para cada campo
2. Explica la lÃ³gica de extracciÃ³n en extraction_notes
3. Identifica posibles ambigÃ¼edades en ambiguity_flags
4. Registra palabras clave encontradas en keywords_found

FORMATO JSON EXTENDIDO:
{
  ... campos normales ...,
  "debug_info": {
    "confidence_details": {
      "field_name": {"score": 0.85, "reasoning": "explicaciÃ³n"}
    },
    "extraction_notes": "notas detalladas del proceso",
    "ambiguity_flags": ["lista de ambigÃ¼edades detectadas"],
    "keywords_found": ["palabras clave que influyeron en la extracciÃ³n"],
    "processing_time_estimate": "estimaciÃ³n del tiempo de procesamiento"
  }
}"""

    def render_prompt(self, template_name: str, **kwargs) -> str:
        """Renderiza un template con los parÃ¡metros dados"""
        if template_name not in self.templates:
            raise ValueError(f"Template '{template_name}' no encontrado")
        
        template = self.jinja_env.from_string(self.templates[template_name])
        return template.render(**kwargs)
    
    def get_system_prompt(self, strategy: ExtractionStrategy = ExtractionStrategy.BALANCED, 
                         debug_mode: bool = False, confidence_threshold: float = 0.7) -> str:
        """Genera system prompt dinÃ¡mico"""
        return self.render_prompt('system_prompt', 
                                strategy=strategy.value,
                                debug_mode=debug_mode,
                                confidence_threshold=confidence_threshold)
    
    def get_extraction_prompt(self, chunk_text: str, strategy: ExtractionStrategy = ExtractionStrategy.BALANCED,
                            debug_mode: bool = False, chunk_index: int = 0, **kwargs) -> str:
        """Genera prompt de extracciÃ³n dinÃ¡mico"""
        
        # Definir campos requeridos con descripciones mejoradas para mejor diferenciaciÃ³n
        required_fields = [
            {"name": "nombre_empresa", "description": "EMPRESA: nombre de la compaÃ±Ã­a/organizaciÃ³n (ej: Chevron, Microsoft, PDVSA). Si solo tienes un email como juan@chevron.com, extrae 'Chevron' del dominio", "format": None},
            {"name": "email_empresa", "description": "EMPRESA: email corporativo general de la empresa (ej: info@chevron.com, contacto@empresa.com). NO el email personal del solicitante", "format": None},
            {"name": "nombre_solicitante", "description": "PERSONA: nombre y apellido de la persona individual que hace la solicitud (ej: 'Sofia Elena Camejo Copello', 'Juan PÃ©rez')", "format": None},
            {"name": "email_solicitante", "description": "PERSONA: email personal/de trabajo de la persona especÃ­fica que solicita (ej: 'sofia.camejo@chevron.com', 'juan.perez@empresa.com')", "format": None},
            {"name": "telefono_solicitante", "description": "PERSONA: nÃºmero telefÃ³nico personal de la persona que solicita", "format": None},
            {"name": "telefono_empresa", "description": "EMPRESA: nÃºmero telefÃ³nico principal/general de la empresa", "format": None},
            {"name": "cargo_solicitante", "description": "PERSONA: puesto/cargo que ocupa la persona en la empresa (ej: 'Gerente', 'Asistente', 'Coordinador')", "format": None},
            {"name": "tipo_solicitud", "description": "tipo de solicitud de catering/evento", "format": None},
            {"name": "fecha", "description": "fecha de entrega del evento", "format": "YYYY-MM-DD"},
            {"name": "hora_entrega", "description": "hora de entrega del evento", "format": "HH:MM"},
            {"name": "lugar", "description": "direcciÃ³n completa o ubicaciÃ³n donde se realizarÃ¡ el evento", "format": None},
            # ðŸ†• MVP: Campo requirements para instrucciones especÃ­ficas del cliente
            {"name": "requirements", "description": "REQUIREMENTS: Instrucciones, preferencias o requisitos especÃ­ficos mencionados por el cliente (ej: 'empleados con +5 aÃ±os experiencia', 'solo opciones vegetarianas', 'presupuesto mÃ¡ximo $1000', 'sin frutos secos por alergias'). Solo extraer si hay instrucciones claras y especÃ­ficas, NO descripciones generales", "format": None},
            {"name": "requirements_confidence", "description": "CONFIDENCE: Nivel de confianza 0.0-1.0 sobre la extracciÃ³n de requirements. 1.0 = muy especÃ­ficos y claros, 0.5 = algo ambiguos, 0.0 = no hay requirements especÃ­ficos", "format": "decimal 0.0-1.0"}
        ]
        
        # CategorÃ­as de productos
        product_categories = [
            "sandwiches", "bocadillos", "ensaladas", "bebidas", "cafÃ©", "agua", 
            "postres", "menÃºs", "comidas", "tequeÃ±os", "empanadas", "canapÃ©s"
        ]
        
        # Ejemplos de productos
        product_examples = [
            {"input": "Catering para 60 personas", "nombre": "Catering", "cantidad": 60, "unidad": "personas"},
            {"input": "30 sandwiches", "nombre": "Sandwiches", "cantidad": 30, "unidad": "unidades"},
            {"input": "CafÃ© para todos", "nombre": "CafÃ©", "cantidad": 1, "unidad": "servicio"}
        ]
        
        if debug_mode:
            # Usar template de debug
            base_prompt = self.render_prompt('extraction_prompt',
                                           required_fields=required_fields,
                                           include_products=True,
                                           product_categories=product_categories,
                                           product_examples=product_examples,
                                           strategy=strategy.value,
                                           debug_mode=debug_mode,
                                           chunk_text=chunk_text)
            
            return self.render_prompt('debug_prompt',
                                    base_prompt=base_prompt,
                                    chunk_index=chunk_index,
                                    chunk_size=len(chunk_text),
                                    previous_extractions_count=kwargs.get('previous_extractions_count', 0),
                                    context_info=kwargs.get('context_info', 'N/A'))
        else:
            # Usar template normal
            return self.render_prompt('extraction_prompt',
                                    required_fields=required_fields,
                                    include_products=True,
                                    product_categories=product_categories,
                                    product_examples=product_examples,
                                    strategy=strategy.value,
                                    debug_mode=debug_mode,
                                    chunk_text=chunk_text)

# ============================================================================
# ðŸ”§ EXTRACTOR MODULAR MEJORADO
# ============================================================================

class ModularRFXExtractor:
    """Extractor modular que coordina todos los extractores especializados"""
    
    def __init__(self, strategy: ExtractionStrategy = ExtractionStrategy.BALANCED, debug_mode: bool = False):
        self.strategy = strategy
        self.debug_mode = debug_mode or FeatureFlags.eval_debug_enabled()
        
        # Inicializar extractores especializados
        self.product_extractor = ProductExtractor(debug_mode=self.debug_mode)
        self.solicitante_extractor = SolicitanteExtractor(debug_mode=self.debug_mode)
        self.event_extractor = EventExtractor(debug_mode=self.debug_mode)
        
        # Inicializar template manager
        self.template_manager = PromptTemplateManager()
        
        # EstadÃ­sticas de debugging
        self.extraction_stats = {
            'chunks_processed': 0,
            'ai_calls_made': 0,
            'retries_attempted': 0,
            'total_processing_time': 0.0
        }
        
        if self.debug_mode:
            logger.info(f"ðŸ”§ ModularRFXExtractor inicializado - Estrategia: {strategy.value}, Debug: {debug_mode}")
    
    def extract_from_chunk(self, chunk_text: str, chunk_index: int = 0, 
                          openai_client: OpenAI = None, openai_config: Any = None) -> ChunkExtractionResult:
        """
        Extrae informaciÃ³n de un chunk usando el sistema modular mejorado
        
        Args:
            chunk_text: Texto del chunk a procesar
            chunk_index: Ãndice del chunk para tracking
            openai_client: Cliente OpenAI configurado
            openai_config: ConfiguraciÃ³n de OpenAI
            
        Returns:
            ChunkExtractionResult: Resultado validado con confidence scores
        """
        start_time = time.time()
        
        try:
            if self.debug_mode:
                logger.debug(f"ðŸ” Procesando chunk {chunk_index} ({len(chunk_text)} chars) con estrategia {self.strategy.value}")
            
            # Generar prompts dinÃ¡micos usando templates
            system_prompt = self.template_manager.get_system_prompt(
                strategy=self.strategy,
                debug_mode=self.debug_mode,
                confidence_threshold=0.7
            )
            
            extraction_prompt = self.template_manager.get_extraction_prompt(
                chunk_text=chunk_text,
                strategy=self.strategy,
                debug_mode=self.debug_mode,
                chunk_index=chunk_index,
                previous_extractions_count=self.extraction_stats['chunks_processed']
            )
            
            # Llamada a OpenAI con retry logic
            raw_result = self._call_openai_with_retry(
                openai_client=openai_client,
                openai_config=openai_config,
                system_prompt=system_prompt,
                user_prompt=extraction_prompt
            )
            
            # Procesar resultado con extractores especializados
            chunk_result = self._process_extraction_result(raw_result, chunk_text, chunk_index)
            
            # Actualizar estadÃ­sticas
            processing_time = time.time() - start_time
            self.extraction_stats['chunks_processed'] += 1
            self.extraction_stats['total_processing_time'] += processing_time
            
            if self.debug_mode:
                logger.debug(f"âœ… Chunk {chunk_index} procesado en {processing_time:.3f}s - Productos: {len(chunk_result.productos)}")
            
            return chunk_result
            
        except Exception as e:
            logger.error(f"âŒ Error procesando chunk {chunk_index}: {e}")
            # Retornar resultado vacÃ­o en caso de error
            return ChunkExtractionResult(
                chunk_index=chunk_index,
                extraction_metadata={
                    'error': str(e),
                    'processing_time': time.time() - start_time,
                    'fallback_used': True
                }
            )
    
    def _call_openai_with_retry(self, openai_client: OpenAI, openai_config: Any,
                               system_prompt: str, user_prompt: str, max_retries: int = 2) -> Dict[str, Any]:
        """Llama a OpenAI con retry logic y parsing robusto"""
        
        for attempt in range(max_retries):
            try:
                self.extraction_stats['ai_calls_made'] += 1
                
                if self.debug_mode:
                    logger.debug(f"ðŸ¤– Llamada OpenAI intento {attempt + 1}/{max_retries}")
                
                response = openai_client.chat.completions.create(
                    model=openai_config.model,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    temperature=openai_config.temperature,
                    max_tokens=openai_config.max_tokens,
                    timeout=30
                )
                
                output = response.choices[0].message.content.strip()
                
                # Parsear JSON con manejo robusto
                return self._parse_json_response(output)
                
            except Exception as e:
                self.extraction_stats['retries_attempted'] += 1
                
                if attempt < max_retries - 1:
                    wait_time = (2 ** attempt) + 1
                    if self.debug_mode:
                        logger.warning(f"âš ï¸ Intento {attempt + 1} fallÃ³: {e}, reintentando en {wait_time}s")
                    time.sleep(wait_time)
                else:
                    logger.error(f"âŒ Todos los intentos OpenAI fallaron: {e}")
                    raise
    
    def _parse_json_response(self, output: str) -> Dict[str, Any]:
        """Parsea respuesta JSON con manejo robusto de errores"""
        # Extraer JSON del output
        json_start = output.find('{')
        json_end = output.rfind('}') + 1
        
        if json_start >= 0 and json_end > json_start:
            json_str = output[json_start:json_end]
            
            try:
                return json.loads(json_str)
            except json.JSONDecodeError:
                # Intentar limpiar JSON
                json_str = clean_json_string(json_str)
                return json.loads(json_str)
        else:
            raise ValueError("No se encontrÃ³ estructura JSON vÃ¡lida en la respuesta")
    
    def _process_extraction_result(self, raw_result: Dict[str, Any], chunk_text: str, 
                                 chunk_index: int) -> ChunkExtractionResult:
        """Procesa resultado raw usando extractores especializados"""
        
        # Extraer productos con validaciÃ³n Pydantic
        productos = self.product_extractor.extract_products(raw_result, chunk_text)
        
        # Extraer informaciÃ³n del cliente
        client_info = self.solicitante_extractor.extract_solicitante_info(raw_result, chunk_text)
        
        # Extraer informaciÃ³n del evento
        event_info = self.event_extractor.extract_event_info(raw_result, chunk_text)
        
        # Combinar confidence scores
        all_confidence_scores = []
        all_confidence_scores.extend(client_info.get('confidence_scores', []))
        all_confidence_scores.extend(event_info.get('confidence_scores', []))
        for producto in productos:
            all_confidence_scores.append(ExtractionConfidence(
                field_name=f"producto_{producto.nombre}",
                confidence=producto.confidence,
                source="ProductExtractor"
            ))
        
        # Construir resultado final
        result = ChunkExtractionResult(
            # InformaciÃ³n del solicitante y empresa
            email=client_info.get('email'),
            email_empresa=client_info.get('email_empresa'),
            nombre_solicitante=client_info.get('nombre_solicitante'),
            nombre_empresa=client_info.get('nombre_empresa'),
            telefono_solicitante=client_info.get('telefono_solicitante'),
            telefono_empresa=client_info.get('telefono_empresa'),
            cargo_solicitante=client_info.get('cargo_solicitante'),
            
            # InformaciÃ³n del evento
            fecha=event_info.get('fecha'),
            hora_entrega=event_info.get('hora_entrega'),
            lugar=event_info.get('lugar'),
            tipo_solicitud=event_info.get('tipo_solicitud'),
            
            # Productos validados
            productos=productos,
            
            # Texto original
            texto_original_relevante=raw_result.get('texto_original_relevante'),
            
            # Metadata de debugging
            confidence_scores=all_confidence_scores,
            chunk_index=chunk_index,
            extraction_metadata={
                'strategy': self.strategy.value,
                'debug_mode': self.debug_mode,
                'productos_count': len(productos),
                'client_fields_found': len([k for k, v in client_info.items() if v and k != 'confidence_scores']),
                'event_fields_found': len([k for k, v in event_info.items() if v and k != 'confidence_scores']),
                'raw_result_keys': list(raw_result.keys()) if self.debug_mode else None
            }
        )
        
        return result
    
    def get_extraction_summary(self) -> ExtractionDebugInfo:
        """Retorna resumen de extracciÃ³n para debugging"""
        
        # Calcular confidence promedio
        avg_confidence = 0.0
        if self.extraction_stats['chunks_processed'] > 0:
            avg_confidence = 0.8  # Placeholder, calcular de confidence_scores reales
        
        # Determinar calidad de extracciÃ³n
        quality = "high"
        if avg_confidence < 0.5:
            quality = "low"
        elif avg_confidence < 0.7:
            quality = "medium"
        
        return ExtractionDebugInfo(
            chunk_count=self.extraction_stats['chunks_processed'],
            total_characters=0,  # Se calcularÃ¡ en el uso
            processing_time_seconds=self.extraction_stats['total_processing_time'],
            ai_calls_made=self.extraction_stats['ai_calls_made'],
            retries_attempted=self.extraction_stats['retries_attempted'],
            confidence_summary={'average': avg_confidence},
            extraction_quality=quality
        )


class RFXProcessorService:
    """Service for processing RFX documents from PDF to structured data"""
    
    def __init__(self):
        self.openai_config = get_openai_config()
        self.openai_client = OpenAI(api_key=self.openai_config.api_key)
        self.db_client = get_database_client()
        
        # Validation helpers (legacy - mantenemos para compatibilidad)
        self.email_validator = EmailValidator()
        self.date_validator = DateValidator()
        self.time_validator = TimeValidator()
        
        # ðŸ†• NUEVO SISTEMA MODULAR
        # Inicializar extractor modular con configuraciÃ³n dinÃ¡mica
        debug_mode = FeatureFlags.eval_debug_enabled()
        extraction_strategy = self._get_extraction_strategy()
        
        self.modular_extractor = ModularRFXExtractor(
            strategy=extraction_strategy,
            debug_mode=debug_mode
        )
        
        # EstadÃ­sticas de procesamiento para debugging
        self.processing_stats = {
            'total_documents_processed': 0,
            'chunks_processed': 0,
            'average_confidence': 0.0,
            'fallback_usage_count': 0
        }
        
        logger.info(f"ðŸš€ RFXProcessorService inicializado - Estrategia: {extraction_strategy.value}, Debug: {debug_mode}")
    
    def _get_extraction_strategy(self) -> ExtractionStrategy:
        """Determina estrategia de extracciÃ³n basada en configuraciÃ³n"""
        # Puedes aÃ±adir feature flags para controlar la estrategia
        if FeatureFlags.eval_debug_enabled():
            return ExtractionStrategy.AGGRESSIVE  # MÃ¡s sensible en modo debug
        else:
            return ExtractionStrategy.BALANCED    # Balance por defecto
    
    def process_rfx_document(self, rfx_input: RFXInput, pdf_content: bytes) -> RFXProcessed:
        """
        Main processing pipeline: PDF â†’ Text â†’ AI Analysis â†’ Structured Data
        """
        try:
            logger.info(f"ðŸš€ Starting RFX processing for: {rfx_input.id}")
            
            # Step 1: Extract text from document
            extracted_text = self._extract_text_from_document(pdf_content)
            logger.info(f"ðŸ“„ Text extracted: {len(extracted_text)} characters")
            
            # Store extracted text in RFXInput for later use
            rfx_input.extracted_content = extracted_text
            
            # Step 2: Process with AI
            raw_data = self._process_with_ai(extracted_text)
            logger.info(f"ðŸ¤– AI processing completed")
            
            # Step 3: Validate and clean data (enhanced with modular data)
            validated_data = self._validate_and_clean_data(raw_data, rfx_input.id)
            logger.info(f"âœ… Data validated successfully")
            
            # Log modular processing statistics if available
            if self.modular_extractor.debug_mode and 'modular_debug_info' in raw_data:
                debug_info = raw_data['modular_debug_info']
                logger.info(f"ðŸ“Š Modular processing stats: Strategy={debug_info['extraction_strategy']}, Time={debug_info['total_processing_time']:.3f}s")
                
                # Update service-level statistics
                self.processing_stats['total_documents_processed'] += 1
                if debug_info['extraction_summary']['extraction_quality'] == 'fallback':
                    self.processing_stats['fallback_usage_count'] += 1
            
            # Step 3.5: Intelligent RFX evaluation (if enabled)
            evaluation_metadata = self._evaluate_rfx_intelligently(validated_data, rfx_input.id)
            
            # Step 4: Create structured RFX object
            rfx_processed = self._create_rfx_processed(validated_data, rfx_input, evaluation_metadata)
            
            # Step 5: Save to database
            self._save_rfx_to_database(rfx_processed)
            
            # Step 6: RFX processing completed - Proposal generation will be handled separately by user request
            
            logger.info(f"âœ… RFX processing completed successfully: {rfx_input.id}")
            return rfx_processed
            
        except Exception as e:
            logger.error(f"âŒ RFX processing failed for {rfx_input.id}: {e}")
            raise
    
    def _extract_text_from_document(self, pdf_content: bytes) -> str:
        """Extract text content from PDF bytes or other file types"""
        try:
            logger.info(f"ðŸ“„ Starting document text extraction, file size: {len(pdf_content)} bytes")
            
            # Try to detect file type from content
            if pdf_content.startswith(b'%PDF'):
                logger.info("ðŸ“„ Detected PDF file")
                # This is a PDF file
                pdf_file = io.BytesIO(pdf_content)
                pdf_reader = PyPDF2.PdfReader(pdf_file)
                
                text_pages = []
                for i, page in enumerate(pdf_reader.pages):
                    page_text = page.extract_text()
                    text_pages.append(page_text)
                    logger.debug(f"ðŸ“„ Page {i+1} extracted {len(page_text)} characters")
                
                full_text = "\n".join(text_pages)
                
                if not full_text.strip():
                    logger.error("âŒ No text extracted from PDF")
                    raise ValueError("No text could be extracted from PDF")
                
                logger.info(f"âœ… PDF extraction successful: {len(full_text)} total characters")
                logger.debug(f"ðŸ“„ PDF text preview: {full_text[:500]}...")
                return full_text
                
            elif pdf_content.startswith(b'PK'):
                logger.info("ðŸ“„ Detected DOCX file (ZIP-based)")
                # This looks like a DOCX file (ZIP format)
                try:
                    from docx import Document
                    import io
                    
                    # Create a document from the bytes
                    doc_file = io.BytesIO(pdf_content)
                    doc = Document(doc_file)
                    
                    # Extract text from all paragraphs
                    paragraphs = []
                    for paragraph in doc.paragraphs:
                        if paragraph.text.strip():
                            paragraphs.append(paragraph.text.strip())
                    
                    # Extract text from tables
                    table_texts = []
                    for table in doc.tables:
                        for row in table.rows:
                            for cell in row.cells:
                                if cell.text.strip():
                                    table_texts.append(cell.text.strip())
                    
                    # Combine all text
                    all_text = "\n".join(paragraphs + table_texts)
                    
                    if not all_text.strip():
                        logger.error("âŒ No text extracted from DOCX")
                        raise ValueError("No text could be extracted from DOCX")
                    
                    logger.info(f"âœ… DOCX extraction successful: {len(all_text)} total characters")
                    logger.info(f"ðŸ“Š Extracted {len(paragraphs)} paragraphs and {len(table_texts)} table cells")
                    logger.debug(f"ðŸ“„ DOCX text preview: {all_text[:500]}...")
                    
                    return all_text
                    
                except ImportError:
                    logger.error("âŒ python-docx not installed, falling back to text decode")
                    # Fallback to text decoding
                except Exception as e:
                    logger.error(f"âŒ DOCX extraction failed: {e}, falling back to text decode")
            
            # Try to decode as text file
            logger.info("ðŸ“„ Attempting text file decoding")
            try:
                text_content = pdf_content.decode('utf-8')
                if not text_content.strip():
                    raise ValueError("Text file is empty")
                logger.info(f"âœ… UTF-8 text extraction successful: {len(text_content)} characters")
                logger.debug(f"ðŸ“„ Text preview: {text_content[:500]}...")
                return text_content
            except UnicodeDecodeError:
                logger.warning("âš ï¸ UTF-8 decode failed, trying other encodings")
                # Try other encodings
                for encoding in ['latin-1', 'cp1252', 'iso-8859-1']:
                    try:
                        text_content = pdf_content.decode(encoding)
                        if text_content.strip():
                            logger.info(f"âœ… Text extraction successful with {encoding}: {len(text_content)} characters")
                            logger.debug(f"ðŸ“„ Text preview: {text_content[:500]}...")
                            return text_content
                    except UnicodeDecodeError:
                        continue
                        
                logger.error("âŒ All text decoding attempts failed")
                raise ValueError("Unable to decode file content")
            
        except Exception as e:
            logger.error(f"âŒ Text extraction failed: {e}")
            raise ValueError(f"Failed to extract text from file: {e}")
    
    def _process_with_ai(self, text: str) -> Dict[str, Any]:
        """ðŸ†• REFACTORIZADO: Process extracted text with modular AI system"""
        try:
            start_time = time.time()
            logger.info(f"ðŸ¤– Starting MODULAR AI processing for text of {len(text)} characters")
            
            if self.modular_extractor.debug_mode:
                logger.debug(f"ðŸ“„ Full text to process: {text[:1000]}..." if len(text) > 1000 else f"ðŸ“„ Full text: {text}")
            
            # Split text into manageable chunks
            chunks = chunk_text(text, max_tokens=1000)
            logger.info(f"ðŸ“ Text split into {len(chunks)} chunks")
            
            # ðŸ†• Process each chunk using modular extractor
            chunk_results = []
            for i, chunk in enumerate(chunks):
                logger.info(f"ðŸ¤– Processing chunk {i+1}/{len(chunks)} ({len(chunk)} characters)")
                
                if self.modular_extractor.debug_mode:
                    logger.debug(f"ðŸ“„ Chunk {i+1} content: {chunk[:300]}...")
                
                # Usar el nuevo sistema modular
                chunk_result = self.modular_extractor.extract_from_chunk(
                    chunk_text=chunk,
                    chunk_index=i,
                    openai_client=self.openai_client,
                    openai_config=self.openai_config
                )
                
                chunk_results.append(chunk_result)
                
                # Log what was found in this chunk (enhanced logging)
                if chunk_result.productos:
                    product_names = [p.nombre for p in chunk_result.productos]
                    confidences = [f"{p.confidence:.2f}" for p in chunk_result.productos]
                    logger.info(f"âœ… Chunk {i+1} found {len(chunk_result.productos)} productos: {product_names}")
                    if self.modular_extractor.debug_mode:
                        logger.debug(f"   Confidence scores: {confidences}")
                else:
                    logger.warning(f"âš ï¸ Chunk {i+1} found NO productos")
                
                # Log extraction metadata if available
                if self.modular_extractor.debug_mode and chunk_result.extraction_metadata:
                    metadata = chunk_result.extraction_metadata
                    logger.debug(f"ðŸ“Š Chunk {i+1} metadata: Strategy={metadata.get('strategy')}, Fields={metadata.get('client_fields_found', 0)}+{metadata.get('event_fields_found', 0)}")
            
            # ðŸ†• Combine results using enhanced combination logic
            logger.info(f"ðŸ”„ Combining results from {len(chunk_results)} chunks using modular system")
            combined_data = self._combine_modular_chunk_results(chunk_results)
            
            # Update processing statistics
            processing_time = time.time() - start_time
            self.processing_stats['chunks_processed'] += len(chunks)
            
            # Enhanced logging with confidence information
            logger.info(f"âœ… MODULAR AI processing completed in {processing_time:.3f}s")
            logger.info(f"ðŸ“Š Final combined products: {len(combined_data.get('productos', []))} total")
            
            if combined_data.get("productos"):
                product_names = [p['nombre'] for p in combined_data['productos']]
                logger.info(f"ðŸ“¦ Product names found: {product_names}")
                
                if self.modular_extractor.debug_mode:
                    avg_confidence = sum(p.get('confidence', 0) for p in combined_data['productos']) / len(combined_data['productos'])
                    logger.debug(f"ðŸ“Š Average product confidence: {avg_confidence:.3f}")
                    
                    # Log extraction summary
                    extraction_summary = self.modular_extractor.get_extraction_summary()
                    logger.debug(f"ðŸ“ˆ Extraction Summary: Quality={extraction_summary.extraction_quality}, AI Calls={extraction_summary.ai_calls_made}, Retries={extraction_summary.retries_attempted}")
            else:
                logger.error(f"âŒ NO PRODUCTS found in final combined data!")
                self.processing_stats['fallback_usage_count'] += 1
            
            # Store debug information in the combined data if in debug mode
            if self.modular_extractor.debug_mode:
                combined_data['modular_debug_info'] = {
                    'extraction_strategy': self.modular_extractor.strategy.value,
                    'total_processing_time': processing_time,
                    'extraction_summary': self.modular_extractor.get_extraction_summary().dict(),
                    'chunks_metadata': [result.extraction_metadata for result in chunk_results]
                }
            
            logger.debug(f"ðŸ“¦ Combined data keys: {list(combined_data.keys())}")
            return combined_data
            
        except Exception as e:
            logger.error(f"âŒ MODULAR AI processing failed: {e}")
            self.processing_stats['fallback_usage_count'] += 1
            
            # En caso de error, usar el sistema legacy como fallback
            logger.warning(f"ðŸ”„ Fallback to legacy system due to modular processing error")
            return self._process_with_ai_legacy(text)
    
    def _combine_modular_chunk_results(self, chunk_results: List[ChunkExtractionResult]) -> Dict[str, Any]:
        """ðŸ†• Combina resultados de chunks usando el sistema modular mejorado"""
        combined = {
            "email": "",
            "email_empresa": "",
            "nombre_solicitante": "",
            "nombre_empresa": "",
            "telefono_solicitante": "",
            "telefono_empresa": "",
            "cargo_solicitante": "",
            "tipo_solicitud": "",
            "productos": [],
            "hora_entrega": "",
            "fecha": "",
            "lugar": "",
            "texto_original_relevante": ""
        }
        
        # EstadÃ­sticas de combinaciÃ³n
        confidence_scores = []
        texto_fragments = []
        extraction_metadata = []
        
        logger.info(f"ðŸ”„ Combining {len(chunk_results)} modular chunk results")
        
        for i, chunk_result in enumerate(chunk_results):
            if self.modular_extractor.debug_mode:
                logger.debug(f"ðŸ“¦ Processing chunk result {i+1}: {chunk_result.chunk_index}")
            
            # Combinar campos usando prioridad de confidence si estÃ¡n disponibles
            fields_to_combine = [
                ("email", chunk_result.email),
                ("email_empresa", chunk_result.email_empresa),
                ("nombre_solicitante", chunk_result.nombre_solicitante),
                ("nombre_empresa", chunk_result.nombre_empresa),
                ("telefono_solicitante", chunk_result.telefono_solicitante),
                ("telefono_empresa", chunk_result.telefono_empresa),
                ("cargo_solicitante", chunk_result.cargo_solicitante),
                ("tipo_solicitud", chunk_result.tipo_solicitud),
                ("hora_entrega", chunk_result.hora_entrega),
                ("fecha", chunk_result.fecha),
                ("lugar", chunk_result.lugar)
            ]
            
            for field_name, field_value in fields_to_combine:
                if field_value and not combined[field_name]:
                    combined[field_name] = field_value
                    
                    if self.modular_extractor.debug_mode:
                        # Buscar confidence score para este campo
                        field_confidence = next(
                            (cs.confidence for cs in chunk_result.confidence_scores 
                             if cs.field_name == field_name), 
                            0.8
                        )
                        logger.debug(f"ðŸ“§ Found {field_name} in chunk {i+1}: {field_value} (confidence: {field_confidence:.2f})")
            
            # Combinar productos con validaciÃ³n Pydantic ya aplicada
            if chunk_result.productos:
                productos_count = len(chunk_result.productos)
                
                # Convertir ProductExtraction a dict para compatibilidad
                for producto in chunk_result.productos:
                    producto_dict = {
                        "nombre": producto.nombre,
                        "cantidad": producto.cantidad,
                        "unidad": producto.unidad
                    }
                    
                    # AÃ±adir confidence si estÃ¡ en modo debug
                    if self.modular_extractor.debug_mode:
                        producto_dict["confidence"] = producto.confidence
                    
                    combined["productos"].append(producto_dict)
                
                logger.debug(f"ðŸ“¦ Added {productos_count} productos from chunk {i+1}")
            
            # Recopilar texto relevante
            if chunk_result.texto_original_relevante:
                texto_fragments.append(f"Chunk {i+1}: {chunk_result.texto_original_relevante}")
            
            # Recopilar confidence scores
            confidence_scores.extend(chunk_result.confidence_scores)
            
            # Recopilar metadata de extracciÃ³n
            if chunk_result.extraction_metadata:
                extraction_metadata.append({
                    'chunk_index': chunk_result.chunk_index,
                    'metadata': chunk_result.extraction_metadata
                })
        
        # Combinar fragmentos de texto
        if texto_fragments:
            combined["texto_original_relevante"] = " | ".join(texto_fragments)
        
        # AÃ±adir metadata de debugging si estÃ¡ habilitado
        if self.modular_extractor.debug_mode:
            combined["modular_extraction_metadata"] = {
                'total_confidence_scores': len(confidence_scores),
                'chunks_with_products': len([cr for cr in chunk_results if cr.productos]),
                'average_products_per_chunk': len(combined["productos"]) / len(chunk_results) if chunk_results else 0,
                'extraction_metadata': extraction_metadata,
                'combined_confidence_scores': [cs.dict() for cs in confidence_scores]
            }
        
        # Log final combined result
        logger.info(f"âœ… Combined result: {len(combined['productos'])} productos total")
        
        if self.modular_extractor.debug_mode:
            # Calcular estadÃ­sticas de confidence
            if confidence_scores:
                avg_confidence = sum(cs.confidence for cs in confidence_scores) / len(confidence_scores)
                logger.debug(f"ðŸ“Š Overall average confidence: {avg_confidence:.3f}")
            
            # Log breakdown by field type with empresa information
            solicitante_fields = [k for k in combined.keys() if combined[k] and k in ['email', 'nombre_solicitante', 'telefono_solicitante', 'cargo_solicitante']]
            empresa_fields = [k for k in combined.keys() if combined[k] and k in ['email_empresa', 'nombre_empresa', 'telefono_empresa']]
            event_fields = [k for k in combined.keys() if combined[k] and k in ['fecha', 'hora_entrega', 'lugar']]
            logger.info(f"ðŸ“‹ Fields found - Solicitante: {len(solicitante_fields)}, Empresa: {len(empresa_fields)}, Event: {len(event_fields)}, Products: {len(combined['productos'])}")
            
            # Log specific empresa and solicitante info if found
            if empresa_fields:
                empresa_details = [f"{k}: {combined[k]}" for k in empresa_fields if combined[k]]
                logger.info(f"ðŸ¢ Empresa info: {', '.join(empresa_details)}")
            
            if solicitante_fields:
                solicitante_details = [f"{k}: {combined[k]}" for k in solicitante_fields if combined[k]]
                logger.info(f"ðŸ‘¤ Solicitante info: {', '.join(solicitante_details)}")
        
        return combined
    
    def _process_with_ai_legacy(self, text: str) -> Dict[str, Any]:
        """ðŸ”§ Sistema legacy como fallback para compatibilidad"""
        logger.warning(f"âš ï¸ Using LEGACY processing system as fallback")
        
        try:
            # Split text into manageable chunks
            chunks = chunk_text(text, max_tokens=1000)
            logger.info(f"ðŸ“ Legacy: Text split into {len(chunks)} chunks")
            
            # Process each chunk using legacy method
            chunk_results = []
            for i, chunk in enumerate(chunks):
                result = self._extract_info_from_chunk_legacy(chunk)
                chunk_results.append(result)
            
            # Combine results using legacy method
            combined_data = self._combine_chunk_results_legacy(chunk_results)
            return combined_data
            
        except Exception as e:
            logger.error(f"âŒ Legacy processing also failed: {e}")
            # Return minimal fallback data
            return {
                "email": "",
                "nombre_solicitante": "",
                "productos": [],
                "hora_entrega": "",
                "fecha": "",
                "lugar": "",
                "texto_original_relevante": ""
            }
    
    def _extract_info_from_chunk_legacy(self, chunk: str, max_retries: int = 2) -> Dict[str, Any]:
        """ðŸ”§ LEGACY: Extract information from a single text chunk using OpenAI"""
        
        system_prompt = """Eres un especialista experto en anÃ¡lisis de documentos RFX, catering y eventos 
        corporativos con especializaciÃ³n en identificaciÃ³n de requisitos especÃ­ficos del cliente.

Tu trabajo es extraer informaciÃ³n EXACTA del texto proporcionado, prestando especial atenciÃ³n a distinguir entre:
- DESCRIPCIÃ“N GENERAL del servicio vs REQUISITOS ESPECÃFICOS del cliente
- INFORMACIÃ“N FACTUAL vs PREFERENCIAS/RESTRICCIONES del solicitante

REGLAS FUNDAMENTALES:
- Si NO encuentras un dato especÃ­fico, usa null (no inventes informaciÃ³n)
- Extrae solo lo que estÃ¡ explÃ­citamente mencionado en el texto
- Para requisitos, busca INSTRUCCIONES ESPECÃFICAS, no descripciones generales
- MantÃ©n el contexto original para validaciÃ³n posterior
- Responde ÃšNICAMENTE con JSON vÃ¡lido, sin texto adicional

DEFINICIONES CRÃTICAS:
- REQUIREMENTS = Instrucciones especÃ­ficas, restricciones, preferencias o condiciones especiales mencionadas por el cliente
- DESCRIPCIÃ“N = InformaciÃ³n general sobre quÃ© tipo de servicio se necesita
- DATOS EMPRESARIALES = InformaciÃ³n de contacto y organizacional"""
        
        user_prompt = f"""Analiza cuidadosamente este texto de un documento de catering/evento y extrae la siguiente informaciÃ³n en formato JSON:

        {{
        "nombre_empresa": "EMPRESA: nombre de la compaÃ±Ã­a/organizaciÃ³n (ej: Chevron, Microsoft). Si solo hay email como juan@chevron.com, extrae 'Chevron' del dominio (null si no se encuentra)",
        "email_empresa": "EMPRESA: email corporativo general de la empresa. NO el email personal del solicitante (null si no se encuentra)",
        "nombre_solicitante": "PERSONA: nombre y apellido de la persona individual que hace la solicitud (null si no se encuentra)",
        "email_solicitante": "PERSONA: email personal/de trabajo de la persona especÃ­fica que solicita (null si no se encuentra)",
        "telefono_solicitante": "PERSONA: nÃºmero telefÃ³nico personal de la persona que solicita (null si no se encuentra)",
        "telefono_empresa": "EMPRESA: nÃºmero telefÃ³nico principal/general de la empresa (null si no se encuentra)",
        "cargo_solicitante": "PERSONA: puesto/cargo que ocupa la persona en la empresa (null si no se encuentra)",
        "tipo_solicitud": "tipo de solicitud de catering/evento (null si no se encuentra)",
        "productos": [
            {{
            "nombre": "nombre exacto del producto/servicio",
            "cantidad": nÃºmero_entero,
            "unidad": "unidades/personas/kg/litros/etc"
            }}
        ],
        "fecha": "fecha de entrega en formato YYYY-MM-DD (null si no se encuentra)",
        "hora_entrega": "hora de entrega en formato HH:MM (null si no se encuentra)",
        "lugar": "direcciÃ³n completa o ubicaciÃ³n del evento (null si no se encuentra)",
        "requirements": "INSTRUCCIONES ESPECÃFICAS, preferencias o restricciones mencionadas por el cliente. NO incluir descripciÃ³n general del servicio (null si no se encuentra)",
        "texto_original_relevante": "fragmento del texto donde encontraste la informaciÃ³n principal",
        "confidence_score": nÃºmero_decimal_entre_0_y_1_indicando_confianza_en_extracciÃ³n
        }}

        â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        ðŸŽ¯ INSTRUCCIONES CRÃTICAS PARA REQUIREMENTS
        â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

        **QUÃ‰ SON REQUIREMENTS (Buscar estas frases/patrones):**
        âœ… Instrucciones especÃ­ficas: "Necesito que...", "Debe incluir...", "Es importante que..."
        âœ… Preferencias explÃ­citas: "Prefiero...", "Me gustarÃ­a...", "SerÃ­a ideal..."
        âœ… Restricciones: "No queremos...", "Evitar...", "Sin...", "Prohibido..."
        âœ… Condiciones especiales: "Solo si...", "Siempre que...", "Mientras..."
        âœ… Experiencia/calificaciones: "Con experiencia de...", "Certificado en...", "Que tenga..."
        âœ… Presupuesto/calidad: "EconÃ³mico", "Premium", "MÃ¡ximo $...", "Presupuesto de..."
        âœ… Timing especÃ­fico: "Antes de las...", "Exactamente a...", "No despuÃ©s de..."
        âœ… Especificaciones tÃ©cnicas: "Vegetariano", "Sin gluten", "Halal", "OrgÃ¡nico"
        âœ… Requisitos de servicio: "Con meseros", "Auto-servicio", "Servicio completo"
        âœ… LogÃ­stica especÃ­fica: "Montaje 2 horas antes", "Incluir vajilla", "Desmontaje incluido"

        **QUÃ‰ NO SON REQUIREMENTS (No incluir):**
        âŒ DescripciÃ³n general: "Necesitamos catering" (esto es tipo_solicitud)
        âŒ InformaciÃ³n bÃ¡sica: "Para 50 personas" (esto va en productos)
        âŒ Datos de contacto: "Llamar a Juan" (esto va en otros campos)
        âŒ UbicaciÃ³n bÃ¡sica: "En nuestra oficina" (esto va en lugar)
        âŒ Fecha/hora estÃ¡ndar: "El viernes 15" (esto va en fecha)

        **EJEMPLOS PRÃCTICOS:**

        ðŸ“‹ **EJEMPLO 1 - CATERING CORPORATIVO:**
        Texto: "Necesitamos catering para 60 personas el viernes. Queremos opciones vegetarianas y que no incluya frutos secos por alergias. Preferimos meseros uniformados y montaje 2 horas antes del evento."

        âœ… CORRECTO requirements: "Opciones vegetarianas, no incluir frutos secos por alergias, meseros uniformados, montaje 2 horas antes del evento"
        âŒ INCORRECTO: "Catering para 60 personas el viernes" (eso va en productos, cantidad, fecha)

        ðŸ“‹ **EJEMPLO 2 - EVENTO CORPORATIVO:**
        Texto: "Organizamos evento de fin de aÃ±o para 100 empleados. El presupuesto es mÃ¡ximo $2000. Solo proveedores con mÃ¡s de 5 aÃ±os de experiencia."

        âœ… CORRECTO requirements: "Presupuesto mÃ¡ximo $2000, solo proveedores con mÃ¡s de 5 aÃ±os de experiencia"
        âŒ INCORRECTO: "Evento de fin de aÃ±o para 100 empleados" (eso va en tipo_solicitud y productos)

        ðŸ“‹ **EJEMPLO 3 - SIN REQUIREMENTS ESPECÃFICOS:**
        Texto: "Hola, necesitamos catering para reuniÃ³n de 30 personas maÃ±ana a las 12pm en sala de juntas."

        âœ… CORRECTO requirements: null (solo informaciÃ³n bÃ¡sica, sin instrucciones especÃ­ficas)

        â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        ðŸ” VALIDACIÃ“N DE CONTEXTO PARA REQUIREMENTS
        â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

        **ANTES DE INCLUIR COMO REQUIREMENT, PREGÃšNTATE:**
        1. Â¿Es esto una instrucciÃ³n especÃ­fica del cliente?
        2. Â¿Va mÃ¡s allÃ¡ de la informaciÃ³n bÃ¡sica del servicio?
        3. Â¿Afecta cÃ³mo debe ejecutarse el servicio?
        4. Â¿Es una preferencia, restricciÃ³n o condiciÃ³n especial?

        **SI RESPONDES SÃ A ALGUNA â†’ ES REQUIREMENT**
        **SI TODAS SON NO â†’ NO ES REQUIREMENT**

        â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

        REGLAS CRÃTICAS PARA EMPRESA vs SOLICITANTE:
        - EMPRESA = compaÃ±Ã­a/organizaciÃ³n que solicita el servicio
        - SOLICITANTE = persona individual dentro de la empresa
        - Si ves "sofia.elena@chevron.com" â†’ nombre_empresa="Chevron", email_solicitante="sofia.elena@chevron.com"

        INSTRUCCIONES ESPECÃFICAS PARA PRODUCTOS:
        - Busca CUALQUIER tipo de comida, bebida o servicio de catering
        - Incluye cantidades: nÃºmeros seguidos de "personas", "pax", "unidades", "kg", "litros"
        - Si solo encuentras "para X personas" sin productos especÃ­ficos, usa "Catering para X personas"
        - SIEMPRE incluye al menos un producto si el texto menciona comida o catering

        CONFIDENCE SCORE (0.0 - 1.0):
        - 0.9-1.0: InformaciÃ³n muy clara y explÃ­cita
        - 0.7-0.8: InformaciÃ³n clara con interpretaciÃ³n mÃ­nima
        - 0.5-0.6: InformaciÃ³n implÃ­cita o requiere interpretaciÃ³n
        - 0.3-0.4: InformaciÃ³n ambigua o parcial
        - 0.0-0.2: InformaciÃ³n muy incierta o extrapolada

        TEXTO A ANALIZAR:
        {chunk}

        Responde SOLO con el JSON solicitado:"""
        
        try:
            # ðŸ” DEBUG: Log the chunk being processed
            logger.info(f"ðŸ¤– Processing chunk of {len(chunk)} characters")
            logger.debug(f"ðŸ“„ Chunk content preview: {chunk[:200]}...")
            
            # Retry logic with exponential backoff
            response = self._call_openai_with_retry(
                model=self.openai_config.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=self.openai_config.temperature,
                max_tokens=self.openai_config.max_tokens,
                timeout=30  # 30s timeout as specified
            )
            
            output = response.choices[0].message.content.strip()
            
            # ðŸ” DEBUG: Log AI response
            logger.info(f"ðŸ¤– AI Response received: {len(output)} characters")
            logger.debug(f"ðŸ“ AI Response preview: {output[:300]}...")
            
            # Extract JSON from response
            json_start = output.find('{')
            json_end = output.rfind('}') + 1
            
            if json_start >= 0 and json_end > json_start:
                json_str = output[json_start:json_end]
                
                # Try to parse JSON with retries
                for attempt in range(max_retries):
                    try:
                        parsed_data = json.loads(json_str)
                        
                        # ðŸ” DEBUG: Log successfully parsed data
                        logger.info(f"âœ… Successfully parsed AI response on attempt {attempt + 1}")
                        logger.debug(f"ðŸ“¦ Parsed data: {parsed_data}")
                        
                        return parsed_data
                    except json.JSONDecodeError as e:
                        logger.warning(f"âš ï¸ JSON parse attempt {attempt + 1} failed: {e}")
                        if attempt < max_retries - 1:
                            json_str = clean_json_string(json_str)
                            logger.info(f"ðŸ”„ Retrying with cleaned JSON...")
                        else:
                            logger.error(f"âŒ Failed to parse JSON after {max_retries} attempts")
                            logger.error(f"ðŸ” Raw JSON: {json_str}")
                            break
            else:
                logger.error(f"âŒ No valid JSON structure found in AI response")
                logger.error(f"ðŸ” Full response: {output}")
            
            # Return empty structure if parsing fails
            logger.warning(f"âš ï¸ Returning empty result due to parsing failure")
            return self._get_empty_extraction_result()
            
        except Exception as e:
            logger.error(f"âŒ OpenAI API call failed: {e}")
            return self._get_empty_extraction_result()
    
    def _combine_chunk_results_legacy(self, chunk_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ðŸ”§ LEGACY: Combine results from multiple text chunks into a single data structure"""
        combined = {
            "email": "",
            "nombre_solicitante": "",
            "productos": [],
            "hora_entrega": "",
            "fecha": "",
            "lugar": "",
            "texto_original_relevante": ""
        }
        
        # ðŸ” DEBUG: Log chunk combination process
        logger.info(f"ðŸ”„ Combining results from {len(chunk_results)} chunks")
        
        texto_fragments = []
        
        for i, result in enumerate(chunk_results):
            logger.debug(f"ðŸ“¦ Processing chunk {i+1} result: {result}")
            
            # Take the first non-empty value for each field
            if result.get("email") and not combined["email"]:
                combined["email"] = result["email"]
                logger.debug(f"ðŸ“§ Found email in chunk {i+1}: {result['email']}")
                
            if result.get("nombre_solicitante") and not combined["nombre_solicitante"]:
                combined["nombre_solicitante"] = result["nombre_solicitante"]
                logger.debug(f"ðŸ‘¤ Found solicitante in chunk {i+1}: {result['nombre_solicitante']}")
                
            if result.get("hora_entrega") and not combined["hora_entrega"]:
                combined["hora_entrega"] = result["hora_entrega"]
                logger.debug(f"ðŸ• Found hora_entrega in chunk {i+1}: {result['hora_entrega']}")
                
            if result.get("fecha") and not combined["fecha"]:
                combined["fecha"] = result["fecha"]
                logger.debug(f"ðŸ“… Found fecha in chunk {i+1}: {result['fecha']}")
                
            if result.get("lugar") and not combined["lugar"]:
                combined["lugar"] = result["lugar"]
                logger.debug(f"ðŸ“ Found lugar in chunk {i+1}: {result['lugar']}")
            
            # Combine products from all chunks
            if result.get("productos"):
                productos_count = len(result["productos"])
                combined["productos"].extend(result["productos"])
                logger.debug(f"ðŸ“¦ Added {productos_count} productos from chunk {i+1}")
            
            # Collect text fragments
            if result.get("texto_original_relevante"):
                texto_fragments.append(f"Chunk {i+1}: {result['texto_original_relevante']}")
        
        # Combine text fragments
        if texto_fragments:
            combined["texto_original_relevante"] = " | ".join(texto_fragments)
        
        # ðŸ” DEBUG: Log final combined result
        logger.info(f"âœ… Combined result: {len(combined['productos'])} productos total")
        logger.debug(f"ðŸ“¦ Final combined data: {combined}")
        
        return combined
    
    # ============================================================================
    # ðŸ†• MVP: FUNCIONES DE VALIDACIÃ“N DE REQUIREMENTS
    # ============================================================================
    
    def _validate_basic_requirements(self, requirements: str, confidence: float) -> Dict[str, Any]:
        """ValidaciÃ³n mÃ­nima para MVP de requirements"""
        if not requirements:
            return {
                'validated_requirements': None,
                'adjusted_confidence': 0.0,
                'validation_issues': [],
                'needs_review': False
            }
        
        issues = []
        adjusted_confidence = confidence
        cleaned_requirements = requirements.strip()
        
        # ValidaciÃ³n 1: Longitud apropiada
        if len(cleaned_requirements) < 5:
            issues.append("requirements_too_short")
            adjusted_confidence *= 0.3
            logger.warning(f"âš ï¸ Requirements muy cortos: '{cleaned_requirements}'")
        
        if len(cleaned_requirements) > 1500:
            issues.append("requirements_too_long")
            adjusted_confidence *= 0.5
            cleaned_requirements = cleaned_requirements[:1500] + "..."
            logger.warning(f"âš ï¸ Requirements truncados por longitud")
        
        # ValidaciÃ³n 2: Detectar si son demasiado genÃ©ricos
        generic_phrases = [
            "necesitamos catering", "queremos evento", "solicito servicio",
            "buen servicio", "servicio de calidad", "excelente atenciÃ³n"
        ]
        
        requirements_lower = cleaned_requirements.lower()
        has_generic = any(phrase in requirements_lower for phrase in generic_phrases)
        
        # Detectar si hay requirements especÃ­ficos
        specific_indicators = [
            "preferimos", "debe", "sin", "con experiencia", "mÃ­nimo", "mÃ¡ximo",
            "aÃ±os", "alergia", "vegetariano", "vegano", "halal", "kosher",
            "presupuesto", "horario", "personal", "certificaciÃ³n"
        ]
        
        has_specific = any(indicator in requirements_lower for indicator in specific_indicators)
        
        if has_generic and not has_specific:
            adjusted_confidence *= 0.4
            issues.append("seems_too_generic")
            logger.warning(f"âš ï¸ Requirements parecen demasiado genÃ©ricos: {confidence:.2f} â†’ {adjusted_confidence:.2f}")
        
        # ValidaciÃ³n 3: Detectar si contiene informaciÃ³n de empresa/contacto (error de extracciÃ³n)
        contact_indicators = ["@", "tel:", "email:", "telÃ©fono", "contacto:"]
        if any(indicator in requirements_lower for indicator in contact_indicators):
            adjusted_confidence *= 0.3
            issues.append("contains_contact_info")
            logger.warning(f"âš ï¸ Requirements contienen informaciÃ³n de contacto")
        
        # ValidaciÃ³n 4: Score demasiado bajo necesita revisiÃ³n
        needs_review = adjusted_confidence < 0.4
        
        if needs_review:
            logger.warning(f"âš ï¸ Requirements necesitan revisiÃ³n - Confidence: {adjusted_confidence:.2f}")
        
        return {
            'validated_requirements': cleaned_requirements,
            'adjusted_confidence': round(adjusted_confidence, 3),
            'validation_issues': issues,
            'needs_review': needs_review,
            'original_confidence': confidence
        }
    
    def _log_requirements_extraction(self, rfx_id: str, validation_result: Dict) -> None:
        """Log detallado para analizar y mejorar requirements extraction"""
        log_data = {
            'rfx_id': rfx_id,
            'timestamp': datetime.now(),
            'requirements_extracted': validation_result.get('validated_requirements'),
            'confidence_score': validation_result.get('adjusted_confidence', 0.0),
            'original_confidence': validation_result.get('original_confidence', 0.0),
            'validation_issues': validation_result.get('validation_issues', []),
            'needs_review': validation_result.get('needs_review', False),
            'requirements_length': len(validation_result.get('validated_requirements', '') or '')
        }
        
        # Log bÃ¡sico para monitoring
        logger.info(f"ðŸ“‹ Requirements extracted for {rfx_id}: "
                   f"Length={log_data['requirements_length']}, "
                   f"Confidence={log_data['confidence_score']:.3f}, "
                   f"Issues={len(log_data['validation_issues'])}")
        
        # Log detallado en debug
        if logger.isEnabledFor(logging.DEBUG):
            logger.debug(f"ðŸ“Š Requirements log data: {log_data}")
        
        # TODO: En el futuro, guardar en tabla de logs para anÃ¡lisis
        # self.requirements_logs.insert(log_data)
    
    def _validate_and_clean_data(self, raw_data: Dict[str, Any], rfx_id: str) -> Dict[str, Any]:
        """Validate and clean extracted data with fallbacks for invalid values"""
        # ðŸ” DEBUG: Log validation process
        logger.info(f"ðŸ” Starting validation for RFX: {rfx_id}")
        logger.debug(f"ðŸ“¦ Raw data to validate: {raw_data}")
        
        cleaned_data = raw_data.copy()
        validation_status = {
            "email_valid": False,
            "fecha_valid": False,
            "hora_valid": False,
            "has_original_data": True
        }
        
        # Validate and clean email - PRESERVE original even if invalid
        email = raw_data.get("email", "").strip()
        if email:
            if self.email_validator.validate(email):
                cleaned_data["email"] = email
                validation_status["email_valid"] = True
                logger.info(f"âœ… Email validated successfully: {email}")
            else:
                # Keep original email but mark as invalid
                cleaned_data["email"] = email
                validation_status["email_valid"] = False
                logger.warning(f"âš ï¸ Email format invalid but preserved: {email}")
        else:
            # Only use fallback if completely empty
            cleaned_data["email"] = "cliente@example.com"
            validation_status["email_valid"] = False
            validation_status["has_original_data"] = False
            logger.warning(f"âš ï¸ No email found, using fallback")
        
        # Validate and clean date - PRESERVE original even if invalid
        fecha = raw_data.get("fecha", "").strip()
        if fecha:
            if self.date_validator.validate(fecha):
                cleaned_data["fecha"] = fecha
                validation_status["fecha_valid"] = True
                logger.info(f"âœ… Date validated successfully: {fecha}")
            else:
                # Try to preserve original date format
                cleaned_data["fecha"] = fecha
                validation_status["fecha_valid"] = False
                logger.warning(f"âš ï¸ Date format may be invalid but preserved: {fecha}")
        else:
            # Only use fallback if completely empty
            cleaned_data["fecha"] = datetime.now().strftime('%Y-%m-%d')
            validation_status["fecha_valid"] = False
            logger.warning(f"âš ï¸ No date found, using current date")
        
        # Validate and clean time - PRESERVE original even if invalid
        hora = raw_data.get("hora_entrega", "").strip()
        if hora:
            if self.time_validator.validate(hora):
                cleaned_data["hora_entrega"] = hora
                validation_status["hora_valid"] = True
                logger.info(f"âœ… Time validated successfully: {hora}")
            else:
                # Keep original time but mark as invalid
                cleaned_data["hora_entrega"] = hora
                validation_status["hora_valid"] = False
                logger.warning(f"âš ï¸ Time format may be invalid but preserved: {hora}")
        else:
            # Only use fallback if completely empty
            cleaned_data["hora_entrega"] = "12:00"
            validation_status["hora_valid"] = False
            logger.warning(f"âš ï¸ No time found, using default")
        
        # Clean and validate client name - MORE PERMISSIVE
        nombre_solicitante = raw_data.get("nombre_solicitante", "").strip()
        if nombre_solicitante and nombre_solicitante.lower() not in ["null", "none", ""]:
            cleaned_data["nombre_solicitante"] = nombre_solicitante.title()
            logger.info(f"âœ… Solicitante name processed: {nombre_solicitante}")
        else:
            cleaned_data["nombre_solicitante"] = f"Solicitante-{rfx_id}"
            validation_status["has_original_data"] = False
            logger.warning(f"âš ï¸ No client name found, using fallback")
        
        # Clean place - MORE PERMISSIVE
        lugar = raw_data.get("lugar", "").strip()
        if lugar and lugar.lower() not in ["null", "none", ""]:
            cleaned_data["lugar"] = lugar
            logger.info(f"âœ… Location preserved: {lugar}")
        else:
            cleaned_data["lugar"] = "UbicaciÃ³n por definir"
            logger.warning(f"âš ï¸ No location found, using fallback")
        
        # Validate products - VERY PERMISSIVE
        productos = raw_data.get("productos", [])
        logger.info(f"ðŸ” Validating {len(productos)} products from AI extraction")
        logger.debug(f"ðŸ“¦ Raw products data: {productos}")
        
        if not productos:
            logger.error(f"âŒ No products array found in AI response")
            logger.debug(f"ðŸ“¦ Full raw_data keys: {list(raw_data.keys())}")
            raise ValueError("No se encontraron productos en el documento")
        
        # Clean and validate each product - VERY PERMISSIVE
        cleaned_productos = []
        for i, producto in enumerate(productos):
            logger.debug(f"ðŸ” Processing product {i+1}: {producto}")
            
            # Handle different formats of product data
            if isinstance(producto, dict):
                nombre = None
                cantidad = 1
                unidad = "unidades"
                
                # Try to extract name from various possible keys
                for name_key in ["nombre", "name", "product", "producto", "item"]:
                    if producto.get(name_key):
                        nombre = str(producto[name_key]).strip()
                        break
                
                # Try to extract quantity
                for qty_key in ["cantidad", "quantity", "qty", "count", "numero"]:
                    if producto.get(qty_key):
                        try:
                            cantidad = max(1, int(float(producto[qty_key])))
                            break
                        except (ValueError, TypeError):
                            logger.warning(f"âš ï¸ Invalid quantity for product {i+1}: {producto.get(qty_key)}")
                            continue
                
                # Try to extract unit
                for unit_key in ["unidad", "unit", "units", "medida"]:
                    if producto.get(unit_key):
                        unidad = str(producto[unit_key]).strip().lower()
                        break
                
                # Accept product if it has any meaningful name
                if nombre and len(nombre.strip()) > 0 and nombre.lower() not in ["null", "none", "", "undefined"]:
                    cleaned_producto = {
                        "nombre": nombre.title(),
                        "cantidad": cantidad,
                        "unidad": unidad or "unidades"
                    }
                    cleaned_productos.append(cleaned_producto)
                    logger.info(f"âœ… Product {i+1} accepted: {cleaned_producto}")
                else:
                    logger.warning(f"âš ï¸ Product {i+1} rejected - invalid name: {nombre}")
                    
            elif isinstance(producto, str):
                # Handle product as simple string
                nombre = producto.strip()
                if nombre and len(nombre) > 0 and nombre.lower() not in ["null", "none", "", "undefined"]:
                    cleaned_producto = {
                        "nombre": nombre.title(),
                        "cantidad": 1,
                        "unidad": "unidades"
                    }
                    cleaned_productos.append(cleaned_producto)
                    logger.info(f"âœ… Product {i+1} accepted as string: {cleaned_producto}")
                else:
                    logger.warning(f"âš ï¸ Product {i+1} rejected - invalid string: {nombre}")
            else:
                logger.warning(f"âš ï¸ Product {i+1} skipped - unrecognized format: {type(producto)}")
        
        logger.info(f"ðŸ“Š Product validation completed: {len(cleaned_productos)} valid products from {len(productos)} raw products")
        
        if not cleaned_productos:
            logger.error(f"âŒ No valid products could be processed from AI extraction")
            logger.error(f"ðŸ” Original products data: {productos}")
            # Create a fallback product to avoid complete failure
            logger.warning(f"âš ï¸ Creating fallback product to prevent complete failure")
            cleaned_productos = [{
                "nombre": "Producto No Especificado",
                "cantidad": 1,
                "unidad": "unidades"
            }]
            logger.info(f"âœ… Fallback product created: {cleaned_productos[0]}")
        
        cleaned_data["productos"] = cleaned_productos
        
        # âœ¨ PRESERVE: Datos de empresa sin validaciÃ³n (son opcionales)
        empresa_fields = ["nombre_empresa", "email_empresa", "telefono_empresa", "telefono_solicitante", "cargo_solicitante"]
        for field in empresa_fields:
            if field in raw_data and raw_data[field]:
                cleaned_data[field] = str(raw_data[field]).strip()
                logger.info(f"âœ… Empresa field preserved: {field} = {cleaned_data[field]}")
            else:
                cleaned_data[field] = ""
                logger.debug(f"ðŸ“ Empresa field empty: {field}")
        
        # ðŸ†• MVP: Validate and clean requirements
        requirements = raw_data.get("requirements", "")
        requirements_confidence = raw_data.get("requirements_confidence", 0.0)
        
        if requirements and requirements.strip():
            # Aplicar validaciÃ³n bÃ¡sica
            validation_result = self._validate_basic_requirements(
                requirements.strip(), 
                float(requirements_confidence) if requirements_confidence else 0.0
            )
            
            # Guardar resultados validados
            cleaned_data["requirements"] = validation_result['validated_requirements']
            cleaned_data["requirements_confidence"] = validation_result['adjusted_confidence']
            
            # Actualizar validation status
            validation_status["requirements_valid"] = not validation_result['needs_review']
            validation_status["requirements_issues"] = validation_result['validation_issues']
            
            # Log extraction para mejora continua
            self._log_requirements_extraction(rfx_id, validation_result)
            
            logger.info(f"âœ… Requirements processed: confidence={validation_result['adjusted_confidence']:.3f}, "
                       f"issues={len(validation_result['validation_issues'])}")
        else:
            # No hay requirements extraÃ­dos
            cleaned_data["requirements"] = None
            cleaned_data["requirements_confidence"] = 0.0
            validation_status["requirements_valid"] = True  # VÃ¡lido que no haya requirements
            validation_status["requirements_issues"] = []
            logger.debug(f"ðŸ“ No requirements extracted for RFX: {rfx_id}")
        
        # Add validation metadata
        cleaned_data["validation_metadata"] = validation_status
        
        # ðŸ” DEBUG: Log final validation result
        logger.info(f"âœ… Validation completed for {len(cleaned_productos)} products")
        logger.info(f"ðŸ“Š Validation status: {validation_status}")
        logger.debug(f"ðŸ“¦ Final cleaned data: {cleaned_data}")
        
        return cleaned_data
    
    def _create_rfx_processed(self, validated_data: Dict[str, Any], rfx_input: RFXInput, evaluation_metadata: Optional[Dict[str, Any]] = None) -> RFXProcessed:
        """Create RFXProcessed object from validated data and evaluation results"""
        try:
            # ðŸ” DEBUG: Log object creation
            logger.info(f"ðŸ”¨ Creating RFXProcessed object for: {rfx_input.id}")
            
            # Convert productos to ProductoRFX objects (map Spanish to English fields)
            productos = [
                ProductoRFX(
                    product_name=p["nombre"],
                    quantity=p["cantidad"],
                    unit=p["unidad"]
                )
                for p in validated_data["productos"]
            ]
            
            # Prepare enhanced metadata including empresa data
            metadata = {
                "original_rfx_id": rfx_input.id,  # Preserve original string ID for frontend
                "texto_original_length": len(rfx_input.extracted_content or ""),
                "productos_count": len(productos),
                "processing_version": "2.1",  # Upgraded version with intelligent evaluation
                "validation_status": validated_data.get("validation_metadata", {}),
                "texto_original_relevante": validated_data.get("texto_original_relevante", ""),
                "ai_extraction_quality": "high" if validated_data.get("validation_metadata", {}).get("has_original_data", False) else "fallback",
                # âœ¨ AÃ‘ADIR: Datos de empresa en metadatos para acceso del frontend
                "nombre_empresa": validated_data.get("nombre_empresa", ""),
                "email_empresa": validated_data.get("email_empresa", ""),
                "telefono_empresa": validated_data.get("telefono_empresa", ""),
                "telefono_solicitante": validated_data.get("telefono_solicitante", ""),
                "cargo_solicitante": validated_data.get("cargo_solicitante", "")
            }
            
            # Integrate intelligent evaluation metadata if available
            if evaluation_metadata:
                metadata["intelligent_evaluation"] = evaluation_metadata
                
                # Log evaluation integration
                if evaluation_metadata.get('evaluation_enabled'):
                    if 'evaluation_error' in evaluation_metadata:
                        logger.warning(f"âš ï¸ Evaluation error included in metadata for {rfx_input.id}")
                    else:
                        score = evaluation_metadata.get('execution_summary', {}).get('consolidated_score')
                        quality = evaluation_metadata.get('execution_summary', {}).get('overall_quality')
                        domain = evaluation_metadata.get('domain_detection', {}).get('primary_domain')
                        logger.info(f"ðŸ“Š Evaluation metadata integrated - Score: {score}, Quality: {quality}, Domain: {domain}")
                else:
                    logger.debug(f"ðŸ”§ Evaluation disabled - metadata reflects feature flag status")
            else:
                logger.debug(f"â„¹ï¸ No evaluation metadata provided for {rfx_input.id}")
            
            # ðŸ” DEBUG: Log metadata
            logger.debug(f"ðŸ“Š Metadata prepared: {metadata}")
            
            # Generate UUID for database, but keep original ID in metadata
            from uuid import uuid4
            rfx_uuid = uuid4()
            
            rfx_processed = RFXProcessed(
                id=rfx_uuid,
                rfx_type=rfx_input.rfx_type,
                title=f"RFX Request - {validated_data.get('nombre_solicitante', 'Unknown')} - {rfx_input.rfx_type.value if hasattr(rfx_input.rfx_type, 'value') else str(rfx_input.rfx_type)}",
                location=validated_data["lugar"],
                delivery_date=validated_data["fecha"],
                delivery_time=validated_data["hora_entrega"],
                status=RFXStatus.IN_PROGRESS,
                original_pdf_text=rfx_input.extracted_content,
                requested_products=[p.dict() for p in productos] if productos else [],
                metadata_json=metadata,
                received_at=datetime.now(),
                
                # Legacy/extracted fields for compatibility
                email=validated_data["email"],
                requester_name=validated_data["nombre_solicitante"],
                company_name=validated_data.get("nombre_empresa", ""),
                products=productos,  # productos is already a list of RFXProductRequest objects
                
                # ðŸ†• MVP: Requirements especÃ­ficos del cliente
                requirements=validated_data.get("requirements"),
                requirements_confidence=validated_data.get("requirements_confidence")
            )
            
            # ðŸ” DEBUG: Log successful creation
            logger.info(f"âœ… RFXProcessed object created successfully")
            empresa_info = metadata or {}
            empresa_nombre = empresa_info.get("nombre_empresa", "No especificada")
            original_id = empresa_info.get("original_rfx_id", str(rfx_processed.id))
            logger.info(f"ðŸ“¦ RFX Object: Original ID={original_id}, UUID={rfx_processed.id}, Solicitante={rfx_processed.requester_name}, Empresa={empresa_nombre}, Productos={len(rfx_processed.products)}")
            
            # Log empresa details if available
            if empresa_info.get("nombre_empresa"):
                logger.info(f"ðŸ¢ Empresa: {empresa_info.get('nombre_empresa')} | Email: {empresa_info.get('email_empresa', 'N/A')} | Tel: {empresa_info.get('telefono_empresa', 'N/A')}")
            
            # Log solicitante details if available  
            if empresa_info.get("telefono_solicitante") or empresa_info.get("cargo_solicitante"):
                logger.info(f"ðŸ‘¤ Solicitante: {rfx_processed.requester_name} | Tel: {empresa_info.get('telefono_solicitante', 'N/A')} | Cargo: {empresa_info.get('cargo_solicitante', 'N/A')}")
            
            # ðŸ†• MVP: Log requirements info if available
            if rfx_processed.requirements:
                req_preview = rfx_processed.requirements[:100] + "..." if len(rfx_processed.requirements) > 100 else rfx_processed.requirements
                logger.info(f"ðŸ“‹ Requirements: '{req_preview}' | Confidence: {rfx_processed.requirements_confidence:.3f}")
            else:
                logger.debug(f"ðŸ“ No requirements extracted for RFX")
                
            return rfx_processed
            
        except Exception as e:
            logger.error(f"âŒ Failed to create RFXProcessed object: {e}")
            raise
    
    def _save_rfx_to_database(self, rfx_processed: RFXProcessed) -> None:
        """Save processed RFX to database V2.0 with normalized structure"""
        try:
            # Extract company and requester information from metadata
            metadata = rfx_processed.metadata_json or {}
            
            # 1. Create or find company
            # Clean email to meet database constraints (no spaces allowed)
            email_empresa = metadata.get("email_empresa", "")
            if email_empresa:
                email_empresa = email_empresa.replace(" ", "").strip()
                if not email_empresa or "@" not in email_empresa:
                    email_empresa = None
            else:
                email_empresa = None
                
            company_data = {
                "name": metadata.get("nombre_empresa", "Unknown Company"),
                "email": email_empresa,
                "phone": metadata.get("telefono_empresa")
            }
            company_record = self.db_client.insert_company(company_data)
            
            # 2. Create or find requester
            # Clean requester email to meet database constraints
            requester_email = rfx_processed.email or ""
            if requester_email:
                requester_email = requester_email.replace(" ", "").strip()
                if not requester_email or "@" not in requester_email:
                    requester_email = None
            else:
                requester_email = None
                
            requester_data = {
                "company_id": company_record.get("id"),
                "name": rfx_processed.requester_name or "Unknown Requester",
                "email": requester_email,
                "phone": metadata.get("telefono_solicitante"),
                "position": metadata.get("cargo_solicitante")
            }
            requester_record = self.db_client.insert_requester(requester_data)
            
            # 3. Prepare RFX data for database (V2.0 schema)
            rfx_data = {
                "id": str(rfx_processed.id),
                "company_id": company_record.get("id"),
                "requester_id": requester_record.get("id"),
                "rfx_type": rfx_processed.rfx_type.value if hasattr(rfx_processed.rfx_type, 'value') else str(rfx_processed.rfx_type),
                "title": f"RFX Request - {rfx_processed.rfx_type}",
                "location": rfx_processed.location,
                "delivery_date": rfx_processed.delivery_date.isoformat() if rfx_processed.delivery_date else None,
                "delivery_time": rfx_processed.delivery_time.isoformat() if rfx_processed.delivery_time else None,
                "status": rfx_processed.status.value if hasattr(rfx_processed.status, 'value') else str(rfx_processed.status),
                "original_pdf_text": rfx_processed.original_pdf_text,
                "requested_products": rfx_processed.requested_products or [],
                "received_at": rfx_processed.received_at.isoformat() if rfx_processed.received_at else None,
                "metadata_json": rfx_processed.metadata_json,
                
                # ðŸ†• MVP: Requirements especÃ­ficos del cliente
                "requirements": rfx_processed.requirements,
                "requirements_confidence": rfx_processed.requirements_confidence
            }
            
            # 4. Insert RFX data
            rfx_record = self.db_client.insert_rfx(rfx_data)
            
            # 5. Insert structured products if available
            if rfx_processed.products:
                structured_products = []
                for product in rfx_processed.products:
                    product_data = {
                        "product_name": product.product_name if hasattr(product, 'product_name') else product.nombre,
                        "quantity": product.quantity if hasattr(product, 'quantity') else product.cantidad,
                        "unit": product.unit if hasattr(product, 'unit') else product.unidad,
                        "estimated_unit_price": getattr(product, 'estimated_unit_price', None),
                        "notes": f"Extracted from RFX processing"
                    }
                    structured_products.append(product_data)
                
                self.db_client.insert_rfx_products(rfx_record["id"], structured_products)
                logger.info(f"âœ… {len(structured_products)} structured products saved")
            
            # 6. Create history event
            history_event = {
                "rfx_id": str(rfx_processed.id),
                "event_type": "rfx_processed",
                "description": f"RFX processed successfully with {len(rfx_processed.products or [])} products",
                "new_values": {
                    "company_name": company_data["name"],
                    "requester_name": requester_data["name"],
                    "processing_version": metadata.get("processing_version", "2.0"),
                    "original_rfx_id": metadata.get("original_rfx_id"),
                    "product_count": len(rfx_processed.products or [])
                },
                "performed_by": "system_ai"
            }
            self.db_client.insert_rfx_history(history_event)
            
            logger.info(f"âœ… RFX saved to database V2.0: {rfx_processed.id}")
            
        except Exception as e:
            logger.error(f"âŒ Failed to save RFX to database: {e}")
            raise
    
    # REMOVED: _generate_proposal_automatically
    # La generaciÃ³n de propuestas ahora se maneja por separado cuando el usuario lo solicite
    # mediante el endpoint /api/proposals/generate despuÃ©s de revisar datos y establecer costos
    
    def _map_rfx_data_for_proposal(self, rfx_data_raw: Dict[str, Any]) -> Dict[str, Any]:
        """
        ðŸ”§ Mapea estructura BD V2.0 (inglÃ©s, normalizada) â†’ ProposalGenerationService (espaÃ±ol, plana)
        
        Args:
            rfx_data_raw: Datos del RFX desde BD V2.0 con companies, requesters, etc.
            
        Returns:
            Dict con estructura esperada por ProposalGenerationService
        """
        try:
            # Extraer datos de BD V2.0
            company_data = rfx_data_raw.get("companies", {}) or {}
            requester_data = rfx_data_raw.get("requesters", {}) or {}
            requested_products = rfx_data_raw.get("requested_products", [])
            metadata = rfx_data_raw.get("metadata_json", {}) or {}
            
            # Mapear productos con fallback a metadata si es necesario
            productos_mapped = []
            if requested_products:
                # Usar productos estructurados de BD
                for producto in requested_products:
                    productos_mapped.append({
                        "nombre": producto.get("product_name", producto.get("nombre", "Producto")),
                        "cantidad": producto.get("quantity", producto.get("cantidad", 1)),
                        "unidad": producto.get("unit", producto.get("unidad", "unidades"))
                    })
            else:
                # Fallback a metadata si no hay productos estructurados
                metadata_productos = metadata.get("productos", [])
                for producto in metadata_productos:
                    if isinstance(producto, dict):
                        productos_mapped.append({
                            "nombre": producto.get("nombre", "Producto"),
                            "cantidad": producto.get("cantidad", 1),
                            "unidad": producto.get("unidad", "unidades")
                        })
            
            # Si aÃºn no hay productos, crear uno por defecto
            if not productos_mapped:
                productos_mapped = [{
                    "nombre": "Servicio de Catering",
                    "cantidad": 1,
                    "unidad": "servicio"
                }]
            
            # Estructura mapeada esperada por ProposalGenerationService
            mapped_data = {
                # âœ… Mapear informaciÃ³n del cliente (combinando empresa + solicitante)
                "clientes": {
                    "nombre": requester_data.get("name", metadata.get("nombre_solicitante", "Cliente")),
                    "email": requester_data.get("email", metadata.get("email", "")),
                    "empresa": company_data.get("name", metadata.get("nombre_empresa", "")),
                    "telefono": requester_data.get("phone", metadata.get("telefono_solicitante", "")),
                    "cargo": requester_data.get("position", metadata.get("cargo_solicitante", "")),
                    # InformaciÃ³n adicional de empresa
                    "email_empresa": company_data.get("email", metadata.get("email_empresa", "")),
                    "telefono_empresa": company_data.get("phone", metadata.get("telefono_empresa", ""))
                },
                
                # âœ… Mapear productos
                "productos": productos_mapped,
                
                # âœ… Mapear informaciÃ³n del evento
                "lugar": rfx_data_raw.get("location", metadata.get("lugar", "Por definir")),
                "fecha_entrega": rfx_data_raw.get("delivery_date", metadata.get("fecha", "")),
                "hora_entrega": rfx_data_raw.get("delivery_time", metadata.get("hora_entrega", "")),
                "tipo": rfx_data_raw.get("rfx_type", metadata.get("tipo_solicitud", "catering")),
                
                # âœ… InformaciÃ³n adicional
                "id": str(rfx_data_raw.get("id", "")),
                "title": rfx_data_raw.get("title", ""),
                "received_at": rfx_data_raw.get("received_at", ""),
                "texto_original_relevante": metadata.get("texto_original_relevante", "")
            }
            
            logger.debug(f"ðŸ”§ Datos mapeados para propuesta: Cliente={mapped_data['clientes']['nombre']}, "
                        f"Empresa={mapped_data['clientes']['empresa']}, Productos={len(mapped_data['productos'])}")
            
            return mapped_data
            
        except Exception as e:
            logger.error(f"âŒ Error mapeando datos para propuesta: {e}")
            # Fallback con estructura mÃ­nima
            return {
                "clientes": {"nombre": "Cliente", "email": ""},
                "productos": [{"nombre": "Servicio de Catering", "cantidad": 1, "unidad": "servicio"}],
                "lugar": "Por definir",
                "fecha_entrega": "",
                "hora_entrega": "",
                "tipo": "catering"
            }
    
    # REMOVED: _log_proposal_generation_event and _log_proposal_generation_error
    # Estas funciones ya no se necesitan porque la generaciÃ³n de propuestas es manual

    def _call_openai_with_retry(self, max_retries: int = 3, **kwargs) -> Any:
        """Call OpenAI API with exponential backoff retry logic"""
        for attempt in range(max_retries):
            try:
                logger.info(f"ðŸ”„ OpenAI API call attempt {attempt + 1}/{max_retries}")
                response = self.openai_client.chat.completions.create(**kwargs)
                logger.info(f"âœ… OpenAI API call successful on attempt {attempt + 1}")
                return response
            except Exception as e:
                wait_time = (2 ** attempt) + 1  # Exponential backoff: 2s, 3s, 5s
                logger.warning(f"âš ï¸ OpenAI API call failed on attempt {attempt + 1}: {e}")
                
                if attempt < max_retries - 1:
                    logger.info(f"ðŸ”„ Retrying in {wait_time} seconds...")
                    time.sleep(wait_time)
                else:
                    logger.error(f"âŒ OpenAI API call failed after {max_retries} attempts")
                    raise

    def _evaluate_rfx_intelligently(self, validated_data: Dict[str, Any], rfx_id: str) -> Dict[str, Any]:
        """
        Ejecuta evaluaciÃ³n inteligente del RFX usando el orquestador.
        
        Args:
            validated_data: Datos del RFX ya validados y limpios
            rfx_id: ID Ãºnico del RFX para logging
            
        Returns:
            Dict con metadata de evaluaciÃ³n para incluir en RFXProcessed
        """
        # Verificar si la evaluaciÃ³n estÃ¡ habilitada
        if not FeatureFlags.evals_enabled():
            logger.debug(f"ðŸ”§ EvaluaciÃ³n inteligente deshabilitada por feature flag para RFX: {rfx_id}")
            return {
                'evaluation_enabled': False,
                'reason': 'Feature flag disabled'
            }
        
        try:
            # Import lazy para evitar circular imports y mejorar startup time
            from backend.services.evaluation_orchestrator import evaluate_rfx_intelligently
            
            logger.info(f"ðŸ” Iniciando evaluaciÃ³n inteligente para RFX: {rfx_id}")
            start_time = datetime.now()
            
            # Ejecutar evaluaciÃ³n completa
            eval_result = evaluate_rfx_intelligently(validated_data)
            execution_time = (datetime.now() - start_time).total_seconds()
            
            # Log resultados principales
            domain = eval_result['domain_detection']['primary_domain']
            confidence = eval_result['domain_detection']['confidence']
            score = eval_result['consolidated_score']
            quality = eval_result['execution_summary']['overall_quality']
            
            logger.info(f"âœ… EvaluaciÃ³n completada para {rfx_id} - Dominio: {domain} ({confidence:.3f}), Score: {score:.3f} ({quality}), Tiempo: {execution_time:.3f}s")
            
            # Extraer recomendaciones crÃ­ticas y de alta prioridad
            critical_recommendations = [
                {
                    'title': rec['title'],
                    'description': rec['description'],
                    'priority': rec['priority'],
                    'category': rec['category'],
                    'type': rec['type']
                }
                for rec in eval_result['recommendations'] 
                if rec.get('priority') in ['critical', 'high']
            ]
            
            # Log recomendaciones crÃ­ticas
            if critical_recommendations:
                logger.warning(f"âš ï¸ RFX {rfx_id} tiene {len(critical_recommendations)} recomendaciones crÃ­ticas/alta prioridad")
                for rec in critical_recommendations[:2]:  # Log solo las primeras 2
                    logger.warning(f"   - [{rec['priority'].upper()}] {rec['title']}")
            
            # Estructura optimizada de metadata (solo datos esenciales)
            evaluation_metadata = {
                'evaluation_enabled': True,
                'execution_summary': {
                    'consolidated_score': score,
                    'overall_quality': quality,
                    'execution_time_seconds': round(execution_time, 3),
                    'evaluators_executed': eval_result['execution_summary']['evaluators_executed'],
                    'timestamp': datetime.now().isoformat()
                },
                'domain_detection': {
                    'primary_domain': domain,
                    'confidence': confidence,
                    'secondary_domains': eval_result['domain_detection'].get('secondary_domains', [])
                },
                'evaluation_scores': {
                    'generic_score': eval_result['generic_evaluation']['score'],
                    'domain_specific_score': eval_result['domain_specific_evaluation']['score'],
                    'generic_evaluators_count': eval_result['generic_evaluation']['count'],
                    'domain_specific_evaluators_count': eval_result['domain_specific_evaluation']['count']
                },
                'critical_recommendations': critical_recommendations,
                'recommendations_summary': {
                    'total_count': len(eval_result['recommendations']),
                    'critical_count': len([r for r in eval_result['recommendations'] if r.get('priority') == 'critical']),
                    'high_priority_count': len([r for r in eval_result['recommendations'] if r.get('priority') == 'high']),
                    'by_category': {
                        cat: len([r for r in eval_result['recommendations'] if r.get('category') == cat])
                        for cat in set(r.get('category', 'other') for r in eval_result['recommendations'])
                    }
                }
            }
            
            # Log opcional de debugging si estÃ¡ habilitado
            if FeatureFlags.eval_debug_enabled():
                logger.debug(f"ðŸ” EvaluaciÃ³n detallada para {rfx_id}: {evaluation_metadata}")
            
            return evaluation_metadata
            
        except Exception as e:
            logger.error(f"âŒ Error ejecutando evaluaciÃ³n inteligente para {rfx_id}: {str(e)}")
            
            # NO fallar el procesamiento principal - solo log el error
            # Retornar metadata de error para debugging
            return {
                'evaluation_enabled': True,
                'evaluation_error': {
                    'error_message': str(e),
                    'error_type': type(e).__name__,
                    'timestamp': datetime.now().isoformat(),
                    'rfx_id': rfx_id
                },
                'execution_summary': {
                    'consolidated_score': None,
                    'overall_quality': 'evaluation_failed',
                    'execution_time_seconds': 0.0,
                    'evaluators_executed': 0
                }
            }

    def _get_empty_extraction_result(self) -> Dict[str, Any]:
        """Return empty result structure for failed extractions"""
        return {
            "email": "",
            "nombre_solicitante": "",
            "productos": [],
            "hora_entrega": "",
            "fecha": "",
            "lugar": "",
            "texto_original_relevante": ""
        }
    
    # ============================================================================
    # ðŸ” MÃ‰TODOS DE DEBUGGING Y ESTADÃSTICAS
    # ============================================================================
    
    def get_processing_statistics(self) -> Dict[str, Any]:
        """ðŸ†• Retorna estadÃ­sticas de procesamiento para monitoring y debugging"""
        base_stats = self.processing_stats.copy()
        
        # AÃ±adir estadÃ­sticas del extractor modular
        if hasattr(self, 'modular_extractor'):
            extraction_summary = self.modular_extractor.get_extraction_summary()
            base_stats.update({
                'modular_extractor_stats': extraction_summary.dict(),
                'modular_extraction_quality': extraction_summary.extraction_quality,
                'modular_chunks_processed': extraction_summary.chunk_count,
                'modular_ai_calls': extraction_summary.ai_calls_made,
                'modular_retries': extraction_summary.retries_attempted
            })
        
        # Calcular ratios y mÃ©tricas derivadas
        if base_stats['total_documents_processed'] > 0:
            base_stats['fallback_usage_ratio'] = base_stats['fallback_usage_count'] / base_stats['total_documents_processed']
            base_stats['average_chunks_per_document'] = base_stats['chunks_processed'] / base_stats['total_documents_processed']
        else:
            base_stats['fallback_usage_ratio'] = 0.0
            base_stats['average_chunks_per_document'] = 0.0
        
        return base_stats
    
    def reset_processing_statistics(self) -> None:
        """ðŸ†• Resetea estadÃ­sticas de procesamiento"""
        self.processing_stats = {
            'total_documents_processed': 0,
            'chunks_processed': 0,
            'average_confidence': 0.0,
            'fallback_usage_count': 0
        }
        
        if hasattr(self, 'modular_extractor'):
            # Resetear estadÃ­sticas del extractor modular
            self.modular_extractor.extraction_stats = {
                'chunks_processed': 0,
                'ai_calls_made': 0,
                'retries_attempted': 0,
                'total_processing_time': 0.0
            }
        
        logger.info("ðŸ“Š Processing statistics reset")
    
    def get_debug_mode_status(self) -> Dict[str, Any]:
        """ðŸ†• Retorna estado del modo debug y configuraciÃ³n actual"""
        return {
            'debug_mode_enabled': getattr(self.modular_extractor, 'debug_mode', False),
            'extraction_strategy': getattr(self.modular_extractor, 'strategy', ExtractionStrategy.BALANCED).value,
            'feature_flags': {
                'evals_enabled': FeatureFlags.evals_enabled(),
                'eval_debug_enabled': FeatureFlags.eval_debug_enabled(),
                'meta_prompting_enabled': FeatureFlags.meta_prompting_enabled() if hasattr(FeatureFlags, 'meta_prompting_enabled') else False,
                'vertical_agent_enabled': FeatureFlags.vertical_agent_enabled() if hasattr(FeatureFlags, 'vertical_agent_enabled') else False
            },
            'processing_version': '2.1_modular',
            'available_extractors': ['ProductExtractor', 'ClientExtractor', 'EventExtractor'],
            'template_manager_initialized': hasattr(self.modular_extractor, 'template_manager')
        }
