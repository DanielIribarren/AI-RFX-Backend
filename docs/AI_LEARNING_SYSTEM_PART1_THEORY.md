# üß† AI LEARNING SYSTEM - PARTE 1: FUNDAMENTOS TE√ìRICOS

**Versi√≥n:** 1.0  
**Fecha:** 26 de Enero, 2026  
**Prop√≥sito:** Fundamentos te√≥ricos y algoritmos profesionales para sistema de aprendizaje continuo

---

## üìã CONTENIDO

1. [Visi√≥n General](#visi√≥n-general)
2. [Continual Learning](#continual-learning)
3. [Knowledge Graphs Temporales](#knowledge-graphs-temporales)
4. [Few-Shot Learning](#few-shot-learning)
5. [Contextual Bandits](#contextual-bandits)
6. [Collaborative Filtering](#collaborative-filtering)

---

## üéØ VISI√ìN GENERAL

### Objetivo Principal

Crear un sistema de aprendizaje continuo que permita a los agentes de IA:
- **Aprender** de las interacciones y correcciones de los usuarios
- **Personalizar** respuestas basadas en patrones hist√≥ricos
- **Mejorar** la precisi√≥n de predicciones con el tiempo
- **Adaptarse** a las G espec√≠ficas de cada organizaci√≥n

### Principios Fundamentales

```
üß† APRENDIZAJE CONTINUO (Continual Learning)
   ‚îú‚îÄ Sin reentrenamiento completo del modelo
   ‚îú‚îÄ Actualizaci√≥n incremental de conocimiento
   ‚îî‚îÄ Preservaci√≥n de aprendizajes previos

üéØ PERSONALIZACI√ìN CONTEXTUAL
   ‚îú‚îÄ Por usuario individual
   ‚îú‚îÄ Por organizaci√≥n
   ‚îî‚îÄ Por industria/dominio

‚ö° TIEMPO REAL
   ‚îú‚îÄ Aprendizaje inmediato de feedback
   ‚îú‚îÄ Aplicaci√≥n instant√°nea de patrones
   ‚îî‚îÄ Sin latencia perceptible

üîí PRIVACIDAD Y SEGURIDAD
   ‚îú‚îÄ Datos aislados por organizaci√≥n
   ‚îú‚îÄ Control de acceso granular
   ‚îî‚îÄ Cumplimiento GDPR/CCPA
```

---

## üìö 1. CONTINUAL LEARNING (APRENDIZAJE CONTINUO)

### Definici√≥n

> Capacidad de aprender de flujos de informaci√≥n no estacionarios de forma incremental, preservando conocimiento previo mientras se integra nueva informaci√≥n.

### Caracter√≠sticas Clave

#### A. Adaptaci√≥n
- Los sistemas pueden adaptarse a nuevas distribuciones de datos sin reentrenamiento masivo
- Mantienen **plasticidad neural**: capacidad de cambiar predicciones basadas en nueva informaci√≥n
- Evitan **p√©rdida de plasticidad**: rigidez que impide aprender de nuevos datos

#### B. Similitud de Tareas (Transfer Learning)
- Aprovechan conocimiento de tareas relacionadas
- **Transferencia positiva**: aprendizaje en tarea A mejora desempe√±o en tarea B
- Ejemplo: Si el agente aprende precios de catering, puede transferir ese conocimiento a eventos corporativos

#### C. Agn√≥stico a Tareas
- Pueden identificar cuando datos pertenecen a distribuciones diferentes
- No requieren etiquetas expl√≠citas de "tipo de tarea"
- Ejemplo: Distinguir autom√°ticamente entre RFX de catering vs. RFX de construcci√≥n

#### D. Tolerancia al Ruido
- Filtran se√±ales err√≥neas en los datos
- Aprenden la distribuci√≥n real sin componentes de ruido
- Cr√≠tico para datos generados por usuarios (errores de tipeo, inconsistencias)

#### E. Eficiencia de Recursos
- Compactos en almacenamiento
- Eficientes en c√≥mputo
- Bajo consumo energ√©tico
- **Clave para escalabilidad empresarial**

### T√©cnicas Principales

#### 1. Elastic Weight Consolidation (EWC)

Protege pesos importantes de tareas previas durante el aprendizaje de nuevas tareas.

```python
class EWCLearning:
    """
    Elastic Weight Consolidation: Protege conocimiento previo
    """
    def __init__(self, model, fisher_matrix, lambda_ewc=0.4):
        self.model = model
        self.fisher = fisher_matrix  # Importancia de cada peso
        self.lambda_ewc = lambda_ewc
        self.old_params = copy.deepcopy(model.parameters())
    
    def compute_loss(self, new_loss):
        """
        Loss = new_task_loss + penalty_for_changing_important_weights
        """
        ewc_loss = 0
        for name, param in self.model.named_parameters():
            fisher_weight = self.fisher[name]
            old_param = self.old_params[name]
            # Penalizar cambios en pesos importantes
            ewc_loss += (fisher_weight * (param - old_param).pow(2)).sum()
        
        return new_loss + (self.lambda_ewc / 2) * ewc_loss
```

**Ventajas:**
- ‚úÖ Preserva conocimiento cr√≠tico de tareas previas
- ‚úÖ Permite aprender nuevas tareas sin "olvidar"
- ‚úÖ Matem√°ticamente fundamentado (Fisher Information Matrix)

**Cu√°ndo usar:**
- Cuando hay tareas secuenciales relacionadas
- Cuando el "olvido catastr√≥fico" es un riesgo

#### 2. Memory Replay

Almacena ejemplos de tareas previas y los "reproduce" peri√≥dicamente durante el entrenamiento.

```python
class MemoryReplay:
    """
    Almacena y reproduce ejemplos hist√≥ricos para evitar olvido
    """
    def __init__(self, buffer_size=1000):
        self.memory_buffer = []
        self.buffer_size = buffer_size
    
    def store_experience(self, data, label):
        """Almacena experiencia en buffer"""
        if len(self.memory_buffer) >= self.buffer_size:
            # Estrategia: reemplazar ejemplos antiguos o menos importantes
            self.memory_buffer.pop(0)
        self.memory_buffer.append((data, label))
    
    def train_with_replay(self, new_batch):
        """Mezcla datos nuevos con ejemplos del pasado"""
        replay_batch = random.sample(self.memory_buffer, k=32)
        combined_batch = new_batch + replay_batch
        return combined_batch
```

**Ventajas:**
- ‚úÖ Simple de implementar
- ‚úÖ Efectivo para evitar olvido
- ‚úÖ No requiere modificar arquitectura del modelo

**Desventajas:**
- ‚ùå Requiere almacenamiento de ejemplos
- ‚ùå Puede tener problemas de privacidad (almacena datos reales)

#### 3. Progressive Neural Networks

A√±ade nuevas "columnas" de red neuronal para nuevas tareas, preservando las anteriores.

```python
class ProgressiveNetwork:
    """
    Red neuronal que crece con nuevas tareas
    """
    def __init__(self):
        self.columns = []  # Una columna por tarea
    
    def add_task(self, new_task):
        """A√±ade nueva columna para nueva tarea"""
        new_column = NeuralColumn(
            lateral_connections=self.columns  # Conexiones a columnas previas
        )
        self.columns.append(new_column)
        # Columnas previas se congelan (no se modifican)
```

**Ventajas:**
- ‚úÖ Cero olvido (columnas previas no se modifican)
- ‚úÖ Transfer learning autom√°tico (conexiones laterales)
- ‚úÖ Escalable a muchas tareas

**Desventajas:**
- ‚ùå Crece en tama√±o con cada nueva tarea
- ‚ùå M√°s complejo de implementar

---

## üï∏Ô∏è 2. KNOWLEDGE GRAPHS TEMPORALES

### Definici√≥n

> Grafos de conocimiento que rastrean entidades, relaciones y su evoluci√≥n temporal, permitiendo memoria din√°mica y contextual para agentes de IA.

### Arquitectura: Graphiti (Zep AI)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    GRAPHITI ARCHITECTURE                     ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ   Episode    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Extraction  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Graph   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  (Event/Msg) ‚îÇ      ‚îÇ   Engine     ‚îÇ      ‚îÇ  Update  ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ         ‚îÇ                      ‚îÇ                     ‚îÇ       ‚îÇ
‚îÇ         ‚ñº                      ‚ñº                     ‚ñº       ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ         TEMPORAL KNOWLEDGE GRAPH                     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ Entity ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Entity ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Entity ‚îÇ            ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  (t1)  ‚îÇ    ‚îÇ  (t2)  ‚îÇ    ‚îÇ  (t3)  ‚îÇ            ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ       ‚îÇ             ‚îÇ             ‚îÇ                  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ              Relationships                           ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ        (with validity intervals)                     ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ              HYBRID SEARCH ENGINE                    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ Semantic ‚îÇ  ‚îÇ Keyword  ‚îÇ  ‚îÇ Graph Traversal  ‚îÇ  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ(Vectors) ‚îÇ  ‚îÇ  (BM25)  ‚îÇ  ‚îÇ   (Cypher)       ‚îÇ  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Caracter√≠sticas Clave

#### A. Modelo Bi-Temporal

```python
class TemporalEdge:
    """
    Cada relaci√≥n tiene dos timestamps:
    - t_occurred: Cu√°ndo ocurri√≥ el evento en el mundo real
    - t_ingested: Cu√°ndo se ingres√≥ al sistema
    """
    def __init__(self, source, target, relation_type):
        self.source = source
        self.target = target
        self.relation_type = relation_type
        self.t_occurred = None      # Tiempo del evento real
        self.t_ingested = None       # Tiempo de ingesta
        self.t_valid = None          # Inicio de validez
        self.t_invalid = None        # Fin de validez (puede ser None)
    
    def is_valid_at(self, timestamp):
        """Verifica si la relaci√≥n era v√°lida en un momento dado"""
        if self.t_valid is None:
            return False
        if self.t_invalid is None:
            return timestamp >= self.t_valid
        return self.t_valid <= timestamp < self.t_invalid
```

#### B. Resoluci√≥n de Conflictos

```python
class ConflictResolver:
    """
    Cuando nueva informaci√≥n contradice conocimiento existente
    """
    def resolve_conflict(self, existing_fact, new_fact):
        # Estrategia 1: M√°s reciente gana
        if new_fact.t_occurred > existing_fact.t_occurred:
            existing_fact.t_invalid = new_fact.t_occurred
            return new_fact
        
        # Estrategia 2: Mantener ambos con intervalos de validez
        if self.are_both_valid_at_different_times(existing_fact, new_fact):
            return [existing_fact, new_fact]
        
        # Estrategia 3: Confianza/fuente
        if new_fact.confidence > existing_fact.confidence:
            existing_fact.t_invalid = new_fact.t_ingested
            return new_fact
        
        return existing_fact
```

#### C. Consultas Hist√≥ricas

```cypher
-- Cypher query: Estado del conocimiento en un momento espec√≠fico
MATCH (e:Entity)-[r:RELATIONSHIP]->(e2:Entity)
WHERE r.t_valid <= $timestamp 
  AND (r.t_invalid IS NULL OR r.t_invalid > $timestamp)
RETURN e, r, e2

-- Ejemplo: ¬øQu√© precio ten√≠a "Teque√±os" el 15 de diciembre?
MATCH (p:Product {name: "Teque√±os"})-[r:HAS_PRICE]->(price:Price)
WHERE r.t_valid <= datetime("2024-12-15")
  AND (r.t_invalid IS NULL OR r.t_invalid > datetime("2024-12-15"))
RETURN price.amount
```

### Ventajas sobre RAG Est√°tico

| Caracter√≠stica | RAG Est√°tico | Graphiti (Temporal KG) |
|----------------|--------------|------------------------|
| **Actualizaci√≥n** | Batch recomputation | Incremental real-time |
| **Latencia** | 10-30 segundos | <300ms (P95) |
| **Conflictos** | Sobrescribe o duplica | Resuelve con temporalidad |
| **Historial** | No disponible | Queries temporales completas |
| **Relaciones** | Impl√≠citas en embeddings | Expl√≠citas en grafo |
| **Escalabilidad** | Recomputa todo el grafo | Solo actualiza nodos afectados |

---

## üéì 3. FEW-SHOT LEARNING & ADAPTIVE PROMPTING

### Definici√≥n

> T√©cnica que permite a modelos de IA aprender nuevas tareas con pocos ejemplos (2-10), adaptando su comportamiento mediante ejemplos contextuales en el prompt.

### Pipeline Completo

```
1. USER QUERY
   "Estimar precio para Teque√±os (200 unidades)"
            ‚Üì
2. SEMANTIC SEARCH (Vector Store)
   Buscar ejemplos similares hist√≥ricos
            ‚Üì
3. RETRIEVE TOP-K EXAMPLES (k=3-5)
   Seleccionar los m√°s relevantes
            ‚Üì
4. PROMPT CONSTRUCTION
   System + Examples + Query
            ‚Üì
5. LLM PROCESSING
   Modelo infiere patr√≥n
            ‚Üì
6. OUTPUT
   "$0.68/u (bulk discount 18%)"
```

### Implementaci√≥n

```python
class FewShotLearningEngine:
    """
    Motor de aprendizaje few-shot con recuperaci√≥n din√°mica de ejemplos
    """
    def __init__(self, vector_store, embedding_model):
        self.vector_store = vector_store
        self.embedding_model = embedding_model
        self.example_cache = {}
    
    def retrieve_examples(self, query, k=5, filters=None):
        """Recupera los k ejemplos m√°s relevantes"""
        # 1. Generar embedding del query
        query_embedding = self.embedding_model.encode(query)
        
        # 2. B√∫squeda sem√°ntica
        results = self.vector_store.similarity_search(
            query_embedding,
            k=k * 2,  # Recuperar m√°s para filtrar
            filters=filters
        )
        
        # 3. Re-ranking por relevancia contextual
        ranked_results = self.rerank_by_context(
            results, query=query, context=filters
        )
        
        return ranked_results[:k]
    
    def construct_prompt(self, query, examples, system_message):
        """Construye prompt con ejemplos din√°micos"""
        prompt_parts = [
            f"System: {system_message}\n",
            "\n# Examples from your organization's history:\n"
        ]
        
        for i, example in enumerate(examples, 1):
            prompt_parts.append(
                f"\nExample {i}:\n"
                f"Input: {example['input']}\n"
                f"Output: {example['output']}\n"
            )
        
        prompt_parts.append(
            f"\n# Now solve:\nInput: {query}\nOutput:"
        )
        
        return "".join(prompt_parts)
```

### Estrategias de Selecci√≥n

```python
class ExampleSelectionStrategy:
    @staticmethod
    def diversity_sampling(examples, k):
        """Selecciona ejemplos diversos"""
        selected = [examples[0]]  # M√°s similar
        remaining = examples[1:]
        
        while len(selected) < k and remaining:
            # Maximizar diversidad
            best_candidate = max(
                remaining,
                key=lambda x: min(
                    1 - cosine_similarity(x, s) for s in selected
                )
            )
            selected.append(best_candidate)
            remaining.remove(best_candidate)
        
        return selected
    
    @staticmethod
    def recency_weighted(examples, k, decay_factor=0.95):
        """Prioriza ejemplos recientes"""
        now = datetime.now()
        scored = []
        
        for ex in examples:
            days_old = (now - ex['timestamp']).days
            recency_score = decay_factor ** days_old
            final_score = ex['similarity'] * recency_score
            scored.append((final_score, ex))
        
        scored.sort(reverse=True)
        return [ex for _, ex in scored[:k]]
```

**Ventajas:**
- ‚úÖ Sin Fine-Tuning: No requiere reentrenar el modelo
- ‚úÖ Adaptaci√≥n R√°pida: Aprende de 2-5 ejemplos
- ‚úÖ Bajo Costo: No consume recursos de entrenamiento
- ‚úÖ Transparencia: Ejemplos interpretables
- ‚úÖ Personalizaci√≥n: Cada organizaci√≥n tiene sus propios ejemplos

---

## üé∞ 4. CONTEXTUAL BANDITS

### Definici√≥n

> Algoritmo de aprendizaje por refuerzo que balancea exploraci√≥n (probar nuevas opciones) y explotaci√≥n (usar la mejor opci√≥n conocida) en contextos espec√≠ficos.

### Problema que Resuelve

```
Escenario: Usuario solicita presupuesto para evento de 300 personas

Opciones (Arms):
1. Usar precio est√°ndar del cat√°logo
2. Usar precio promedio hist√≥rico del usuario
3. Usar precio de eventos similares (same size)
4. Usar precio con descuento por volumen autom√°tico

¬øCu√°l opci√≥n genera mayor satisfacci√≥n del cliente?
‚Üí Depende del CONTEXTO (tipo de cliente, historial, industria, etc.)
```

### Implementaci√≥n: Thompson Sampling

```python
class ContextualBandit:
    """
    Contextual Bandit con Thompson Sampling
    """
    def __init__(self, arms):
        self.arms = arms
        # Distribuciones Beta para cada arm
        self.arm_distributions = {
            arm: {'alpha': 1, 'beta': 1}
            for arm in arms
        }
    
    def select_arm(self, context):
        """
        Thompson Sampling:
        1. Para cada arm, samplea de su distribuci√≥n Beta
        2. Selecciona el arm con el sample m√°s alto
        """
        samples = {}
        for arm in self.arms:
            alpha = self.arm_distributions[arm]['alpha']
            beta = self.arm_distributions[arm]['beta']
            samples[arm] = np.random.beta(alpha, beta)
        
        return max(samples, key=samples.get)
    
    def update(self, arm, context, reward):
        """Actualiza distribuci√≥n basado en reward"""
        normalized_reward = self._normalize_reward(reward)
        
        if normalized_reward >= 0.5:  # Success
            self.arm_distributions[arm]['alpha'] += 1
        else:  # Failure
            self.arm_distributions[arm]['beta'] += 1
```

### Comparaci√≥n de Estrategias

| Estrategia | Exploraci√≥n/Explotaci√≥n | Complejidad | Convergencia |
|------------|-------------------------|-------------|--------------|
| **Thompson Sampling** | Autom√°tico (probabil√≠stico) | Media | R√°pida ‚≠ê |
| **Œµ-greedy** | Manual (par√°metro Œµ) | Baja | Lenta |
| **UCB** | Autom√°tico (determin√≠stico) | Media | Media |

---

## ü§ù 5. COLLABORATIVE FILTERING

### Definici√≥n

> T√©cnica que aprovecha similitudes entre usuarios e √≠tems para generar recomendaciones personalizadas.

### Tipos

#### A. User-Based
```
"Usuarios similares a ti tambi√©n usaron..."
Usuario A: ‚úì Teque√±os, ‚úì Empanadas, ‚úì Canap√©s
Usuario B: ‚úì Teque√±os, ‚úì Empanadas, ? Canap√©s
                                      ‚Üë
                              Recomendar Canap√©s a B
```

#### B. Item-Based
```
"Productos similares a los que usaste..."
Teque√±os ‚Üí Empanadas (80% co-occurrence)
Teque√±os ‚Üí Canap√©s (75% co-occurrence)
```

### Implementaci√≥n con Embeddings

```python
class CollaborativeFilteringEngine:
    """
    Sistema de recomendaciones basado en embeddings
    """
    def __init__(self, embedding_dim=64):
        self.embedding_dim = embedding_dim
        self.user_embeddings = {}
        self.product_embeddings = {}
    
    def train(self, interactions):
        """
        Matrix Factorization: R ‚âà U √ó P^T
        """
        # Crear matriz de interacciones
        users = list(set(i['user_id'] for i in interactions))
        products = list(set(i['product_id'] for i in interactions))
        
        R = np.zeros((len(users), len(products)))
        # ... llenar matriz ...
        
        # SVD para factorizaci√≥n
        from scipy.sparse.linalg import svds
        U, sigma, Vt = svds(R, k=self.embedding_dim)
        
        # Guardar embeddings
        for i, user in enumerate(users):
            self.user_embeddings[user] = U[i]
        for i, product in enumerate(products):
            self.product_embeddings[product] = Vt.T[i]
    
    def recommend_products(self, user_id, k=10):
        """Recomienda top-k productos"""
        user_emb = self.user_embeddings[user_id]
        
        scores = {}
        for product_id, product_emb in self.product_embeddings.items():
            # Score = similitud coseno
            score = np.dot(user_emb, product_emb) / (
                np.linalg.norm(user_emb) * np.linalg.norm(product_emb)
            )
            scores[product_id] = score
        
        ranked = sorted(scores.items(), key=lambda x: x[1], reverse=True)
        return [p for p, s in ranked[:k]]
```

**Ventajas:**
- ‚úÖ Serendipity: Descubre patrones no obvios
- ‚úÖ Sin Features Manuales: Aprende autom√°ticamente
- ‚úÖ Escalable: Funciona con millones de usuarios/productos
- ‚úÖ Explainable: Puede justificar recomendaciones

---

## üìä RESUMEN COMPARATIVO

| T√©cnica | Cu√°ndo Usar | Complejidad | Escalabilidad |
|---------|-------------|-------------|---------------|
| **Continual Learning** | Aprendizaje incremental sin olvido | Alta | Alta |
| **Knowledge Graphs** | Relaciones complejas y temporales | Alta | Media-Alta |
| **Few-Shot Learning** | Adaptaci√≥n r√°pida con pocos ejemplos | Baja-Media | Alta |
| **Contextual Bandits** | Optimizaci√≥n de decisiones contextuales | Media | Alta |
| **Collaborative Filtering** | Recomendaciones basadas en similitud | Media | Muy Alta |

---

**Contin√∫a en:** `AI_LEARNING_SYSTEM_PART2_IMPLEMENTATION.md`
